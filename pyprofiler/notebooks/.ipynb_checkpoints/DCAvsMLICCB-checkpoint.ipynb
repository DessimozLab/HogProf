{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dir': {'datadir': '/scratch/dmoi/datasets/birds/', 'omadir': '/scratch/dmoi/datasets/birds/'}, 'orthoxmltar': '', 'email': 'dmoi@unil.ch'}\n"
     ]
    }
   ],
   "source": [
    "from pyprofiler.utils import hashutils\n",
    "import ete3\n",
    "import random\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.utils.hashutils as hashutils\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pyprofiler.profiler as profiler\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import redis\n",
    "##to get the mapping of oma hogs to cogs to interactions in specific species I used dask distributed and a redis server\n",
    "#you may need to get these up and running for you own cluster configuration before this notebook will work for you\n",
    "import dask\n",
    "import scipy\n",
    "from dask import dataframe as dd\n",
    "import pickle\n",
    "from bloom_filter2 import BloomFilter\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lsh\n",
      "indexing lsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/tables/leaf.py:367: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  warnings.warn(\"\"\"\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#lets load a compiled db containing the OMA root HOGs into a profiler oject \n",
    "p = profiler.Profiler(lshforestpath = '/scratch/dmoi/datasets/all/newlshforest.pkl' , hashes_h5='/scratch/dmoi/datasets/birds/all/hashes.h5' , mat_path= None, oma = '/scratch/dmoi/datasets/OMA/apr2021/OmaServer.h5', tar= None , nsamples = 256 , mastertree = '/scratch/dmoi/datasets/birds/all_test_master_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabHog(ID, verbose = True):\n",
    "    try:\n",
    "        entry = p.db_obj.entry_by_entry_nr(p.db_obj.id_resolver.resolve(ID))\n",
    "        return entry[4].decode() , entry\n",
    "    except:\n",
    "        return np.nan,np.nan\n",
    "#map to OMA HOGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap = '/scratch/dmoi/datasets/humap_PPI/humap2_ppis_ACC_20200821.pairsWprob'\n",
    "calc_humap = False\n",
    "if calc_humap == True:\n",
    "    #load humap data\n",
    "    humap_df = pd.read_table(humap, header = None)\n",
    "    print(humap_df)\n",
    "    humap_df = humap_df[humap_df[2] > .75 ]\n",
    "    mapper = set( list(humap_df[1]) + list(humap_df[0]) )\n",
    "    mapper = { protid: grabHog(protid) for protid in mapper }\n",
    "    humap_df['hog1'] = humap_df[1].map(mapper)\n",
    "    humap_df['hog2'] = humap_df[0].map(mapper)\n",
    "    humap_df['hogid_1'] = humap_df['hog1'].map(lambda x:x[0])\n",
    "    humap_df['hogid_2'] = humap_df['hog2'].map(lambda x:x[0])\n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df['fam1'] = humap_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    humap_df['fam2'] = humap_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df.fam1 = humap_df.fam1.map(int)\n",
    "    humap_df.fam2 = humap_df.fam2.map(int)\n",
    "    print(len(humap_df))\n",
    "    humap_df.to_csv(humap+'hogmapped.csv')\n",
    "else:\n",
    "    humap_df = pd.read_csv(humap+'hogmapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap_pairs = humap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "calc_hogs_humap = False\n",
    "if calc_hogs_humap == True:\n",
    "    allhogs = set([])\n",
    "    allhogs = allhogs.union( set(humap_df.fam1.unique() ) )\n",
    "    allhogs = allhogs.union( set(humap_df.fam2.unique() ) )\n",
    "    print(len(allhogs))\n",
    "    for fam in allhogs:\n",
    "        print(fam)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "        except:\n",
    "            print('err', fam)\n",
    "        print(prof)\n",
    "        profiles.update(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Hogs_humap = False\n",
    "if save_Hogs_humap == True:\n",
    "    with open(humap + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(humap + 'gold_standard_profiles.pkl' , 'rb') as profiles_out:\n",
    "    humap_profiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      mat  \\\n",
      "524288  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "516097  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "516099  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "786438  [[1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "573452  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                     tree  \n",
      "524288  (((\\n--7757)), ((\\n--7764)), ((\\n--9597)), ((\\...  \n",
      "516097  (((\\n--9597)), ((\\n--9598)), ((\\n--9606)), ((\\...  \n",
      "516099  (((\\n--9597)), ((\\n--9598)), ((\\n--9606)), ((\\...  \n",
      "786438  (((\\n--868595)), ((\\n--349161)), ((\\n--696281)...  \n",
      "573452  (((\\n--120017)), ((\\n--237631)), ((\\n--1230383...  \n"
     ]
    }
   ],
   "source": [
    "humap_df = pd.DataFrame.from_dict(humap_profiles , orient = 'index')\n",
    "print(humap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################begin building the string dataset ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_coglinks = False\n",
    "if filter_coglinks == True:\n",
    "    coglink_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')\n",
    "    print(coglink_df)\n",
    "    print(coglink_df.columns)\n",
    "    dropcols = ['neighborhood', 'fusion', 'cooccurence', 'combined_score' ]\n",
    "    coglink_df = coglink_df.drop(columns = dropcols)\n",
    "    coglink_df['score'] = coglink_df.coexpression + coglink_df.experimental +coglink_df.database+ coglink_df.textmining\n",
    "    coglink_df= coglink_df[coglink_df.score>1000]\n",
    "    coglink_df = coglink_df.compute()\n",
    "    print(coglink_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the interacting cogs to the proteins\n",
    "compute_grabcogs = False\n",
    "if compute_grabcogs == True:\n",
    "    grabcogs = set( list(coglink_df.group1.unique()) + list(coglink_df.group2.unique()) )\n",
    "    grabcogs= list(grabcogs)\n",
    "    COGmapings_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt', blocksize=25e6 , header = 0, sep = '\\t')\n",
    "    COGmapings_df = COGmapings_df.set_index('orthologous_group')\n",
    "    COGmapings_df.astype(str)\n",
    "    COGmapings_df['##protein'].map( lambda x : x.strip() )\n",
    "    COGmapings_df['species'] = COGmapings_df['##protein'].map( lambda x : x.split('.')[0] )\n",
    "    COGmapings_df['COG'] = COGmapings_df.index\n",
    "    COGmapings_df = COGmapings_df.loc[grabcogs]\n",
    "    COGmapings_df = COGmapings_df.compute()\n",
    "    print(COGmapings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take the proteins in our cogs of interest\n",
    "if compute_grabcogs == True:\n",
    "    grabprots =list(COGmapings_df['##protein'].unique())\n",
    "    print(len(grabprots))\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabcogs ]) )\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabprots ]) )\n",
    "else:\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'r') as protsout:\n",
    "        grabcogs = [ cog for cog in protsout.readlines()]\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'r') as protsout:\n",
    "        grabprots = [ prot for prot in protsout.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_mappers = False\n",
    "rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "if calc_mappers == True:\n",
    "    count = 0 \n",
    "    for i,r in COGmapings_df.iterrows():\n",
    "        rdb.set(r['##protein'], i)\n",
    "        count+=1\n",
    "        if count < 10:\n",
    "            print(i+'\\n',r)\n",
    "        if count%1000000==0:\n",
    "            print(count/len(COGmapings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maphogs = False\n",
    "if maphogs == True:\n",
    "    #mapping each string cog to an oma hog by selecting a member of the cog\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    hogmap = {}\n",
    "    for i,prot in enumerate(grabprots):\n",
    "        if i % 100000 == 0 :\n",
    "            print(i/len(grabprots))\n",
    "        cog = rdb.get(prot)\n",
    "        if cog not in hogmap:\n",
    "            mapped =  grabHog(prot)\n",
    "            #retry until something maps\n",
    "            if mapped[0] != np.nan and type(mapped[0]) == str :\n",
    "                if len(mapped[0])>1 :\n",
    "                    hogmap[cog] = mapped\n",
    "    with open('stringhogmap.pkl' , 'wb')as hogmapout:\n",
    "        hogmapout.write(pickle.dumps(hogmap))\n",
    "else:\n",
    "    with open('stringhogmap.pkl' , 'rb')as hogmapout:\n",
    "        hogmap = pickle.loads(hogmapout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12761\n",
      "b'COG0022' ('HOG:A0772469', (1134780, 359776777, 338, 36877, b'HOG:A0772469', b'A', 136963, 137976, -1, 0, b'Q9K3H1', 1080465110, 1015, b'6161cbb7b6aed10547b7c95555c1c63a', 60128051, 322, b'', -1, -1))\n",
      "b'COG1071' ('HOG:A0780700.10b.7b', (3753108, 1179251817, 734, 598427, b'HOG:A0780700.10b.7b', b'A', 2952449, 2954650, -1, 0, b'F4GWD5', 3541508558, 2203, b'0eaa8538e27a234812d4b19c67f1f560', 234217123, 43, b'', -1, -1))\n",
      "b'COG0508' ('HOG:A0757171.22b.54b.29a', (3288989, 1034223672, 469, 598421, b'HOG:A0757171.22b.54b.29a', b'A', 1363913, 1365319, 1, 0, b'G4R8Y9', 3105960004, 1408, b'3b9ecc15f5881cc16928ba7e6a656fe6', 205095449, 48, b'', -1, -1))\n",
      "b'COG0462' ('HOG:A0780700', (14151364, 5935651032, 846, 598427, b'HOG:A0780700', b'supercontig_To_g48347', 2246, 4990, -1, 0, b'K0RIS8', 17821104459, 2539, b'8f2ac7bfb36b9ffe82ad5d8dc2410bae', 696068665, 22, b'', -1, -1))\n",
      "b'COG4886' ('HOG:A0779810.2c.30a.12a.2a', (16107232, 6695185680, 411, 598444, b'HOG:A0779810.2c.30a.12a.2a', b'1', 16009478, 16013321, 1, 0, b'A0A0A0LUF3', 20101664271, 1234, b'5ef6b97572ab9b6e5b14a390dee7e0b0', 775353403, 22, b'', -1, -1))\n",
      "b'KOG0836' ('HOG:A0340147', (7186393, 2441445398, 730, 232423, b'HOG:A0340147', b'supercontig_CCM_S00003', 3237807, 3240958, -1, 0, b'G3JGC8', 7331522586, 2191, b'0ec2bdd82de40b4a83db93dc6e49531c', 404397013, 22, b'', -1, -1))\n",
      "b'COG0025' ('HOG:A0645179.17f', (1135340, 359947462, 529, 807503, b'HOG:A0645179.17f', b'A', 120051, 121637, -1, 0, b'Q9S2Y0', 1080977725, 1588, b'8ae35ad6b3503cb02fe7286519489d30', 60305019, 797, b'', -1, -1))\n",
      "b'COG2905' ('HOG:A0792471', (3753076, 1179241622, 870, 810460, b'HOG:A0792471', b'A', 2919860, 2922469, -1, 0, b'F4GWA3', 3541477941, 2611, b'628cfc8860369d797b5b145e312368fd', 234216107, 40, b'', -1, -1))\n",
      "b'COG0569' ('HOG:A0743626.16j', (4647593, 1466328081, 602, 808080, b'HOG:A0743626.16j', b'A', 62922, 64727, -1, 0, b'A0A0H3FI71', 4403631835, 1807, b'306b5f8eb255c80fd14d7df21f3f257d', 286428690, 72, b'', -1, -1))\n",
      "b'COG0589' ('HOG:A0757574.22l', (1750695, 568860072, 544, 808054, b'HOG:A0757574.22l', b'Chromosome', 2723405, 2725036, 1, 0, b'Q8YUS0', 1708330910, 1633, b'4e54ac253f4a82b2e6da32dd095c5be2', 111675212, 25, b'', -1, -1))\n"
     ]
    }
   ],
   "source": [
    "print(len(hogmap))\n",
    "for i, key in enumerate(hogmap):\n",
    "    if i < 10:\n",
    "        print(key, hogmap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the HOGs to the COGdf\n",
    "#grab the corresponding profiles\n",
    "\n",
    "compile_final_cogdf = False\n",
    "if compile_final_cogdf == True:\n",
    "    print(len(coglink_df))\n",
    "    try:\n",
    "        coglink_df.group1  = coglink_df.group1.map( lambda x : x.encode())\n",
    "        coglink_df.group2  = coglink_df.group2.map( lambda x : x.encode())\n",
    "    except:\n",
    "        pass\n",
    "    coglink_df['hog1'] = coglink_df.group1.map(hogmap)\n",
    "    coglink_df['hog2'] = coglink_df.group2.map(hogmap)\n",
    "    coglink_df=coglink_df.dropna()\n",
    "    print(len(coglink_df))\n",
    "    print(coglink_df.head())\n",
    "    coglink_df['hogid_1'] = coglink_df['hog1'].map(lambda x:x[0])\n",
    "    coglink_df['hogid_2'] = coglink_df['hog2'].map(lambda x:x[0])\n",
    "    coglink_df['fam1'] = coglink_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    coglink_df['fam2'] = coglink_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    coglink_df.fam1 = coglink_df.fam1.map(int)\n",
    "    coglink_df.fam2 = coglink_df.fam2.map(int)\n",
    "    stringHOGs = set(coglink_df.fam1.unique()).union(set(coglink_df.fam2.unique()))\n",
    "    print(len(stringHOGs))\n",
    "    print(coglink_df)\n",
    "    coglink_df.to_csv('STRINGCOGS2OMAHOGS.csv')\n",
    "else:\n",
    "    coglink_df = pd.read_csv('STRINGCOGS2OMAHOGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringPairs = coglink_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive explicit profiles for our hogs of interest in string\n",
    "calc_hogs_string = False\n",
    "stringprofiles = {}\n",
    "if calc_hogs_string == True:\n",
    "    print('profiles to calclulate',len(stringHOGs))\n",
    "    for i,fam in enumerate(stringHOGs):\n",
    "        if i % 100 ==0:\n",
    "            print(i)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "            stringprofiles.update(prof)\n",
    "        except:\n",
    "            print('err',fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_hogs_string == True:\n",
    "    with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(stringprofiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'rb' )as profiles_out:\n",
    "    stringprofiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = pd.DataFrame.from_dict(stringprofiles , orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786432 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12ba00b76)}\n",
      "786433 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12b9d07ac)}\n",
      "524294 {'mat': array([[0., 0., 0., ..., 0., 0., 0.]]), 'tree': Tree node '7742' (0x7f12b9a2670)}\n",
      "786440 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12b99ccfa)}\n",
      "524299 {'mat': array([[0., 0., 0., ..., 0., 0., 0.]]), 'tree': Tree node '7742' (0x7f12b966cbe)}\n",
      "786448 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12b96a44c)}\n",
      "786449 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12b834310)}\n",
      "786450 {'mat': array([[1., 1., 1., ..., 0., 0., 0.]]), 'tree': Tree node '131567' (0x7f12b804dd0)}\n",
      "131092 {'mat': array([[0., 0., 0., ..., 0., 0., 0.]]), 'tree': Tree node '1648030' (0x7f12b38c394)}\n",
      "524309 {'mat': array([[0., 0., 0., ..., 0., 0., 0.]]), 'tree': Tree node '7742' (0x7f12b38c3a6)}\n"
     ]
    }
   ],
   "source": [
    "#make the profiles for this small set of HOGs\n",
    "for i, key in enumerate(stringprofiles):\n",
    "    if i < 10:\n",
    "        print(key,stringprofiles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have profiles for all HUMAP and COG interactions\n",
    "#String has interactions from each COG in different species.\n",
    "#We need a way to check for the presence of interaction within a species for a COG\n",
    "#for this we will create a bloom filter with all the interactions between our cogs\n",
    "\n",
    "calc_filter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    from dask.distributed import fire_and_forget\n",
    "    from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    from dask.distributed import  utils_perf\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    import dask\n",
    "    import redis\n",
    "    from bloom_filter2 import BloomFilter\n",
    "    import lzma\n",
    "    from dask import dataframe as dd\n",
    "    distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        NCORE = 5\n",
    "        print('deploying cluster')\n",
    "        cluster = SLURMCluster(\n",
    "            walltime='4:00:00',\n",
    "            n_workers = NCORE,\n",
    "            cores=NCORE,\n",
    "            processes = NCORE,\n",
    "            interface='ib0',\n",
    "            memory=\"80GB\",\n",
    "            env_extra=[\n",
    "            'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "            'conda activate ML2'\n",
    "            ],\n",
    "            scheduler_options={'interface': 'ens2f0' },\n",
    "            #extra=[\"--lifetime\", \"3h55m\", \"--lifetime-stagger\", \"4m\"]\n",
    "        )\n",
    "        print(cluster.job_script())\n",
    "\n",
    "    else:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        print(cluster)\n",
    "        cluster.scale(jobs = 100)\n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "if calc_filter == True:\n",
    "    link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.physical.links.detailed.v11.5.txt', blocksize=100e6 , header = 0, sep = ' ')\n",
    "    print(link_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "@dask.delayed\n",
    "def mapcogs(df ):\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    if type( df ) == tuple:\n",
    "        df = df[0]\n",
    "    protlist1 = list(df.protein1.map(lambda x:str(x).strip()))\n",
    "    protlist2 = list(df.protein2.map(lambda x:str(x).strip()))\n",
    "    protlist = list(set(protlist1+protlist2))\n",
    "    data = rdb.mget(protlist)\n",
    "    mapper = dict(zip(protlist, data) )\n",
    "    df['COG1'] = df.protein1.map(mapper)\n",
    "    df['COG2'] = df.protein2.map(mapper)\n",
    "    df = df.dropna()\n",
    "    df['COG1'] = df.COG1.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['COG2'] = df.COG2.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['species'] = df.protein1.map(lambda x:x.split('.')[0])\n",
    "    df['coglinks'] = df.COG1 + '_' + df.COG2 + '_' + df.species\n",
    "    return  list(df.coglinks.unique())\n",
    "\n",
    "@dask.delayed\n",
    "def return_filter(coglinks, verbose = True):\n",
    "    if type( coglinks ) == tuple:\n",
    "        coglinks = coglinks[0]\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in coglinks:\n",
    "        b.add( p )\n",
    "    return   b , len(coglinks)\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    f3 = f1.__ior__(f2)\n",
    "    return f3 , total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    partitions  = link_df.to_delayed()\n",
    "    print('map cogs')\n",
    "    res1 = [ mapcogs(p) for p in partitions ] \n",
    "    print('done')\n",
    "    print('make filters')\n",
    "    res2 = [ return_filter(p) for p in res1 ] \n",
    "    totalfilter = res2\n",
    "    print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        print(len(totalfilter))\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    resfinal = dask.compute(totalfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cancel(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    with open('bloomfinal.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(resfinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bloomfinal.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "cogfilter =  resfinal[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('COG1756_COG0088_4113' in cogfilter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try it out. we should be able to find in which species two cogs interact with the bloom filter\n",
    "cog1='COG0088'\n",
    "cog2 ='COG1756'\n",
    "coglink = cog1 + '_' + cog2 + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COG0088_COG1756_768706', 'COG0088_COG1756_282458', 'COG0088_COG1756_367830', 'COG0088_COG1756_655816', 'COG0088_COG1756_481743', 'COG0088_COG1756_370552', 'COG0088_COG1756_891391', 'COG0088_COG1756_479436', 'COG0088_COG1756_443150', 'COG0088_COG1756_1768', 'COG0088_COG1756_1790', 'COG0088_COG1756_504474', 'COG0088_COG1756_1032480', 'COG0088_COG1756_1214101', 'COG0088_COG1756_2064', 'COG0088_COG1756_471855', 'COG0088_COG1756_292563', 'COG0088_COG1756_502801', 'COG0088_COG1756_585035', 'COG0088_COG1756_439855', 'COG0088_COG1756_481805', 'COG0088_COG1756_272620', 'COG0088_COG1756_561230', 'COG0088_COG1756_1905730', 'COG0088_COG1756_243265', 'COG0088_COG1756_529507', 'COG0088_COG1756_718251', 'COG0088_COG1756_382245', 'COG0088_COG1756_1167006', 'COG0088_COG1756_592205', 'COG0088_COG1756_235279', 'COG0088_COG1756_582899', 'COG0088_COG1756_288000', 'COG0088_COG1756_316057', 'COG0088_COG1756_696125', 'COG0088_COG1756_1082931', 'COG0088_COG1756_955', 'COG0088_COG1756_205920', 'COG0088_COG1756_357244', 'COG0088_COG1756_696127', 'COG0088_COG1756_634452', 'COG0088_COG1756_626418', 'COG0088_COG1756_375286', 'COG0088_COG1756_860228', 'COG0088_COG1756_688270', 'COG0088_COG1756_983548', 'COG0088_COG1756_444179', 'COG0088_COG1756_331104', 'COG0088_COG1756_672161', 'COG0088_COG1756_608538', 'COG0088_COG1756_1173701', 'COG0088_COG1756_148305', 'COG0088_COG1756_665079', 'COG0088_COG1756_436907', 'COG0088_COG1756_4829', 'COG0088_COG1756_9371', 'COG0088_COG1756_105023', 'COG0088_COG1756_80972', 'COG0088_COG1756_7263', 'COG0088_COG1756_7460', 'COG0088_COG1756_3871', 'COG0088_COG1756_3352', 'COG0088_COG1756_3197', 'COG0088_COG1756_44056', 'COG0088_COG1756_1093141', 'COG0088_COG1756_5857', 'COG0088_COG1756_312017', 'COG0088_COG1756_857967', 'COG0088_COG1756_353153', 'COG0088_COG1756_666510', 'COG0088_COG1756_439386', 'COG0088_COG1756_877455', 'COG0088_COG1756_523846', 'COG0088_COG1756_593750', 'COG0088_COG1756_224325', 'COG0088_COG1756_673860']\n",
      "['768706', '282458', '367830', '655816', '481743', '370552', '891391', '479436', '443150', '1768', '1790', '504474', '1032480', '1214101', '2064', '471855', '292563', '502801', '585035', '439855', '481805', '272620', '561230', '1905730', '243265', '529507', '718251', '382245', '1167006', '592205', '235279', '582899', '288000', '316057', '696125', '1082931', '955', '205920', '357244', '696127', '634452', '626418', '375286', '860228', '688270', '983548', '444179', '331104', '672161', '608538', '1173701', '148305', '665079', '436907', '4829', '9371', '105023', '80972', '7263', '7460', '3871', '3352', '3197', '44056', '1093141', '5857', '312017', '857967', '353153', '666510', '439386', '877455', '523846', '593750', '224325', '673860']\n"
     ]
    }
   ],
   "source": [
    "#test out to find the species for a cog pair\n",
    "coglinks_species = [ coglink+spec.name  for spec in p.tree.get_leaves()]\n",
    "species = [ spec.name  for spec in p.tree.get_leaves()]\n",
    "checklinks = [ coglink+spec in cogfilter for spec in coglinks_species ]\n",
    "species_set = [s for s in species]\n",
    "links = [ l for l,c in list(zip(coglinks_species,checklinks))  if c== True  ]\n",
    "species_set = [ s for s,c in list(zip(species_set,checklinks))  if c== True  ]\n",
    "print(links)\n",
    "print(species_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3145, 10524)\n",
      "(11788, 10524)\n",
      "string\n",
      "train 389499\n",
      "test 97133\n",
      "humap\n",
      "train 6592\n",
      "test 1701\n"
     ]
    }
   ],
   "source": [
    "#mapfam to matrow\n",
    "humap_fam_map= { f:i for i,f in enumerate(humap_df.index)}\n",
    "humap_profilemat = np.vstack(humap_df.mat)\n",
    "print(humap_profilemat.shape)\n",
    "string_fam_map = { f:i for i,f in enumerate(string_df.index)}\n",
    "string_mat = np.vstack(string_df.mat)\n",
    "print(string_mat.shape)\n",
    "#train test split\n",
    "Datasets = {}\n",
    "for label,df,mapping,profilemat in [  ('string', stringPairs, string_fam_map ,string_mat ) , ('humap',humap_pairs, humap_fam_map , humap_profilemat) ]: \n",
    "    keys = set(mapping.keys())\n",
    "    entry1 = [ f in keys for f in df.fam1]\n",
    "    df = df.iloc[entry1]\n",
    "    entry2 = [ f in keys for f in df.fam2]\n",
    "    df = df.iloc[entry2]\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    df_train = df.iloc[msk]\n",
    "    df_test = df.iloc[~msk]\n",
    "    Datasets[label]={'Train':df_train,'Test':df_test , 'mapping': mapping , 'mat':profilemat }\n",
    "    print(label)\n",
    "    print('train',len(df_train))\n",
    "    print('test',len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(df, n):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i + n]\n",
    "def generateXYchunk(explicit_profiles, goldstandardDF,fam_map,  nsamples=100, posi_percent = .5):\n",
    "    #shuffle\n",
    "    goldstandardDF = goldstandardDF.sample(frac=1)\n",
    "    for chunkdf in chunks(goldstandardDF , int( nsamples*posi_percent)):\n",
    "        #negatives drawn from the overall dataset\n",
    "        X = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam1]) , np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam2]) ] )\n",
    "        Y = [1]* X.shape[0]\n",
    "        neg1 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam1)\n",
    "        neg2 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam2)\n",
    "        \n",
    "        if len(neg1)>0:\n",
    "            mixchunk = np.hstack([np.vstack([profilemat[fam_map[f]] for f in neg1]),np.vstack([profilemat[fam_map[f]] for f in neg2])])\n",
    "            Y =np.hstack([[0]* mixchunk.shape[0] , Y])\n",
    "            X= np.vstack([mixchunk,X])    \n",
    "        #positive samples\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def ROC_curve(y_data, label = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot( recall, precision , label= l )\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 00:20:01.558782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-17 00:20:01.558809: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#try a vanilla deep NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "overwrite = True\n",
    "train_dnn = False\n",
    "\n",
    "if train_dnn == True:\n",
    "    for dataset in Datasets:\n",
    "        modelpath = './'+dataset+'_dropout_DNN.h5'\n",
    "        callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= modelpath+'.logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=modelpath)\n",
    "        ]\n",
    "        print(dataset)\n",
    "        df_train = Datasets[label]['Train']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        if os.path.exists(modelpath) and overwrite == False:\n",
    "            model = load_model(modelpath)\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=100, activation='sigmoid', input_dim=profilemat.shape[1]*2))\n",
    "            model.add(tf.keras.layers.Dropout( .2 , seed=42 ))\n",
    "            model.add(Dense(units=30, activation='sigmoid' ) )\n",
    "            model.add(Dense(units=1, activation='sigmoid' ) )\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        generator = generateXYchunk(profilemat, df_train, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        sample = next(generator)\n",
    "        model.fit(itertools.cycle(generator) , steps_per_epoch = 300 , epochs = 50, callbacks=callbacks)\n",
    "        # Save the model\n",
    "        model.save(modelpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal distance metrics to try\n",
    "\n",
    "from scipy.spatial.distance import euclidean , hamming, jaccard\n",
    "from sklearn.covariance import empirical_covariance\n",
    "from scipy.stats import pearsonr\n",
    "from keract import get_activations\n",
    "def pearsonR(v1,v2):\n",
    "        return -pearsonr(v1,v2)[0]\n",
    "\n",
    "visualize = False    \n",
    "if visualize == True:\n",
    "    for label in Datasets:\n",
    "        print(label)\n",
    "        df_test = Datasets[label]['Test']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        ydata =  {}\n",
    "\n",
    "        for func, name in [ (euclidean, 'Euclidean' ), (hamming,'Hamming') , (jaccard,'Jaccard') , (pearsonR,'Pearson') ]:\n",
    "            print(name)\n",
    "            generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "            #test all the easy metrics\n",
    "            ypreds = []\n",
    "            ytruth = []\n",
    "            for X,y in generator:\n",
    "                #distances\n",
    "                x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                x2 = X[:,int(X.shape[1]/2):]\n",
    "                predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                ypreds.append(predictions)\n",
    "                ytruth.append(y)\n",
    "            ytest= np.hstack(ytruth)\n",
    "            ypred = np.hstack(ypreds)\n",
    "            ydata[name] = { 'Ypred': ypred , 'Ytrue':ytest} \n",
    "        #get DNN values\n",
    "        print('DNN')\n",
    "        generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        mats = [(x,y) for x,y in generator ]\n",
    "        y_test = np.hstack([y for x,y in mats])\n",
    "        modelpath = './'+label+'_dropout_DNN.h5'\n",
    "        if os.path.exists(modelpath):\n",
    "            model = load_model(modelpath)\n",
    "\n",
    "\n",
    "        ypred = np.vstack([ model.predict(x) for x,y in mats])\n",
    "\n",
    "\n",
    "        #activations = [ get_activations(model, x , auto_compile=True) for x,y in mats] \n",
    "        #activations = activations[list(activations.keys())[0]]\n",
    "        #print( activations.shape)\n",
    "        #n_samples = activations.shape[0]\n",
    "        #activations = np.sum(activations,axis =0)/n_samples\n",
    "        #representations = np.sum(X_test, axis = 0)\n",
    "\n",
    "        #print(activations.shape)\n",
    "\n",
    "        #np.save(modelpath + 'activation.np' , activations)\n",
    "        #np.save(modelpath + 'representation.np', representations)\n",
    "\n",
    "        ydata['DNN'] ={ 'Ypred': ypred , 'Ytrue':ytest}\n",
    "        #plot ROC\n",
    "        print(ydata)\n",
    "        ROC_curve(ydata , label = label)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################being the graph NN part of the paper #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "#create graphs on the fly to represent pairs of profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dendropy\n",
    "taxnwk = '/scratch/dmoi/datasets/birds/all_test_master_tree.nwk'\n",
    "with open( 'taxtree.nwk' , 'w') as treeout:\n",
    "    treeout.write(p.tree.write())\n",
    "dendrotree = dendropy.Tree.get(\n",
    "        data=p.tree.write(format=3),\n",
    "        rooting=\"force-rooted\",\n",
    "        suppress_internal_node_taxa=False,\n",
    "        schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set([ n.name for n in p.tree.traverse() ])\n",
    "dendrotree_nodes = set([str(n.taxon.label) if n.taxon else '-1' for n in dendrotree.nodes()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508\n",
      "3508\n",
      "3508\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes))\n",
    "print(len(dendrotree_nodes))\n",
    "print(len(nodes.intersection(dendrotree_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { n.name:i for i,n in enumerate(p.tree.traverse()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(dendrotree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { (n.taxon.label if n.taxon else '-1'):n.matrow for n in  dendrotree.nodes() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smallpars\n",
    "#we're checking for interaction in a subset of species and propagating up\n",
    "allowed_symbols =set([0,1])\n",
    "transition_dict = { (c[0],c[1]):i for i,c in enumerate(itertools.permutations(allowed_symbols,2) ) }\n",
    "def calc_interaction_on_taxonomy(cog1,cog2,tree, bloom , verbose = False):\n",
    "    #set interaction states\n",
    "    #look for interactions in bloom\n",
    "    for i,l in enumerate(tree.leaf_nodes()):\n",
    "        l.event = {}\n",
    "        l.scores = {}\n",
    "        l.symbols = {}\n",
    "        l.scores = { c:10**10 for c in allowed_symbols }\n",
    "        if cog1+'_'+cog2+'_'+l.taxon.label in bloom:\n",
    "            if verbose == True:\n",
    "                print(l.taxon.label)\n",
    "            l.char= 1\n",
    "            l.symbols = {1}\n",
    "        else:\n",
    "            l.char= 0\n",
    "            l.symbols = {0}\n",
    "        l.scores[l.char] = 0\n",
    "    t = smallpars.calculate_small_parsimony(tree ,allowed_symbols, transition_dict)\n",
    "    return  t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2Single_sparse_graph(tree):\n",
    "    N = len(tree.nodes())\n",
    "    #mimic the fitch algo\n",
    "    #propagate up and down in separate graphs\n",
    "    index = np.hstack([ [[c.matrow, n.matrow ],[ N+n.matrow, N+c.matrow ],[c.matrow,N+c.matrow],[N+c.matrow,c.matrow]]\n",
    "                      for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat = scipy.sparse.lil_matrix(( 2*N ,  2*N ) )\n",
    "    #integration\n",
    "    connectmat[index[:,0],index[:,1]] = 1 \n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    template_features = np.stack([ntime ,  Norm_nchild  ]).T\n",
    "    \n",
    "    template_features = np.vstack([template_features , template_features  ])\n",
    "    \n",
    "    return connectmat, template_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def getmrca(treein,taxset):\n",
    "    tree = copy.deepcopy(treein)\n",
    "    n = tree.mrca(taxon_labels=taxset)\n",
    "    subtree = dendropy.Tree(seed_node=n)\n",
    "    taxset = set([ t.taxon.label if n.taxon else '-1'  for t in subtree.nodes()])\n",
    "    matrows = [ t.matrow for t in subtree.nodes()]\n",
    "    return taxset, matrows , n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data( tree, coglinkdf, bloom, profiles ,  fam_map , taxindex , posi_percent= .5 , verbose = False, loop= True):\n",
    "        connectmat, template_features = tree2Single_sparse_graph(tree)\n",
    "        num_total_nodes = template_features.shape[0]\n",
    "        print('dataset size' , len(coglinkdf))\n",
    "        lineiterator = coglinkdf.iterrows()\n",
    "        N = len(tree.nodes())\n",
    "        #grab signal from the downward propagation\n",
    "        leafnodes = np.array([N+n.matrow for n in tree.leaf_nodes()])\n",
    "        overview_connect = scipy.sparse.lil_matrix(( 2*N , 2 ) )\n",
    "        overview_connect[leafnodes,0] = 1\n",
    "        godnode_connect= np.array([[0,1],[1,0]])\n",
    "        allfams = list(set(coglinkdf.fam1.unique()).union( set(coglinkdf.fam2.unique() ) ))\n",
    "        while True:\n",
    "            toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "            if verbose == True:\n",
    "                print('posi/nega',toss)\n",
    "            if toss == 0:\n",
    "                fam1 = random.choice(allfams)\n",
    "                fam2 = fam1\n",
    "                while fam1 == fam2:\n",
    "                    fam2 = random.choice(allfams)\n",
    "                labels = np.zeros((template_features.shape[0],))\n",
    "            else:\n",
    "                #positive sample\n",
    "                idx,dfline = next(lineiterator)\n",
    "                cog1= str(dfline.group1).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                cog2= str(dfline.group2).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                fam1 = dfline.fam1\n",
    "                fam2 = dfline.fam2\n",
    "                #get the features from fitch and profile\n",
    "                t = calc_interaction_on_taxonomy(cog1,cog2,tree, bloom)\n",
    "                labels = np.array( 2*[ n.char for n in t.nodes() ] )                \n",
    "            profile1 = profiles[fam1]['tree']\n",
    "            profile2 = profiles[fam2]['tree']\n",
    "            presence1 = set([ n.name  for n in profile1.get_leaves() if n.nbr_genes > 0  ])\n",
    "            presence2 = set([ n.name  for n in profile2.get_leaves() if n.nbr_genes > 0  ])\n",
    "            #restrict graph to clade where the proteins are found\n",
    "            skip = False\n",
    "            try:\n",
    "                taxset,matrows,n = getmrca(tree,presence1.intersection(presence2))\n",
    "            except ValueError:\n",
    "                #no species overlap\n",
    "                skip = True\n",
    "            if skip == False:\n",
    "                matrows1 = [m +N for m in matrows]\n",
    "                matrows1+=matrows            \n",
    "                subconnect = connectmat[matrows1,:]\n",
    "                subconnect = subconnect[:,matrows1]\n",
    "                suboverview = overview_connect[N+np.array(matrows),:]\n",
    "                subconnect = scipy.sparse.find(subconnect)\n",
    "                suboverview = scipy.sparse.find(suboverview)\n",
    "                subconnect = np.vstack([subconnect[0],subconnect[1]])\n",
    "                suboverview = np.vstack([suboverview[0],suboverview[1]])\n",
    "                nodefeatures = []\n",
    "                for i,tp in enumerate([profile1,profile2]):    \n",
    "                    profilefeatures = np.zeros((template_features.shape[0],3) )\n",
    "                    #find on which nodes the events happened\n",
    "                    losses = [ taxindex[n.name]  for n in tp.traverse() if n.lost and n.name in taxindex and n.name in taxset ]\n",
    "                    dupl = [ taxindex[n.name]  for n in tp.traverse() if n.dupl  and n.name in taxindex and n.name in taxset  ]\n",
    "                    presence = [ taxindex[n.name]  for n in tp.traverse() if n.nbr_genes > 0  and n.name in taxindex and n.name in taxset ]\n",
    "                    profilefeatures[losses, 0] = 1\n",
    "                    profilefeatures[dupl, 1] = 1\n",
    "                    profilefeatures[presence, 2] = 1\n",
    "                    nodefeatures.append(profilefeatures)\n",
    "                nodefeatures=np.hstack(nodefeatures)\n",
    "                #prune features\n",
    "                sub_template_features= template_features[matrows1,:]\n",
    "                sub_node_features= nodefeatures[matrows1,:]\n",
    "                nodefeatures = np.hstack([sub_template_features,sub_node_features])\n",
    "                labels = labels[matrows1]\n",
    "                if verbose == True:\n",
    "                    print('features',nodefeatures)\n",
    "                    print( 'labels' , labels)\n",
    "                data = HeteroData()            \n",
    "                data['phylonodes'].x = torch.tensor(nodefeatures, dtype=torch.double )\n",
    "                data['phylonodes', 'phylolink', 'phylonodes'].edge_index = torch.tensor(subconnect ,  dtype=torch.long )\n",
    "                data['phylonodes', 'informs', 'godnode'].edge_index = torch.tensor(suboverview,  dtype=torch.long )\n",
    "                #data['godnode', 'integrates' , 'godnode'].edge_index = torch.tensor(godnode_connect,  dtype=torch.long )\n",
    "                data['godnode'].x =torch.tensor(  np.zeros((1,nodefeatures.shape[1]))  ,  dtype=torch.double )\n",
    "                data['phylonodes'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "                data['godnode'].y =torch.tensor( np.ones((1,))*toss  ,  dtype=torch.long )\n",
    "                data = T.AddSelfLoops()(data)\n",
    "                data = T.NormalizeFeatures()(data)\n",
    "                yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 389499\n"
     ]
    }
   ],
   "source": [
    "Traindata = create_data(dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "data = next(Traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mphylonodes\u001b[0m={\n",
      "    x=[180, 8],\n",
      "    y=[180]\n",
      "  },\n",
      "  \u001b[1mgodnode\u001b[0m={\n",
      "    x=[1, 8],\n",
      "    y=[1]\n",
      "  },\n",
      "  \u001b[1m(phylonodes, phylolink, phylonodes)\u001b[0m={ edge_index=[2, 180] },\n",
      "  \u001b[1m(phylonodes, informs, godnode)\u001b[0m={ edge_index=[2, 53] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('phylonodes', 'phylolink', 'phylonodes'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                #('godnode', 'lordsover', 'phylonodes'): SAGEConv((-1, -1), hidden_channels),\n",
    "                #('godnode', 'integrates' , 'godnode'):SAGEConv((-1, -1), hidden_channels),\n",
    "                ('phylonodes', 'informs', 'godnode'):SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            \n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            \n",
    "            x_dict = {key: self.lin(x) for key, x in x_dict.items()}\n",
    "        #1d output for each node\n",
    "        return {key: F.log_softmax(x,dim=1) for key, x in x_dict.items()}\n",
    "\n",
    "model = HeteroGNN(hidden_channels=20, out_channels=20,\n",
    "                  num_layers=2)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict ,data.edge_index_dict)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=10**-4)\n",
    "optimizer=torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=10**-4, momentum=0.01, centered=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 97133\n"
     ]
    }
   ],
   "source": [
    "Testdata = create_data(dendrotree, Datasets['string']['Test'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "testchunk = [next(Testdata) for i in range(100) ]\n",
    "test_dataset = DataLoader(testchunk , batch_size = 10)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = 50\n",
    "\n",
    "for epoch in range(200):\n",
    "    try:\n",
    "        datachunk = [next(Traindata) for i in range(100) ]\n",
    "    except:\n",
    "        Traindata = create_data(dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "        datachunk = [next(Traindata) for i in range(100) ]\n",
    "    dataset = DataLoader(datachunk , batch_size =100)\n",
    "    model.train()\n",
    "    losses1=[]\n",
    "    losses2 =[]\n",
    "    for loop in range(loops):\n",
    "        for data in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            #loss = F.nll_loss(out, data.y)\n",
    "            loss1 =  F.nll_loss(out['phylonodes'], data['phylonodes'].y)\n",
    "            loss2 =  F.nll_loss(out['godnode'], data['godnode'].y)\n",
    "            loss = loss1 + loss2\n",
    "            losses2.append(float(loss2))\n",
    "            losses1.append(float(loss1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(np.mean(losses2))\n",
    "        if loop%10 == 0 :\n",
    "            print(f'loss1: {np.mean(losses1):.4f}')\n",
    "            print(f'loss2: {np.mean(losses2):.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    gncorrects=[]\n",
    "    ncorrects=[]\n",
    "    composition_gn = []\n",
    "    composition_n = []\n",
    "    \n",
    "    for testdata in test_dataset: \n",
    "        pred = model(data.x_dict ,data.edge_index_dict)\n",
    "        correct = (pred['godnode'].argmax(dim=1) == data['godnode'].y).sum()\n",
    "        composition_gn.append(data['godnode'].y.sum() / data['godnode'].y.shape[0] )\n",
    "        gncorrects.append(correct)\n",
    "        correct = (pred['phylonodes'].argmax(dim=1) == data['phylonodes'].y).sum()\n",
    "        ncorrects.append(correct)\n",
    "        composition_n.append(data['phylonodes'].y.sum() / data['phylonodes'].y.shape[0] )\n",
    "\n",
    "    \n",
    "    acc = np.mean(gncorrects) / int(pred['godnode'].shape[0])\n",
    "    print(f'gn Accuracy: {acc:.4f}', len(gncorrects), pred['godnode'].shape[0] )\n",
    "    print(f'gn composition: {np.mean(composition_gn) :.4f}', )\n",
    "    \n",
    "    acc = np.mean(ncorrects) / int(pred['phylonodes'].shape[0])\n",
    "    print(f'n Accuracy: {acc:.4f}', len(ncorrects) , pred['phylonodes'].shape[0] )\n",
    "    print(f'n composition: {np.mean(composition_n) :.4f}', )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
