{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recycle RNN2struct for the purpose of GO pred. Use distmat data w aln data from pdb70\n",
    "#use cath mapping for GO terms\n",
    "#2 outputs. train on cath pt sequence and distmat pca\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler,  Normalizer , MinMaxScaler , RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "epsilon = K.epsilon()\n",
    "from io import BytesIO, StringIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import argparse\n",
    "\n",
    "import keras\n",
    "global max_epochs\n",
    "\n",
    "\n",
    "flag_show_plots = True # True for Notebooks, False otherwise\n",
    "if flag_show_plots:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.pyplot import figure\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Input, Dense , Bidirectional , ConvLSTM2D , concatenate , Reshape , CuDNNGRU , subtract , SpatialDropout2D\n",
    "\n",
    "from keras import backend\n",
    "from keras.callbacks import Callback as Callback\n",
    "from keras.backend import tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "################################################################################\n",
    "dirlocal = './'\n",
    "dataset = 'full' # 'sample' or 'full'\n",
    " \n",
    "modelfile = 'model-' + str(stamp) + '.h5'\n",
    "\n",
    "max_epochs = 64\n",
    "es_patience = 100\n",
    "\n",
    "if dataset == 'sample':\n",
    "    max_epochs = 8\n",
    "    es_patience = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NDSRobust(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = RobustScaler(copy=True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.inverse_transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "    \n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "\n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X\n",
    "\n",
    "\n",
    "    \n",
    "class NDSPCA(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self._scaler = PCA(copy = True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        \n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        self.explained_variance_ratio_ = self._scaler.explained_variance_ratio_\n",
    "        self.components_ =self._scaler.components_\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.inverse_transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            n_dims = np.prod(self._orig_shape)\n",
    "            X = X.reshape(-1, n_dims)\n",
    "        return X\n",
    "\n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X\n",
    "    \n",
    "\n",
    "def fit_y( y , components = 300 , FFT = False ):\n",
    "    if FFT == True:\n",
    "        y = np.stack([ np.fft.rfft2(y[i,:,:]) for i in range(y.shape[0])] )\n",
    "        print(y.shape)\n",
    "        y =  np.hstack( [ np.real(y) , np.imag(y)]  )\n",
    "    print(y.shape)\n",
    "    ndpca = NDSPCA(n_components=components)\n",
    "    ndpca.fit(y)\n",
    "    print('explained variance')\n",
    "    print(np.sum(ndpca.explained_variance_ratio_))\n",
    "    y = ndpca.transform(y)\n",
    "    scaler0 = RobustScaler( )\n",
    "    scaler0.fit(y)\n",
    "    return scaler0, ndpca \n",
    "\n",
    "def transform_y( y, scaler,ndpca , FFT = False ):\n",
    "    if FFT == True:\n",
    "        y = np.stack([ np.fft.rfft2(y[i,:,:]) for i in range(y.shape[0])] )\n",
    "        print(y.shape)\n",
    "        y =  np.hstack( [ np.real(y) , np.imag(y)]  )\n",
    "    y = ndpca.transform(y)\n",
    "    print(y.shape)\n",
    "    y = scaler0.transform(y)\n",
    "    \n",
    "    return y \n",
    "\n",
    "def inverse_transform_y( y , scaler, ndpca , FFT=False):\n",
    "    \n",
    "    y = scaler0.inverse_transform(y)\n",
    "    y = ndpca.inverse_transform(y)\n",
    "\n",
    "    if FFT == True:\n",
    "        split = int(y.shape[1]/2)\n",
    "        y = np.stack([ np.fft.irfft2(y[i,:split,:] + 1j*y[i,split:,:]) for i in range(y.shape[0]) ] )\n",
    "                  \n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### todo, get MSAs, get pdb distmats, get pdb GO\n",
    "\n",
    "\n",
    "\n",
    "### time distributed layer for aln in\n",
    "\n",
    "\n",
    "### \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction= 0.90\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "#systematicall probe net arch\n",
    "#insert bottleneck in dense\n",
    "\n",
    "#first part of the network, stacked GRU\n",
    "LSTMoutdim_max = 500\n",
    "LSTMoutdim_min = 500\n",
    "\n",
    "LSTMlayers = 2\n",
    "\n",
    "Densegrulayers = 2\n",
    "Densegruoutdim_min = 300\n",
    "Densegruoutdim_max = 300\n",
    "\n",
    "retrain = False\n",
    "print(XTRAINrows.shape)\n",
    "print(XVALIDrows.shape)\n",
    "\n",
    "def bottleneck( minn, maxn, layers ):\n",
    "    split = int(layers/2)\n",
    "    slope = (minn - maxn) / split\n",
    "    units =[maxn]    \n",
    "    last = maxn\n",
    "    for k in range(split):\n",
    "        last += slope \n",
    "        units.append(int(last))\n",
    "    slope = -slope\n",
    "    for k in range(split):\n",
    "        last += slope \n",
    "        units.append(int(last))\n",
    "    return units\n",
    "\n",
    "lstmbottle = bottleneck(LSTMoutdim_min , LSTMoutdim_max  , LSTMlayers )\n",
    "bottle = bottleneck(Densegruoutdim_min , Densegruoutdim_max  ,Densegrulayers )\n",
    "print(lstmbottle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "retrain = True\n",
    "if retrain == False:\n",
    "    inputrnn = Input(name='Seqin', shape=( XTRAINrows.shape[1] ,XTRAINrows.shape[2] ) )\n",
    "    layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "    x = layer(inputrnn)\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    for b in lstmbottle:\n",
    "            #layer = CuDNNGRU(LSTMoutdim ,  name='gru_'+str(n),\n",
    "            #return_sequences=True, return_state=False, go_backwards=False, stateful=False )\n",
    "            \n",
    "            layer = CuDNNLSTM(b , kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                    recurrent_constraint=None, bias_constraint=None, return_sequences=True, return_state=False, stateful=False)\n",
    "            \n",
    "            layer = Bidirectional(layer, merge_mode='concat', weights=None)\n",
    "            x = layer(x)\n",
    "            \n",
    "            layer = Dropout(.5 , noise_shape=None, seed=None)\n",
    "            x = layer(x)\n",
    "            \n",
    "            layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "            x = layer(x)\n",
    "\n",
    "    #layer = CuDNNGRU(LSTMoutdim ,  name='gru_'+str(n+1),\n",
    "    #return_sequences=True, return_state=False, go_backwards=False, stateful=False )\n",
    "    \n",
    "    #todo... regularizers \n",
    "    layer = CuDNNLSTM( LSTMoutdim_max , kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                              bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                              recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                              recurrent_constraint=None, bias_constraint=None, return_sequences=True, return_state=False, stateful=False)\n",
    "    x = layer(x)\n",
    "    layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "    x = layer(x)\n",
    "\n",
    "    x =  Flatten()(x)\n",
    "    layer = Dropout(.5 , noise_shape=None, seed=None)\n",
    "    x = layer(x)\n",
    "    \n",
    "    layer = Dense( YTRAINpca.shape[1] , activation='linear'  )\n",
    "    x = layer(x)\n",
    "    \n",
    "    \n",
    "    layer = Dense( YTRAINpca.shape[1] , activation='linear'  )\n",
    "    xpre= layer(x)\n",
    "    \n",
    "    \n",
    "    #final transform to img\n",
    "    layer = Dense( YTRAIN.shape[2] * YTRAIN.shape[1] , activation='linear'  )\n",
    "    x = layer(xpre)\n",
    "    \n",
    "    layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "    x = layer(x)\n",
    "    \n",
    "    layer = Reshape( ( YTRAIN.shape[1], YTRAIN.shape[2] ))\n",
    "    xfinal = layer(x)\n",
    "    \n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        model = Model(inputs = [inputrnn] , outputs = [xpre,xfinal] )\n",
    "    o = keras.optimizers.Adadelta(lr=.0001, rho=0.95)\n",
    "    model.compile( optimizer=o, loss= 'mean_absolute_percentage_error' , metrics=['mae'] )\n",
    "else:\n",
    "    model = print('Load the model..')\n",
    "    modelfile = 'model-12_01_2019_12_12_34_169126.h5'\n",
    "    model = load_model(modelfile, custom_objects= { 'tf' : tf  } )\n",
    "    o = keras.optimizers.Adadelta(lr=.025, rho=0.95)\n",
    "    model.compile( optimizer=o, loss= 'mean_absolute_error' , metrics=['mae'])\n",
    "\n",
    "max_len = 3000\n",
    "mc = ModelCheckpoint(modelfile, monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 2, patience = es_patience)\n",
    "history = model.fit( { 'Seqin': XTRAINrows } , [YTRAINpca,YTRAIN] , batch_size = None , verbose = 2 , epochs = 100000 ,  validation_data=([XVALIDrows], [YVALIDpca,YVALID] ), callbacks=[es, mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
