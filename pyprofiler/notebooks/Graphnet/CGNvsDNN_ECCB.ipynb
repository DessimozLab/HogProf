{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo- regression on interaction score\n",
    "#todo- refactor dnn to pytorch\n",
    "#todo- supplementary: redo dnn features on phylo\n",
    "#todo- phylo aware interaction graph?\n",
    "#todo- find cog examples of cooption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyprofiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/20589387/ipykernel_812026/2933930801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mete3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoatools_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyprofiler'"
     ]
    }
   ],
   "source": [
    "from pyprofiler.utils import hashutils\n",
    "import ete3\n",
    "import random\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.utils.hashutils as hashutils\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pyprofiler.profiler as profiler\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import redis\n",
    "import dask\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import scipy\n",
    "from dask import dataframe as dd\n",
    "import pickle\n",
    "from bloom_filter2 import BloomFilter\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lsh\n",
      "indexing lsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outdated database version, but only minor version change: 3.5 != 3.4. Some functions might fail\n",
      "Cannot load SequenceSearch. Any future call to seq_search will fail!\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/pyoma/browser/db.py\", line 2035, in __init__\n",
      "    self.seq_idx = self.seq_idx()\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/tables/link.py\", line 393, in __call__\n",
      "    self.extfile = tables.open_file(filename, **kwargs)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/pyoma/browser/db.py\", line 113, in synchronized_open_file\n",
      "    return _tables_file._original_open_file(*args, **kwargs)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/tables/file.py\", line 315, in open_file\n",
      "    return File(filename, mode, title, root_uep, filters, **kwargs)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/pyoma/browser/db.py\", line 103, in __init__\n",
      "    super(ThreadsafeFile, self).__init__(*args, **kargs)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/tables/file.py\", line 778, in __init__\n",
      "    self._g_new(filename, mode, **params)\n",
      "  File \"tables/hdf5extension.pyx\", line 374, in tables.hdf5extension.File._g_new\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/tables/utils.py\", line 154, in check_file_access\n",
      "    raise IOError(\"``%s`` does not exist\" % (filename,))\n",
      "OSError: ``/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/OMA/sep2022/OmaServer.h5.idx`` does not exist\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/pyoma/browser/db.py\", line 189, in __init__\n",
      "    self.seq_search = SequenceSearch(self)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/pyoma/browser/db.py\", line 2040, in __init__\n",
      "    raise DBConsistencyError(\n",
      "pyoma.browser.db.DBConsistencyError: Suffix index for protein sequences is not available: ``/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/OMA/sep2022/OmaServer.h5.idx`` does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "path = '/work/FAC/FBM/DBC/cdessim2/default/dmoi/'\n",
    "#lets load a compiled db containing the OMA root HOGs into a profiler oject \n",
    "p = profiler.Profiler(lshforestpath = path + 'datasets/OMA/sep2022/all/newlshforest.pkl' , hashes_h5=path +'datasets/OMA/sep2022/all/hashes.h5' , mat_path= None, oma = path + 'datasets/OMA/sep2022/OmaServer.h5', tar= None , nsamples = 256 , mastertree = path + 'datasets/OMA/sep2022/all_master_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map ids to OMA HOGs\n",
    "def grabHog(ID, verbose = True):\n",
    "    try:\n",
    "        entry = p.db_obj.entry_by_entry_nr(p.db_obj.id_resolver.resolve(ID))\n",
    "        return entry[4].decode() , entry\n",
    "    except:\n",
    "        return np.nan,np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0                1             2\n",
      "0            P46926           Q8TDQ7  1.000000e+00\n",
      "1            P43631           Q14954  1.000000e+00\n",
      "2            P43631           Q14953  1.000000e+00\n",
      "3            P43631           P43629  1.000000e+00\n",
      "4         100287045           Q86YD7  1.000000e+00\n",
      "...             ...              ...           ...\n",
      "17526306     Q9H6Z4           Q7L1Q6  3.000000e-14\n",
      "17526307     Q9Y265           P54578  3.000000e-14\n",
      "17526308     P16152           P04406  3.000000e-14\n",
      "17526309     Q96AE4           P78417  3.000000e-14\n",
      "17526310     P20042  ENSG00000180574  3.000000e-14\n",
      "\n",
      "[17526311 rows x 3 columns]\n",
      "17526311\n",
      "33777\n",
      "map all hogids\n",
      "done compiling mapper\n",
      "31627\n"
     ]
    }
   ],
   "source": [
    "#filtering down the humap dataset\n",
    "humap = path + 'datasets/humap_PPI/humap2_ppis_ACC_20200821.pairsWprob'\n",
    "calc_humap = True\n",
    "if calc_humap == True:\n",
    "    #load humap data\n",
    "    humap_df = pd.read_table(humap, header = None)\n",
    "    print(humap_df)\n",
    "    print(len(humap_df))\n",
    "    humap_df = humap_df[humap_df[2] > 0.03 ]\n",
    "    \n",
    "    print(len(humap_df))\n",
    "\n",
    "    print('map all hogids')\n",
    "    mapper = set( list(humap_df[1]) + list(humap_df[0]) )\n",
    "    mapper = { protid: grabHog(protid) for protid in mapper }\n",
    "    \n",
    "    print('done compiling mapper')\n",
    "    humap_df['hog1'] = humap_df[1].map(mapper)\n",
    "    humap_df['hog2'] = humap_df[0].map(mapper)\n",
    "    humap_df['hogid_1'] = humap_df['hog1'].map(lambda x:x[0])\n",
    "    humap_df['hogid_2'] = humap_df['hog2'].map(lambda x:x[0])\n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df['fam1'] = humap_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    humap_df['fam2'] = humap_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df.fam1 = humap_df.fam1.map(int)\n",
    "    humap_df.fam2 = humap_df.fam2.map(int)\n",
    "    print(len(humap_df))\n",
    "    humap_df.to_csv(humap+'hogmapped.csv')\n",
    "else:\n",
    "    humap_df = pd.read_csv(humap+'hogmapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31627\n"
     ]
    }
   ],
   "source": [
    "print(len(humap_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap_pairs = humap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7128\n"
     ]
    }
   ],
   "source": [
    "#calculating the profile vectors for each HOG in the Humap dataset\n",
    "calc_hogs_humap = True\n",
    "if calc_hogs_humap == True:\n",
    "    allhogs = set([])\n",
    "    allhogs = allhogs.union( set(humap_df.fam1.unique() ) )\n",
    "    allhogs = allhogs.union( set(humap_df.fam2.unique() ) )\n",
    "    allhogs = list(allhogs)\n",
    "    print(len(allhogs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-1] child process calling self.run()\n",
      "[INFO/Process-2] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-3] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-4] child process calling self.run()\n",
      "[INFO/Process-5] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worker start2worker start4worker start3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-6] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "worker start5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-7] child process calling self.run()\n",
      "[INFO/Process-8] child process calling self.run()\n",
      "[INFO/Process-9] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start8worker start6worker start7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-10] child process calling self.run()\n",
      "[INFO/Process-11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "worker start9worker start10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-12] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-13] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-14] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-15] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start13\n",
      "worker start14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-16] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worker start15"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-17] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worker start16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-18] child process calling self.run()\n",
      "[INFO/Process-19] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start18worker start17"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-20] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worker start19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-21] child process calling self.run()\n",
      "[INFO/Process-22] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start20worker start21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-23] child process calling self.run()\n",
      "[INFO/Process-24] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worker start23worker start22\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-25] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker start24\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n"
     ]
    }
   ],
   "source": [
    "if calc_hogs_humap == True:\n",
    "    profiles = p.retmat_mp_profiles(allhogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Hogs_humap = True\n",
    "if save_Hogs_humap == True:\n",
    "    \n",
    "    with open(humap + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(humap + 'gold_standard_profiles.pkl' , 'rb') as profiles_out:\n",
    "    humap_profiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "humap_df = pd.DataFrame.from_dict(humap_profiles , orient = 'index')\n",
    "print(humap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################begin building the string dataset ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_coglinks = False\n",
    "#filtering down the COGlinks using cutoffs for global score and text mining\n",
    "if filter_coglinks == True:\n",
    "    from collections import Counter\n",
    "    coglink_df = dd.read_csv(path +'datasets/STRING/COG.links.detailed.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')\n",
    "    print(coglink_df)\n",
    "    print(coglink_df.columns)\n",
    "    dropcols = [ 'cooccurence', 'combined_score' ]\n",
    "    coglink_df = coglink_df.drop(columns = dropcols)\n",
    "    coglink_df['score'] = coglink_df.coexpression + coglink_df.experimental +coglink_df.database+ coglink_df.textmining\n",
    "    #these cutoffs ere found below using jaccard and AUC\n",
    "    coglink_df= coglink_df[coglink_df.score>1000]\n",
    "    coglink_df= coglink_df[coglink_df.textmining>500]\n",
    "    coglink_df = coglink_df.compute()\n",
    "    coglink_count = Counter(list(coglink_df.group1)+list(coglink_df.group2))\n",
    "    coglink_df['count1']= coglink_df.group1.map(coglink_count)\n",
    "    coglink_df['count2']= coglink_df.group2.map(coglink_count)\n",
    "    #filter input set\n",
    "    #coglink_df = coglink_df[coglink_df.count1 > 50 ]\n",
    "    #coglink_df = coglink_df[coglink_df.count2 > 50 ]\n",
    "    \n",
    "    print(coglink_df.head() , len(coglink_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coglink_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/20551092/ipykernel_769607/1774482055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcompute_grabcogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompute_grabcogs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrabcogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoglink_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoglink_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgrabcogs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrabcogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mCOGmapings_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'datasets/STRING/COG.mappings.v11.5.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25e6\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coglink_df' is not defined"
     ]
    }
   ],
   "source": [
    "#map the interacting cogs to the proteins\n",
    "compute_grabcogs = True\n",
    "if compute_grabcogs == True:\n",
    "    grabcogs = set( list(coglink_df.group1.unique()) + list(coglink_df.group2.unique()) )\n",
    "    grabcogs= list(grabcogs)\n",
    "    COGmapings_df = dd.read_csv(path +'datasets/STRING/COG.mappings.v11.5.txt', blocksize=25e6 , header = 0, sep = '\\t')\n",
    "    COGmapings_df = COGmapings_df.set_index('orthologous_group')\n",
    "    COGmapings_df.astype(str)\n",
    "    COGmapings_df['##protein'].map( lambda x : x.strip() )\n",
    "    COGmapings_df['species'] = COGmapings_df['##protein'].map( lambda x : x.split('.')[0] )\n",
    "    COGmapings_df['COG'] = COGmapings_df.index\n",
    "    COGmapings_df = COGmapings_df.loc[grabcogs]\n",
    "    COGmapings_df = COGmapings_df.compute()\n",
    "    print(COGmapings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt.grabcogs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/20551092/ipykernel_769607/1525428614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprotsout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrabprots\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'datasets/STRING/COG.links.detailed.v11.5.txt'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.grabcogs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprotsout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mgrabcogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcog\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprotsout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'datasets/STRING/COG.mappings.v11.5.txt'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.grabprots.txt'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprotsout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt.grabcogs.txt'"
     ]
    }
   ],
   "source": [
    "#only take the proteins in our cogs of interest\n",
    "if compute_grabcogs == True:\n",
    "    grabprots =list(COGmapings_df['##protein'].unique())\n",
    "    print(len(grabprots))\n",
    "    with open(path + 'datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabcogs ]) )\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabprots ]) )\n",
    "else:\n",
    "    with open(path + 'datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'r') as protsout:\n",
    "        grabcogs = [ cog for cog in protsout.readlines()]\n",
    "    with open(path +'datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'r') as protsout:\n",
    "        grabprots = [ prot for prot in protsout.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_mappers = False\n",
    "#use redis to store the mapping of proteins to their cogs\n",
    "if calc_mappers == True:\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    count = 0\n",
    "    for i,r in COGmapings_df.iterrows():\n",
    "        rdb.set(r['##protein'], i)\n",
    "        count+=1\n",
    "        if count < 10:\n",
    "            print(i+'\\n',r)\n",
    "        if count%1000000==0:\n",
    "            print(count/len(COGmapings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maphogs = False\n",
    "if maphogs == True:\n",
    "    #you need to change this for your own rdb configuration\n",
    "    #this is used later by the distributed computation\n",
    "    #mapping each string cog to an oma hog by selecting a member of the cog\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    hogmap = {}\n",
    "    for i,prot in enumerate(grabprots):\n",
    "        if i % 100000 == 0 :\n",
    "            print(i/len(grabprots))\n",
    "        cog = rdb.get(prot)\n",
    "        if cog not in hogmap:\n",
    "            mapped =  grabHog(prot)\n",
    "            #retry until something maps\n",
    "            if mapped[0] != np.nan and type(mapped[0]) == str :\n",
    "                if len(mapped[0])>1 :\n",
    "                    hogmap[cog] = mapped\n",
    "    with open('stringhogmap.pkl' , 'wb')as hogmapout:\n",
    "        hogmapout.write(pickle.dumps(hogmap))\n",
    "else:\n",
    "    with open('stringhogmap.pkl' , 'rb')as hogmapout:\n",
    "        hogmap = pickle.loads(hogmapout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hogmap))\n",
    "for i, key in enumerate(hogmap):\n",
    "    if i < 10:\n",
    "        print(key, hogmap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the HOGs to the COGdf\n",
    "#grab the corresponding profiles\n",
    "compile_final_cogdf = False\n",
    "if compile_final_cogdf == True:\n",
    "    print(len(coglink_df))\n",
    "    try:\n",
    "        coglink_df.group1  = coglink_df.group1.map( lambda x : x.encode())\n",
    "        coglink_df.group2  = coglink_df.group2.map( lambda x : x.encode())\n",
    "    except:\n",
    "        pass\n",
    "    coglink_df['hog1'] = coglink_df.group1.map(hogmap)\n",
    "    coglink_df['hog2'] = coglink_df.group2.map(hogmap)\n",
    "    coglink_df=coglink_df.dropna()\n",
    "    print(len(coglink_df))\n",
    "    print(coglink_df.head())\n",
    "    coglink_df['hogid_1'] = coglink_df['hog1'].map(lambda x:x[0])\n",
    "    coglink_df['hogid_2'] = coglink_df['hog2'].map(lambda x:x[0])\n",
    "    coglink_df['fam1'] = coglink_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    coglink_df['fam2'] = coglink_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    coglink_df.fam1 = coglink_df.fam1.map(int)\n",
    "    coglink_df.fam2 = coglink_df.fam2.map(int)\n",
    "    stringHOGs = set(coglink_df.fam1.unique()).union(set(coglink_df.fam2.unique()))\n",
    "    print(len(stringHOGs))\n",
    "    print(coglink_df)\n",
    "    coglink_df.to_csv('STRINGCOGS2OMAHOGS.csv')\n",
    "else:\n",
    "    coglink_df = pd.read_csv('STRINGCOGS2OMAHOGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringPairs = coglink_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive explicit profiles for our hogs of interest in string\n",
    "calc_hogs_string = False\n",
    "stringprofiles = {}\n",
    "if calc_hogs_string == True:\n",
    "    print('profiles to calclulate',len(stringHOGs))\n",
    "    for i,fam in enumerate(stringHOGs):\n",
    "        if i % 100 ==0:\n",
    "            print(i)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "            stringprofiles.update(prof)\n",
    "        except:\n",
    "            print('err',fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_hogs_string == True:\n",
    "    with open(path + 'datasets/STRING/' + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(stringprofiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'datasets/STRING/' + 'gold_standard_profiles.pkl' , 'rb' )as profiles_out:\n",
    "    stringprofiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = pd.DataFrame.from_dict(stringprofiles , orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the profiles for this small set of HOGs\n",
    "for i, key in enumerate(stringprofiles):\n",
    "    if i < 10:\n",
    "        print(key,stringprofiles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have profiles for all HUMAP and COG interactions\n",
    "#String has interactions from each COG in different species.\n",
    "#We need a way to check for the presence of interaction within a species for a COG\n",
    "#for this we will create a bloom filter with all the interactions between our cogs\n",
    "\n",
    "calc_filter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    from dask.distributed import fire_and_forget\n",
    "    from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    from dask.distributed import  utils_perf\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    import dask\n",
    "    import redis\n",
    "    from bloom_filter2 import BloomFilter\n",
    "    import lzma\n",
    "    from dask import dataframe as dd\n",
    "    distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    #using distributed computation on a slurm cluster here. This is my particular config. You will need to alter this: https://distributed.dask.org/en/stable/\n",
    "    if distributed == True:\n",
    "        NCORE = 4\n",
    "        print('deploying cluster')\n",
    "        cluster = SLURMCluster(\n",
    "            #change theses settings for your cluster\n",
    "            walltime='4:00:00',\n",
    "            n_workers = NCORE,\n",
    "            cores=NCORE,\n",
    "            processes = NCORE,\n",
    "            interface='ib0',\n",
    "            memory=\"120GB\",\n",
    "            env_extra=[\n",
    "            \n",
    "            path + 'miniconda/etc/profile.d/conda.sh',\n",
    "            'conda activate ML2'\n",
    "            ],\n",
    "            #scheduler_options={'interface': 'ens2f0' },\n",
    "            #if gpu node\n",
    "            scheduler_options={'interface': 'ens3f0' },\n",
    "            #extra=[\"--lifetime\", \"3h55m\", \"--lifetime-stagger\", \"4m\"]\n",
    "        )\n",
    "        print(cluster.job_script())\n",
    "    else:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        print(cluster)\n",
    "        cluster.scale(jobs = 100)\n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "if calc_filter == True:\n",
    "    \n",
    "    #link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.physical.links.detailed.v11.5.txt', blocksize=100e6 , header = 0, sep = ' ')\n",
    "    link_df = dd.read_csv(path + 'datasets/STRING/protein.links.full.v11.5.txt',  blocksize=75e6 , header = 0, sep = ' ')\n",
    "    print(link_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "@dask.delayed\n",
    "def mapcogs(df ):\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    if type( df ) == tuple:\n",
    "        df = df[0]\n",
    "    protlist1 = list(df.protein1.map(lambda x:str(x).strip()))\n",
    "    protlist2 = list(df.protein2.map(lambda x:str(x).strip()))\n",
    "    protlist = list(set(protlist1+protlist2))\n",
    "    data = rdb.mget(protlist)\n",
    "    mapper = dict(zip(protlist, data) )\n",
    "    df['COG1'] = df.protein1.map(mapper)\n",
    "    df['COG2'] = df.protein2.map(mapper)\n",
    "    df = df.dropna()\n",
    "    df['COG1'] = df.COG1.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['COG2'] = df.COG2.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['species'] = df.protein1.map(lambda x:x.split('.')[0])\n",
    "    df['coglinks'] = df.COG1 + '_' + df.COG2 + '_' + df.species\n",
    "    ret = set(df.coglinks.unique())\n",
    "    return ret\n",
    "@dask.delayed\n",
    "def return_filter(coglinks, verbose = True):\n",
    "    if type( coglinks ) == tuple:\n",
    "        coglinks = coglinks[0]\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in coglinks:\n",
    "        b.add( p )\n",
    "    retlen = len(coglinks)\n",
    "    return   b , retlen\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    f3 = f1.__ior__(f2)\n",
    "    return f3 , total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treesum(totalfilter):\n",
    "    print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        print(len(totalfilter))\n",
    "    return totalfilter\n",
    "\n",
    "if calc_filter == True:\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    partitions  = link_df.to_delayed()\n",
    "    print('map cogs')\n",
    "    res1 = [ mapcogs(p) for p in partitions ]\n",
    "    print('done')\n",
    "    print('make filters')\n",
    "    res2 = [ return_filter(p) for p in res1 ]\n",
    "    finals =[]\n",
    "    for chunk in range(int(len(res2)/1024)+1):\n",
    "        print(chunk*1024)\n",
    "        res3 = res2[chunk*1024:(chunk+1)*1024]\n",
    "        res4 = treesum(res3)\n",
    "        res4 = dask.compute(res4)\n",
    "        print(res4)\n",
    "        finals.append(res4[0])\n",
    "\n",
    "    with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(finals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(finals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bloomfinal_big.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "print(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if our filter function works\n",
    "def check_filters(element,filters):\n",
    "    for f in filters:\n",
    "        if element in f[0][0]:\n",
    "            return True\n",
    "    return False\n",
    "import functools\n",
    "bfilter = functools.partial(check_filters , filters= resfinal)\n",
    "#should be in there\n",
    "print(bfilter('COG1756_COG0088_4113'))\n",
    "#should not be in there...\n",
    "print(bfilter('crap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make sure all of our string species are mapped to their corresponding OMA equivalent\n",
    "taxmapper = 'string2oma_specmap.pkl'\n",
    "calc_taxmapper = False\n",
    "if calc_taxmapper == True:\n",
    "     with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        #compare lineages to get the closest\n",
    "        from ete3 import NCBITaxa\n",
    "        ncbi = NCBITaxa()\n",
    "        #map profiler leaves to closest leaf in string\n",
    "        strings_species = '/scratch/dmoi/datasets/STRING/species.v11.5.txt'\n",
    "        string_specdata = pd.read_table(strings_species)\n",
    "        print(string_specdata)\n",
    "        stringset = set([ str(tax) for tax in list(string_specdata['#taxon_id']) ]) \n",
    "        omaset =  set([ spec.name  for spec in p.tree.get_leaves()])\n",
    "        print('omaset',len(omaset))\n",
    "        print('stringset',len(stringset))\n",
    "        shared_leaves = omaset.intersection(stringset)\n",
    "        missing_leaves = omaset - stringset\n",
    "        #these are oma leaves not in string...we can map to the closest string species.\n",
    "        string_missing = stringset - omaset\n",
    "        omalineages = {tax: set(ncbi.get_lineage(int(tax))) for tax in missing_leaves}\n",
    "        stringlineages = {tax: set(ncbi.get_lineage(int(tax))) for tax in string_missing}\n",
    "        string2oma={}\n",
    "        print('done lineages')\n",
    "        for i,tax in enumerate(stringlineages):\n",
    "            #find the closest\n",
    "            shared = { tax_oma: len(stringlineages[tax].intersection(omalineages[tax_oma]))/len(omalineages[tax_oma]) for tax_oma in omalineages } \n",
    "\n",
    "            string2oma[tax] = max(shared, key=shared.get)\n",
    "            if i %1000 == 0:\n",
    "                print(i/ len(stringlineages) )\n",
    "        with open(taxmapper, 'wb') as taxmapper:\n",
    "            taxmapper.write(pickle.dumps(string2oma))\n",
    "else:\n",
    "    with open(taxmapper, 'rb') as taxmapper:\n",
    "        string2oma= pickle.loads(taxmapper.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(string2oma.keys())))\n",
    "print(len(set(string2oma.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out to find the OMA species for a cog pair\n",
    "species = [ spec.name  for spec in p.tree.get_leaves()] + list(string2oma.keys())\n",
    "species = set(species)\n",
    "print(len(species))\n",
    "coglinks_species = [ coglink+spec for spec in species ]\n",
    "checklinks = [ bfilter( spec) for spec in coglinks_species ]\n",
    "species_set = [s for s in species]\n",
    "links = [ l for l,c in list(zip(coglinks_species,checklinks))  if c== True  ]\n",
    "species_set = [ s for s,c in list(zip(species_set,checklinks))  if c== True  ]\n",
    "species_set = set([ s if s not in string2oma else string2oma[s] for s in species_set  ])\n",
    "\n",
    "print(len(links))\n",
    "print(species_set)\n",
    "\n",
    "print(len(species_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dendropy\n",
    "taxnwk = '/scratch/dmoi/datasets/birds/all_test_master_tree.nwk'\n",
    "with open( 'taxtree.nwk' , 'w') as treeout:\n",
    "    treeout.write(p.tree.write())\n",
    "dendrotree = dendropy.Tree.get(\n",
    "        data=p.tree.write(format=3),\n",
    "        rooting=\"force-rooted\",\n",
    "        suppress_internal_node_taxa=False,\n",
    "        schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(dendrotree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try it out. we should be able to find in which species two cogs interact with the bloom filter\n",
    "cog1='COG0088'\n",
    "cog2 ='COG1756'\n",
    "coglink = cog1 + '_' + cog2 + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smallpars\n",
    "import copy\n",
    "#we're checking for interaction in a subset of species and propagating up\n",
    "#the functions for the small parsimony problem are included in the smallpars script.\n",
    "allowed_symbols =set([0,1,None])\n",
    "transition_dict = { (c[0],c[1]):i for i,c in enumerate(itertools.permutations(allowed_symbols,2) ) }\n",
    "\n",
    "\n",
    "def calc_interaction_on_taxonomy(cog1,cog2,treein ,species_set = species, string2oma= string2oma, verbose = False):\n",
    "    #set interaction states\n",
    "    #look for interactions in bloom\n",
    "    coglink1 = cog1+'_'+cog2 +'_'\n",
    "    coglinks_species = [ coglink1+spec for spec in species_set ]\n",
    "    coglink2 = cog2+'_'+cog1+'_'\n",
    "    coglinks_species += [ coglink2+spec for spec in species_set ]\n",
    "    \n",
    "    checklinks = [ bfilter( spec) for spec in coglinks_species ]\n",
    "    \n",
    "    species_set = [ s for s,c in list(zip(species_set,checklinks))  if c== True  ]\n",
    "    if verbose == True:\n",
    "        print(cog1,cog2)\n",
    "        print('string entries:',len(species_set))\n",
    "    species_set = set([ s if s not in string2oma else string2oma[s] for s in species_set  ])\n",
    "    tree = copy.deepcopy(treein)\n",
    "    for i,l in enumerate(tree.leaf_nodes()):\n",
    "        l.event = {}\n",
    "        l.scores = {}\n",
    "        l.symbols = {}\n",
    "        l.scores = { c:10**10 for c in allowed_symbols }\n",
    "        if l.taxon.label in species_set:\n",
    "            l.symbols = {1}\n",
    "            l.scores[1] = 0\n",
    "        else:\n",
    "            l.symbols = {0}\n",
    "            l.scores[0] = 0\n",
    "        l.char = min(l.scores, key=l.scores.get)\n",
    "    t = smallpars.calculate_small_parsimony(tree ,allowed_symbols, transition_dict)\n",
    "    labels = np.array( [ n.char for n in t.nodes() ] )\n",
    "    return  labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import colour\n",
    "import dendropy\n",
    "#quick and dirty check of trees using a networkx graphing function\n",
    "\n",
    "def tree_circle(node, start , sliver  ):\n",
    "    if start == True :\n",
    "        global count\n",
    "        count = 0\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if child.is_leaf() == True:\n",
    "            child.radians = count*sliver\n",
    "            count+=1\n",
    "        if child.radians is None and child.is_leaf() == False :\n",
    "            tree_circle( child , start = False , sliver = sliver  )\n",
    "    radians = np.mean([ child.radians if child.radians else 0 for child in node.child_nodes() ])\n",
    "    if node.radians is None:\n",
    "        node.radians = radians\n",
    "\n",
    "def phylograph(treein,labels , title = None ):\n",
    "    N = len(treein.nodes())\n",
    "    tree = copy.deepcopy(treein)\n",
    "    pdm = tree.phylogenetic_distance_matrix()\n",
    "    sliver = 2*np.pi / len(tree.leaf_nodes())\n",
    "    \n",
    "    root = tree.seed_node\n",
    "    radii = [ n.distance_from_root() for n in tree.nodes()]\n",
    "    for n in tree.nodes():\n",
    "        n.radians = None\n",
    "    tree_circle(tree.seed_node, start=True , sliver = sliver)\n",
    "    thetas = [n.radians for n in tree.nodes() ]\n",
    "    pos = { i: [ np.sin(thetas[i])*radii[i] , np.cos(thetas[i])*radii[i]] for i in range(len(thetas)) }\n",
    "    index = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat = np.zeros(( N ,  N ) )\n",
    "    connectmat[index[:,0],index[:,1]] = 1 \n",
    "    connectmat += connectmat.T\n",
    "    G = nx.from_numpy_array(connectmat)\n",
    "    red = colour.Color(\"red\")\n",
    "    blue = colour.Color(\"blue\")\n",
    "    crange = dict( zip ( list(set(labels)),  [ c.hex_l for c in list(red.range_to(blue, len(set(labels)) ) ) ] ) )\n",
    "    colors = [crange[n] for n in labels ]\n",
    "    #color according to downstream node\n",
    "    edge_colors= [crange[labels[v]] for u,v in G.edges() ]    \n",
    "    plt.figure(figsize= (20,20) )\n",
    "    nx.draw_networkx(G, pos = pos,  node_color = colors , node_size = 5, width = .5 , edge_color = edge_colors , with_labels=False)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "        plt.savefig( title +'_phylograph.svg' )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels needs to be a list with the same ordering as a loop over the tree nodes\n",
    "#label = [some_label for n in tree.nodes() ]\n",
    "labels = calc_interaction_on_taxonomy(cog1,cog2,dendrotree, verbose = True )\n",
    "phylograph(dendrotree, labels , title = cog1+'vs'+cog2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_interaction_pairs = False\n",
    "if check_interaction_pairs == True:\n",
    "    samples = 10\n",
    "    for i in range(samples):\n",
    "        r = coglink_df.sample(n = 1).iloc[0]\n",
    "        print(r)\n",
    "        cog1 = str(r.group1).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "        cog2 = str(r.group2).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "        labels = calc_interaction_on_taxonomy(cog1,cog2,dendrotree ,verbose = True)\n",
    "        print(np.sum(labels))\n",
    "        phylograph(dendrotree, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map fam to matrow\n",
    "humap_fam_map= { f:i for i,f in enumerate(humap_df.index)}\n",
    "humap_profilemat = np.vstack(humap_df.mat)\n",
    "print(humap_profilemat.shape)\n",
    "string_fam_map = { f:i for i,f in enumerate(string_df.index)}\n",
    "string_mat = np.vstack(string_df.mat)\n",
    "print(string_mat.shape)\n",
    "\n",
    "#train test split\n",
    "Datasets = {}\n",
    "for label,df,mapping,profilemat in [  ('string', stringPairs, string_fam_map ,string_mat ) , ('humap',humap_pairs, humap_fam_map , humap_profilemat) ]: \n",
    "    keys = set(mapping.keys())\n",
    "    entry1 = [ f in keys for f in df.fam1]\n",
    "    df = df.iloc[entry1]\n",
    "    entry2 = [ f in keys for f in df.fam2]\n",
    "    df = df.iloc[entry2]\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    df_train = df.iloc[msk]\n",
    "    df_test = df.iloc[~msk]\n",
    "    Datasets[label]={'Train':df_train,'Test':df_test , 'mapping': mapping , 'mat':profilemat }\n",
    "    print(label)\n",
    "    print('train',len(df_train))\n",
    "    print('test',len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator for training the DNN\n",
    "def chunks(df, n):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i + n]\n",
    "def generateXYchunk(explicit_profiles, goldstandardDF,fam_map,  nsamples=100, posi_percent = .5):\n",
    "    #shuffle\n",
    "    goldstandardDF = goldstandardDF.sample(frac=1)\n",
    "    for chunkdf in chunks(goldstandardDF , int( nsamples*posi_percent)):\n",
    "        #negatives drawn from the overall dataset\n",
    "        X = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam1]) , np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam2]) ] )\n",
    "        Y = [1]* X.shape[0]\n",
    "        neg1 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam1)\n",
    "        neg2 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam2)\n",
    "        \n",
    "        if len(neg1)>0:\n",
    "            mixchunk = np.hstack([np.vstack([profilemat[fam_map[f]] for f in neg1]),np.vstack([profilemat[fam_map[f]] for f in neg2])])\n",
    "            Y =np.hstack([[0]* mixchunk.shape[0] , Y])\n",
    "            X= np.vstack([mixchunk,X])    \n",
    "        #positive samples\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC and PR plotting functions to visulize the performance of our metrics\n",
    "\n",
    "def ROC_curve(y_data, label = None , title_add= ''):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        truth = y_data[l]['Ytrue']\n",
    "        pred = y_data[l]['Ypred']\n",
    "        \n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(   truth , pred)\n",
    "        plt.plot(fpr, tpr, label=l + ' auc: '+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title(title_add+'ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.ylim(.4, 1) \n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        truth = y_data[l]['Ytrue']\n",
    "        pred = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( truth, pred  )\n",
    "        plt.plot(    recall , precision , label= l )\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.title(title_add+' PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single( truth , pred , label = None ):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(truth , pred )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(truth , pred )\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC_single.svg' )\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  truth , pred )\n",
    "    plt.plot( recall , precision )\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR_single.svg' )\n",
    "    plt.title('PR curve')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a vanilla deep NN to distinguish between interacting and non interacting pairs\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "overwrite = False\n",
    "train_dnn = False\n",
    "\n",
    "if train_dnn == True:\n",
    "    for dataset in Datasets:\n",
    "        modelpath = './'+dataset+'_dropout_DNN.h5'\n",
    "        callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= modelpath+'.logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=modelpath)\n",
    "        ]\n",
    "        print(dataset)\n",
    "        df_train = Datasets[label]['Train']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        if os.path.exists(modelpath) and overwrite == False:\n",
    "            model = load_model(modelpath)\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=100, activation='sigmoid', input_dim=profilemat.shape[1]*2))\n",
    "            model.add(tf.keras.layers.Dropout( .5 , seed=42 ))\n",
    "            model.add(Dense(units=30, activation='sigmoid' ) )           \n",
    "            model.add(tf.keras.layers.Dropout( .5 , seed=42 ))\n",
    "            model.add(Dense(units=1, activation='sigmoid' ) )\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        generator = generateXYchunk(profilemat, df_train, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        sample = next(generator)\n",
    "        model.fit(itertools.cycle(generator) , steps_per_epoch = 300 , epochs = 100, callbacks=callbacks)\n",
    "        # Save the model\n",
    "        model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use jaccard distance AUC to tune dataset cutoffs for string data\n",
    "#just eyeballing a decent AUC vs dataset size compromise\n",
    "#this was later used to adjust the threshold for individual evidence channels\n",
    "#in the end only a cutoff for text mining at 500 was used\n",
    "visualize = False    \n",
    "if visualize == True:\n",
    "    dataset = Datasets['string']\n",
    "    label = 'string'\n",
    "\n",
    "    df_test = Datasets[label]['Test']\n",
    "    fam_map = Datasets[label]['mapping']\n",
    "    profilemat = Datasets[label]['mat']\n",
    "    ydata =  {}\n",
    "\n",
    "    df_test.hist()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    threshes = [ 1000 , 1500 , 2000 , 3000]\n",
    "    dataset_len =[]\n",
    "    xaxis =[]\n",
    "    for thresh in threshes:\n",
    "        func, name =  (jaccard,'Jaccard')\n",
    "        print(name)\n",
    "\n",
    "        sub = df_test[df_test.score>thresh]\n",
    "        dataset_len.append(len(sub))\n",
    "        xaxis.append(thresh)\n",
    "\n",
    "        generator = generateXYchunk(profilemat, sub, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        #test all the easy metrics\n",
    "        ypreds = []\n",
    "        ytruth = []\n",
    "        for X,y in generator:\n",
    "            #distances\n",
    "            x1 = X[:,0:int(X.shape[1]/2)]\n",
    "            x2 = X[:,int(X.shape[1]/2):]\n",
    "            predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "            ypreds.append(predictions)\n",
    "            ytruth.append(y)\n",
    "        ytest= np.hstack(ytruth)\n",
    "        ypred = np.hstack(ypreds)\n",
    "        ydata[name+str(thresh)] = { 'Ypred': ypred , 'Ytrue':ytest}\n",
    "    ROC_curve(ydata , label = str(thresh ) + 'string' , title_add='combined score ')\n",
    "    plt.show()\n",
    "    plt.plot(xaxis ,dataset_len)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    threshes = [ 100 , 200 , 300, 500, 600 ,700, 800, 900 ]\n",
    "    for metric in ['database', 'coexpression' , 'textmining' , 'experimental' ]:\n",
    "        print(metric)\n",
    "        ydata =  {}\n",
    "\n",
    "        dataset_len =[]\n",
    "        xaxis =[]\n",
    "        for thresh in threshes:\n",
    "            func, name =  (jaccard,'Jaccard')\n",
    "            print(name)\n",
    "            sub = df_test[df_test[metric] > thresh]\n",
    "            if len(sub) > 0:\n",
    "\n",
    "                dataset_len.append(len(sub))\n",
    "                xaxis.append(thresh)\n",
    "                generator = generateXYchunk(profilemat, sub, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "                #test all the easy metrics\n",
    "                ypreds = []\n",
    "                ytruth = []\n",
    "                for X,y in generator:\n",
    "                    #distances\n",
    "                    x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                    x2 = X[:,int(X.shape[1]/2):]\n",
    "                    predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                    ypreds.append(predictions)\n",
    "                    ytruth.append(y)\n",
    "                ytest= np.hstack(ytruth)\n",
    "                ypred = np.hstack(ypreds)\n",
    "                ydata[name+str(thresh)] = { 'Ypred': ypred , 'Ytrue':ytest}\n",
    "\n",
    "        ROC_curve(ydata , label = str(thresh ) + 'string' ,  title_add=metric + ' ')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(xaxis ,dataset_len)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    threshes = [ 5,10, 50, 100 , 200 , 300, 500, 600 ,700, 800, 900 ]\n",
    "\n",
    "    print(metric)\n",
    "    ydata =  {}\n",
    "\n",
    "    dataset_len =[]\n",
    "    xaxis =[]\n",
    "    for thresh in threshes:\n",
    "        func, name =  (jaccard,'Jaccard')\n",
    "        print(name)\n",
    "\n",
    "        sub = df_test[df_test.count1 > thresh]\n",
    "        sub = sub[sub.count2 > thresh]\n",
    "\n",
    "        if len(sub) > 50:\n",
    "            dataset_len.append(len(sub))\n",
    "            xaxis.append(thresh)\n",
    "\n",
    "            generator = generateXYchunk(profilemat, sub, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "            #test all the easy metrics\n",
    "            ypreds = []\n",
    "            ytruth = []\n",
    "            for X,y in generator:\n",
    "                #distances\n",
    "                x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                x2 = X[:,int(X.shape[1]/2):]\n",
    "                predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                ypreds.append(predictions)\n",
    "                ytruth.append(y)\n",
    "\n",
    "            ytest= np.hstack(ytruth)\n",
    "            ypred = np.hstack(ypreds)\n",
    "            ydata[name+str(thresh)] = { 'Ypred': ypred , 'Ytrue':ytest}\n",
    "    ROC_curve(ydata , label = str(thresh ) + 'string' , title_add='interaction count ' )\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(xaxis ,dataset_len)\n",
    "    plt.title('testing dataset size' )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal distance metrics to try\n",
    "from scipy.spatial.distance import euclidean , hamming, jaccard\n",
    "from sklearn.covariance import empirical_covariance\n",
    "from scipy.stats import pearsonr\n",
    "from keract import get_activations\n",
    "\n",
    "\n",
    "def pearsonR(v1,v2):\n",
    "        return -pearsonr(v1,v2)[0]\n",
    "visualize = True    \n",
    "\n",
    "#here we compare vector distance metrics to the DNN's performance\n",
    "if visualize == True:\n",
    "    for label in Datasets:\n",
    "        print(label)\n",
    "        df_test = Datasets[label]['Test']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        ydata =  {}\n",
    "\n",
    "        for func, name in [ (euclidean, 'Euclidean' ), (hamming,'Hamming') , (jaccard,'Jaccard') , (pearsonR,'Pearson') ]:\n",
    "            print(name)\n",
    "            generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "            #test all the easy metrics\n",
    "            ypreds = []\n",
    "            ytruth = []\n",
    "            for X,y in generator:\n",
    "                #distances\n",
    "                x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                x2 = X[:,int(X.shape[1]/2):]\n",
    "                predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                ypreds.append(predictions)\n",
    "                ytruth.append(y)\n",
    "            ytest= np.hstack(ytruth)\n",
    "            ypred = np.hstack(ypreds)\n",
    "            ydata[name] = { 'Ypred': ypred , 'Ytrue':ytest} \n",
    "        #get DNN values\n",
    "        print('DNN')\n",
    "        generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        mats = [(x,y) for x,y in generator ]\n",
    "        y_test = np.hstack([y for x,y in mats])\n",
    "        modelpath = './'+label+'_dropout_DNN.h5'\n",
    "        if os.path.exists(modelpath):\n",
    "            model = load_model(modelpath)\n",
    "\n",
    "\n",
    "        ypred = np.vstack([ model.predict(x) for x,y in mats])\n",
    "\n",
    "\n",
    "        #activations = [ get_activations(model, x , auto_compile=True) for x,y in mats] \n",
    "        #activations = activations[list(activations.keys())[0]]\n",
    "        #print( activations.shape)\n",
    "        #n_samples = activations.shape[0]\n",
    "        #activations = np.sum(activations,axis =0)/n_samples\n",
    "        #representations = np.sum(X_test, axis = 0)\n",
    "\n",
    "        #print(activations.shape)\n",
    "\n",
    "        #np.save(modelpath + 'activation.np' , activations)\n",
    "        #np.save(modelpath + 'representation.np', representations)\n",
    "\n",
    "        ydata['DNN'] ={ 'Ypred': ypred , 'Ytrue':ytest}\n",
    "        #plot ROC\n",
    "        print(ydata)\n",
    "        ROC_curve(ydata , label = label)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        with open(label + '_ROCdata.pkl' , 'wb' ) as ydata_out:\n",
    "            ydata_out.write(pickle.dumps(ydata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################being the graph NN part of the paper #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "#create graphs on the fly to represent pairs of profiles\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set([ n.name for n in p.tree.traverse() ])\n",
    "dendrotree_nodes = set([str(n.taxon.label) if n.taxon else '-1' for n in dendrotree.nodes()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(dendrotree_nodes))\n",
    "print(len(nodes.intersection(dendrotree_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { n.name:i for i,n in enumerate(p.tree.traverse()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { (n.taxon.label if n.taxon else '-1'):n.matrow for n in  dendrotree.nodes() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating graphs using the phylogenetic connectivity matrix\n",
    "def tree2Single_sparse_graph_updown(tree):\n",
    "    N = len(tree.nodes())\n",
    "    #mimic the fitch algo\n",
    "    #propagate up and down in separate graphs\n",
    "    index_up = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    index_down = np.vstack([ [c.matrow, n.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat_up = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_down = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_up[index_up[:,0],index_up[:,1]] = 1 \n",
    "    connectmat_down[index_down[:,0],index_down[:,1]] = 1 \n",
    "    diag = [[n,n] for n in range(N)]\n",
    "    connectmat_diag=scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_diag[diag,diag] = 1 \n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    Norm_nsister= np.array( [ len(n.sister_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    msis =np.amax(Norm_nsister)\n",
    "    Norm_nsister/=msis    \n",
    "    template_features = np.stack([ntime ,  Norm_nchild , Norm_nsister ]).T    \n",
    "    return connectmat_up, connectmat_down, connectmat_diag, template_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to get the most recent common ancestor of a leafset\n",
    "def getmrca(treein,taxset):\n",
    "    \n",
    "    tree = copy.deepcopy(treein)\n",
    "    n = tree.mrca(taxon_labels=taxset)\n",
    "    if n is not None:\n",
    "        subtree = dendropy.Tree(seed_node=n)\n",
    "        taxset = set([ t.taxon.label if t.taxon else '-1'  for t in subtree.nodes()])\n",
    "        matrows = [ t.matrow for t in treein.nodes() if t.taxon and t.taxon.label in taxset]\n",
    "        return taxset, matrows , n\n",
    "    else: \n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change a sparse matrix to coordinate format\n",
    "def sparse2pairs(sparsemat, matrows = None):\n",
    "    if matrows :\n",
    "        sparsemat = sparsemat[matrows,:]\n",
    "        sparsemat = sparsemat[:,matrows]\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree sector based aggregation\n",
    "#label sectors\n",
    "###not used in the end... but it is pretty... \n",
    "\n",
    "def process_node_down(node, sector = 0, breakpt = 10 , total = 0 ):\n",
    "    node.sector = sector\n",
    "    if sector == 0 :\n",
    "        global count\n",
    "        count = 0\n",
    "    total += len(node.child_nodes())\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if total > breakpt:\n",
    "            if len(child.child_nodes())>0:\n",
    "                #new sector w new total\n",
    "                count+=1\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "            else:\n",
    "                #leaf\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "        else:\n",
    "            process_node_down(child, count , total = total , breakpt = breakpt)\n",
    "\n",
    "    \n",
    "def get_sectors(tree, breakpt = 20):\n",
    "    process_node_down( tree.seed_node , sector = 0, breakpt = breakpt )\n",
    "    row = [n.matrow for n in tree.nodes()]\n",
    "    col = [n.sector for n in tree.nodes()]\n",
    "    data = np.ones((len(row)))\n",
    "    sectormat = scipy.sparse.csc_matrix( (data,(row,col)) )\n",
    "    return sectormat\n",
    "\n",
    "for i,l in enumerate(dendrotree.nodes()):\n",
    "    l.sum_lengths = None\n",
    "for i,l in enumerate(dendrotree.leaf_nodes()):\n",
    "    l.sum_lengths = 1\n",
    "sectormat = get_sectors(dendrotree, breakpt = 60 )\n",
    "labels = [n.sector for n in dendrotree.nodes()]\n",
    "print('nsectors' , len(set(labels)))\n",
    "phylograph( dendrotree, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating data samples for the CGN. we transform the tree into a connectivity matrix and feature vectors\n",
    "#the we add the HOGprof data and trim the tree down to a subgraph containing the mrca of the species where either hog is present\n",
    "#\n",
    "def create_data_updown_nosectors( tree, coglinkdf, profiles , taxindex , posi_percent = .5 ,  q = None , iolock= None,  verbose = False, loop= True ):\n",
    "        #upward and downward connected phylo nodes\n",
    "        connectmat_up, connectmat_down, connectmat_diag, template_features = tree2Single_sparse_graph_updown(tree)\n",
    "        N = len(tree.nodes())\n",
    "        Nsectors = sectormat.shape[1]\n",
    "        allfams = list(set(coglinkdf.fam1.unique()).union( set(coglinkdf.fam2.unique() ) ))\n",
    "        leafset = set([n.taxon.label for n in tree.leaf_nodes()])\n",
    "        while True:\n",
    "            toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "            if verbose == True:\n",
    "                print('posi/nega',toss)\n",
    "            if toss == 0:\n",
    "                fam1 = random.choice(allfams)\n",
    "                fam2 = fam1\n",
    "                while fam1 == fam2:\n",
    "                    fam2 = random.choice(allfams)\n",
    "                labels = np.zeros((template_features.shape[0],))\n",
    "            else:\n",
    "                #positive sample\n",
    "                dfline = coglinkdf.sample(n=1, random_state = random.randint(1,1000)).iloc[0]\n",
    "                cog1= str(dfline.group1).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                cog2= str(dfline.group2).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                fam1 = dfline.fam1\n",
    "                fam2 = dfline.fam2\n",
    "                labels = calc_interaction_on_taxonomy(cog1,cog2,tree)\n",
    "            nodefeatures = []\n",
    "            presences= []\n",
    "            #find profile nameset\n",
    "            for i,tp in enumerate([profiles[fam1]['tree'], profiles[fam2]['tree']]):    \n",
    "                profilefeatures = np.zeros((template_features.shape[0],3) )\n",
    "                #find on which nodes the events happened\n",
    "                losses = [ taxindex[n.name]  for n in tp.traverse() if n.lost ]\n",
    "                dupl = [ taxindex[n.name]  for n in tp.traverse() if n.dupl ]\n",
    "                presence = [ n.name  for n in tp.traverse() if n.nbr_genes > 0   ]\n",
    "                presences.append(presence)\n",
    "                presence = [taxindex[n] for n in presence]\n",
    "                profilefeatures[losses, 0] = 1\n",
    "                profilefeatures[dupl, 1] = 1\n",
    "                profilefeatures[presence, 2] = 1\n",
    "                nodefeatures.append(profilefeatures)\n",
    "            nodeset = set(presences[0]).union(set(presences[1]))\n",
    "            if len(nodeset)> 10:\n",
    "                skip = False\n",
    "                try:\n",
    "                    taxset,matrows,n = getmrca(tree,leafset.intersection(nodeset))\n",
    "                    if taxset == None:\n",
    "                        skip = True\n",
    "                except ValueError:\n",
    "                    #no species overlap\n",
    "                    skip = True\n",
    "\n",
    "                if skip == False:\n",
    "                    \n",
    "                    #pare down labels\n",
    "                    labels = labels[matrows]\n",
    "                    \n",
    "                    if verbose == True:\n",
    "                        print('features',nodefeatures)\n",
    "                        print( 'labels' , labels)\n",
    "                    neglabels = np.ones(labels.shape)\n",
    "                    neglabels = neglabels - labels\n",
    "                    labels = np.vstack([labels,neglabels]).T\n",
    "\n",
    "                    overview = scipy.sparse.lil_matrix( (len(matrows) , 2 ) )\n",
    "                    overview[:,0] = 1\n",
    "\n",
    "                    overview_rev = sparse2pairs(overview.T)\n",
    "                    overview = sparse2pairs(overview)\n",
    "\n",
    "                    #phylonode connections\n",
    "                    subconnect_up = sparse2pairs(connectmat_up, matrows)\n",
    "                    subconnect_down = sparse2pairs(connectmat_down, matrows)\n",
    "                    subdiag = sparse2pairs(connectmat_diag, matrows)\n",
    "                    \n",
    "                    \n",
    "                    #profile features\n",
    "                    nodefeatures=np.hstack(nodefeatures)\n",
    "                    \n",
    "                    sub_template_features= template_features[matrows,:]\n",
    "                    sub_node_features= nodefeatures[matrows,:]\n",
    "                    sub_node_features = np.hstack([sub_template_features , sub_node_features])\n",
    "                    godlabel = np.ones((1,1))*toss\n",
    "                    godlabel = np.hstack([np.ones((1,1))-godlabel, godlabel])\n",
    "                    \n",
    "                    data = HeteroData()    \n",
    "                    #add input data\n",
    "                    data['phylonodes_up'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                    data['phylonodes_down'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                    data['godnode'].x =torch.tensor(  np.zeros((1,1))  ,  dtype=torch.double )\n",
    "\n",
    "                    #up down fitch net\n",
    "                    data['phylonodes_up', 'phylolink_up', 'phylonodes_up'].edge_index = torch.tensor(subconnect_up ,  dtype=torch.long )\n",
    "                    data['phylonodes_down', 'phylolink_down', 'phylonodes_down'].edge_index = torch.tensor(subconnect_down ,  dtype=torch.long )             \n",
    "                    data['phylonodes_up', 'phylolink_up_down', 'phylonodes_down'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "                    data['phylonodes_down', 'phylolink_down_up', 'phylonodes_up'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "\n",
    "                    #pooling connections\n",
    "                    data['phylonodes_down', 'informs', 'godnode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "                    data['phylonodes_up', 'informs', 'godnode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "                    \n",
    "                    #pooling connections\n",
    "                    data['godnode',  'informs','phylonodes_down' ].edge_index = torch.tensor(overview_rev,  dtype=torch.long )\n",
    "                    data['godnode',  'informs','phylonodes_up'].edge_index = torch.tensor(overview_rev ,  dtype=torch.long )\n",
    "                    \n",
    "                    #add labels            \n",
    "                    data['phylonodes_up'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "                    data['phylonodes_down'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "                    data['godnode'].y =torch.tensor( godlabel  ,  dtype=torch.long )\n",
    "\n",
    "                    data = T.AddSelfLoops()(data)\n",
    "                    data = T.NormalizeFeatures()(data)\n",
    "                    if q:\n",
    "                        q.put(data)\n",
    "                    else:\n",
    "                        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = create_data_updown_nosectors( dendrotree, Datasets['string']['Train']  , stringprofiles , profile_mapper, .5  , verbose = False  )\n",
    "print(next(gen))\n",
    "#use the dataset generation script to create the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('trainingset_nosectors.pkl' , 'rb')as trainout:\n",
    "#    trainingdata = pickle.loads(trainout.read())\n",
    "with open('testgset_nosectors' , 'rb')as trainout:\n",
    "    testingdata = pickle.loads(trainout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training samples ' ,len(trainingdata))\n",
    "print('testing samples' , len(testingdata))\n",
    "\n",
    "data = trainingdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv , SAGEConv, Linear , ResGatedGraphConv , GATv2Conv , TransformerConv , MFConv , FiLMConv \n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#without sectornode\n",
    "class HeteroGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins =  torch.nn.ModuleList()\n",
    "        self.lins2 =  torch.nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                \n",
    "                ('phylonodes_up', 'phylolink_up', 'phylonodes_up'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down', 'phylonodes_down'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down_up', 'phylonodes_up'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'phylolink_up_down', 'phylonodes_down'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                \n",
    "                ('phylonodes_down', 'informs', 'godnode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'informs', 'godnode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                #('godnode', 'informs', 'phylonodes_down'):SAGEConv((-1,-1),  int( hidden_channels) ),\n",
    "                #('godnode', 'informs', 'phylonodes_up'):SAGEConv((-1,-1),  int( hidden_channels) ),\n",
    "            \n",
    "            } , aggr='sum')\n",
    "            \n",
    "            self.convs.append(conv)\n",
    "\n",
    "            for vectype in  ['phylonodes_up', 'phylonodes_down' , 'sectornode' , 'godnode' ]:\n",
    "                lin1 = Linear(-1 , int( hidden_channels))\n",
    "                self.lins.append( lin1 )\n",
    "                \n",
    "            print( 'hidden units' , int( hidden_channels) )\n",
    "            print( 'layer' , i )\n",
    "\n",
    "        for vectype in ['phylonodes_up', 'phylonodes_down' , 'sectornode' , 'godnode' ]:\n",
    "            lin2 = Linear(-1 , out_channels)\n",
    "            self.lins2.append( lin2 ) \n",
    "\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        lins = iter(self.lins)\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            x_dict = conv(x_dict , edge_index_dict)\n",
    "            x_dict = {key: F.dropout(x , p = .75 , training = self.training ) for key, x in x_dict.items()}\n",
    "\n",
    "            for key, x in x_dict.items():\n",
    "                x_dict[key] = next(lins)(x)\n",
    "        lins2 = iter(self.lins2)\n",
    "        \n",
    "        for key, x in x_dict.items():\n",
    "            x_dict[key] =  next(lins2)(x)\n",
    "        \n",
    "        return {key: F.tanh(x) for key, x in x_dict.items()}\n",
    "        #return x_dict\n",
    "\n",
    "model = HeteroGCN(hidden_channels=50 , out_channels=2, num_layers=3)\n",
    "model = model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('./phylographnet_job_final.torch'))\n",
    "model = model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict , data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./phylographnet_job_final50.torch')\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader( trainingdata , batch_size = 50 , shuffle=True)\n",
    "testloader = DataLoader(testingdata , batch_size = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('start training')\n",
    "import warnings\n",
    "#lastauc = 0.69224568015331\n",
    "lastauc = 0\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = model.to(device)\n",
    "    for epoch in range(10000):\n",
    "        model.train()\n",
    "        losses1=[]\n",
    "        losses2 =[]\n",
    "        losses3 =[]\n",
    "\n",
    "        \n",
    "        truths = []\n",
    "        preds = []\n",
    "\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        if epoch % 2 == 0 :\n",
    "            for i,data in enumerate(trainloader):\n",
    "                data = data.to(device)\n",
    "                out = model(data.x_dict ,data.edge_index_dict)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss1 =  F.smooth_l1_loss(out['phylonodes_up'].double(), data['phylonodes_up'].y.double())\n",
    "                loss2 =  F.smooth_l1_loss(out['phylonodes_down'].double(), data['phylonodes_down'].y.double())\n",
    "                loss3 =  F.smooth_l1_loss(out['godnode'].double(), data['godnode'].y.double())\n",
    "\n",
    "                loss = loss1 + loss2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses3.append(float(loss3.to('cpu')))\n",
    "                losses2.append(float(loss2.to('cpu')))\n",
    "                losses1.append(float(loss1.to('cpu')))\n",
    "\n",
    "                truth = data['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "                predy =  out['godnode'][:,0].to('cpu').detach().numpy()\n",
    "                truths.append(truth)\n",
    "                preds.append(predy)\n",
    "\n",
    "            truth = np.hstack(truths)\n",
    "            predy = np.hstack(preds)\n",
    "            fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "            auc_n = auc(fpr, tpr)\n",
    "            print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "            print('train auc', auc_n)\n",
    "        truths = []\n",
    "        preds = []\n",
    "        losses3 = []\n",
    "        \n",
    "        \n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss3 =  F.smooth_l1_loss(out['godnode'].double(), data['godnode'].y.double())\n",
    "            loss = loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            truth = data['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  out['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_n = auc(fpr, tpr)\n",
    "        print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "        print('train auc', auc_n)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        truths = []\n",
    "        preds = []\n",
    "\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        \n",
    "        \n",
    "        for i,testdata in enumerate(testloader):\n",
    "            testdata = testdata.to(device)\n",
    "            pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "            truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "            truth_n = testdata['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "            pred_n =  pred['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "\n",
    "            truths_n.append(truth_n)\n",
    "            preds_n.append(pred_n)\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_gn = auc(fpr, tpr)\n",
    "        print('test auc',auc_gn)\n",
    "        truth_n = np.hstack(truths_n)\n",
    "        predy_n = np.hstack(preds_n)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "        auc_n = auc(fpr, tpr)        \n",
    "        print('test node auc',auc_n)\n",
    "        if auc_gn > lastauc:\n",
    "            lastauc = auc_gn\n",
    "            print('saving')\n",
    "            torch.save(model, './phylographnet_job_final50.torch')\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./phylographnet_job_final50.torch')\n",
    "model = model.double()\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "truths = []\n",
    "preds = []\n",
    "\n",
    "truths_n = []\n",
    "preds_n = []\n",
    "\n",
    "testloader = DataLoader(testingdata , batch_size = 10 )\n",
    "for i,testdata in enumerate(testloader):\n",
    "    testdata = testdata.to(device)\n",
    "    pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "    pred = {idx:x.to('cpu') for idx,x in pred.items()}\n",
    "    \n",
    "    truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "    predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "    truths.append(truth)\n",
    "    preds.append(predy)\n",
    "    \n",
    "    truth_n = testdata['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "    pred_n =  pred['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "    \n",
    "    truths_n.append(truth_n)\n",
    "    preds_n.append(pred_n)\n",
    "    \n",
    "truth = np.hstack(truths)\n",
    "predy = np.hstack(preds)\n",
    "\n",
    "print(predy)\n",
    "print(truth)\n",
    "ROC_curve_single( truth , predy)\n",
    "fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "auc_gn = auc(fpr, tpr)\n",
    "print(auc_gn)\n",
    "truth_n = np.hstack(truths_n)\n",
    "predy_n = np.hstack(preds_n)\n",
    "\n",
    "ROC_curve_single( truth_n, predy_n , 'node_specific')\n",
    "fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "auc_n = auc(fpr, tpr)\n",
    "print(auc_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./string_ROCdata.pkl' , 'rb') as ydata_in:\n",
    "    ydata   = pickle.loads(ydata_in.read())\n",
    "\n",
    "ydata['CGN'] ={ 'Ypred': predy , 'Ytrue':truth}\n",
    "#plot ROC\n",
    "print(ydata)\n",
    "ROC_curve(ydata , label = 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####training the CGN on HUMAP data #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we don't infer ancestral states to this function is faster than the one used for string\n",
    "def create_data_updown_nosectors_humap( tree, coglinkdf, profiles , taxindex , posi_percent = .5 ,  q = None , iolock= None,  verbose = False, loop= True ):\n",
    "        #upward and downward connected phylo nodes\n",
    "        connectmat_up, connectmat_down, connectmat_diag, template_features = tree2Single_sparse_graph_updown(tree)\n",
    "        N = len(tree.nodes())\n",
    "        allfams = list(set(coglinkdf.fam1.unique()).union( set(coglinkdf.fam2.unique() ) ))\n",
    "        leafset = set([n.taxon.label for n in tree.leaf_nodes()])\n",
    "        while True:\n",
    "            toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "            if verbose == True:\n",
    "                print('posi/nega',toss)\n",
    "            if toss == 0:\n",
    "                fam1 = random.choice(allfams)\n",
    "                fam2 = fam1\n",
    "                while fam1 == fam2:\n",
    "                    fam2 = random.choice(allfams)\n",
    "                labels = np.zeros((template_features.shape[0],))\n",
    "            else:\n",
    "                #positive sample\n",
    "                dfline = coglinkdf.sample(n=1, random_state = random.randint(1,1000)).iloc[0]\n",
    "                fam1 = dfline.fam1\n",
    "                fam2 = dfline.fam2\n",
    "            nodefeatures = []\n",
    "            presences= []\n",
    "            #find profile nameset\n",
    "            for i,tp in enumerate([profiles[fam1]['tree'], profiles[fam2]['tree']]):    \n",
    "                profilefeatures = np.zeros((template_features.shape[0],3) )\n",
    "                #find on which nodes the events happened\n",
    "                losses = [ taxindex[n.name]  for n in tp.traverse() if n.lost ]\n",
    "                dupl = [ taxindex[n.name]  for n in tp.traverse() if n.dupl ]\n",
    "                presence = [ n.name  for n in tp.traverse() if n.nbr_genes > 0   ]\n",
    "                presences.append(presence)\n",
    "                presence = [taxindex[n] for n in presence]\n",
    "                profilefeatures[losses, 0] = 1\n",
    "                profilefeatures[dupl, 1] = 1\n",
    "                profilefeatures[presence, 2] = 1\n",
    "                nodefeatures.append(profilefeatures)\n",
    "            nodeset = set(presences[0]).union(set(presences[1]))\n",
    "            if len(nodeset)> 10:\n",
    "                skip = False\n",
    "                try:\n",
    "                    taxset,matrows,n = getmrca(tree,leafset.intersection(nodeset))\n",
    "                    if taxset == None:\n",
    "                        skip = True\n",
    "                except ValueError:\n",
    "                    #no species overlap\n",
    "                    skip = True\n",
    "\n",
    "                if skip == False:\n",
    "                    \n",
    "                    #pare down labels\n",
    "\n",
    "                    overview = scipy.sparse.lil_matrix( (len(matrows) , 2 ) )\n",
    "                    overview[:,0] = 1\n",
    "\n",
    "                    overview_rev = sparse2pairs(overview.T)\n",
    "                    overview = sparse2pairs(overview)\n",
    "\n",
    "                    #phylonode connections\n",
    "                    subconnect_up = sparse2pairs(connectmat_up, matrows)\n",
    "                    subconnect_down = sparse2pairs(connectmat_down, matrows)\n",
    "                    subdiag = sparse2pairs(connectmat_diag, matrows)\n",
    "                    \n",
    "                    \n",
    "                    #profile features\n",
    "                    nodefeatures=np.hstack(nodefeatures)\n",
    "                    \n",
    "                    sub_template_features= template_features[matrows,:]\n",
    "                    sub_node_features= nodefeatures[matrows,:]\n",
    "                    sub_node_features = np.hstack([sub_template_features , sub_node_features])\n",
    "                    godlabel = np.ones((1,1))*toss\n",
    "                    godlabel = np.hstack([np.ones((1,1))-godlabel, godlabel])\n",
    "                    \n",
    "                    data = HeteroData()    \n",
    "                    #add input data\n",
    "                    data['phylonodes_up'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                    data['phylonodes_down'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                    data['godnode'].x =torch.tensor(  np.zeros((1,1))  ,  dtype=torch.double )\n",
    "\n",
    "                    #up down fitch net\n",
    "                    data['phylonodes_up', 'phylolink_up', 'phylonodes_up'].edge_index = torch.tensor(subconnect_up ,  dtype=torch.long )\n",
    "                    data['phylonodes_down', 'phylolink_down', 'phylonodes_down'].edge_index = torch.tensor(subconnect_down ,  dtype=torch.long )             \n",
    "                    data['phylonodes_up', 'phylolink_up_down', 'phylonodes_down'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "                    data['phylonodes_down', 'phylolink_down_up', 'phylonodes_up'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "\n",
    "                    #pooling connections\n",
    "                    data['phylonodes_down', 'informs', 'godnode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "                    data['phylonodes_up', 'informs', 'godnode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "                    \n",
    "                    #pooling connections\n",
    "                    data['godnode',  'informs','phylonodes_down' ].edge_index = torch.tensor(overview_rev,  dtype=torch.long )\n",
    "                    data['godnode',  'informs','phylonodes_up'].edge_index = torch.tensor(overview_rev ,  dtype=torch.long )\n",
    "                    \n",
    "                    data['godnode'].y =torch.tensor( godlabel  ,  dtype=torch.long )\n",
    "\n",
    "                    data = T.AddSelfLoops()(data)\n",
    "                    data = T.NormalizeFeatures()(data)\n",
    "                    if q:\n",
    "                        q.put(data)\n",
    "                    else:\n",
    "                        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genTrain = create_data_updown_nosectors_humap( dendrotree, Datasets['humap']['Train']  , humap_profiles  ,profile_mapper , .5  , verbose = False  )\n",
    "print(next(genTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final AUC comparisons\n",
    "genTest = create_data_updown_nosectors_humap( dendrotree, Datasets['humap']['Test']  , humap_profiles  ,profile_mapper , .5  , verbose = False  )\n",
    "print(next(genTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_gen = True\n",
    "trainsample= 10000\n",
    "#create training set using the generator on training samples\n",
    "reload = True\n",
    "if traindata_gen == True:\n",
    "    if reload == True:\n",
    "        with open('trainingset_nosectors_humap.pkl' , 'rb')as trainout:\n",
    "            samples = pickle.loads(trainout.read())\n",
    "        print(len(samples))\n",
    "    else:\n",
    "        print('newsamples')\n",
    "        samples = []\n",
    "    if traindata_gen == True:\n",
    "        print('gen samples ' )\n",
    "        gen = genTrain\n",
    "        for i , data in enumerate( gen ):\n",
    "            if i < 5:\n",
    "                print(data)\n",
    "                print(data['godnode'].y)\n",
    "            if i % 100 ==0:\n",
    "                print(i, len(samples))\n",
    "                with open('trainingset_nosectors_humap.pkl' , 'wb')as trainout:\n",
    "                    trainout.write(pickle.dumps(samples))\n",
    "            samples.append(data)\n",
    "            if i > trainsample:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing set using the testing set dataframe\n",
    "testdata_gen = True\n",
    "testsample= 5000\n",
    "NCORE = 20\n",
    "reload = True\n",
    "\n",
    "if testdata_gen == True:\n",
    "    if reload == True:\n",
    "        with open('testgset_nosectors_humap' , 'rb')as trainout:\n",
    "            samples = pickle.loads(trainout.read())\n",
    "    else:\n",
    "        print('newsamples')\n",
    "        samples = []\n",
    "    if testdata_gen == True:\n",
    "        gen = genTest\n",
    "        for i , data in enumerate( gen ):\n",
    "            if i % 100 ==0:\n",
    "                print(i, len(samples))\n",
    "                with open('testgset_nosectors_humap' , 'wb')as trainout:\n",
    "                    trainout.write(pickle.dumps(samples))\n",
    "            samples.append(data)\n",
    "            if i > testsample:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainingset_nosectors_humap.pkl' , 'rb')as trainout:\n",
    "    trainingdata = pickle.loads(trainout.read())\n",
    "with open('testgset_nosectors_humap' , 'rb')as trainout:\n",
    "    testingdata = pickle.loads(trainout.read())\n",
    "    \n",
    "data = trainingdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv , SAGEConv, Linear , ResGatedGraphConv , GATv2Conv , TransformerConv , MFConv , FiLMConv \n",
    "\n",
    "    \n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#without sectornode\n",
    "class HeteroGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins =  torch.nn.ModuleList()\n",
    "        self.lins2 =  torch.nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                \n",
    "                ('phylonodes_up', 'phylolink_up', 'phylonodes_up'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down', 'phylonodes_down'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down_up', 'phylonodes_up'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'phylolink_up_down', 'phylonodes_down'):MFConv((-1,-1),  int( hidden_channels) ),\n",
    "                \n",
    "                ('phylonodes_down', 'informs', 'godnode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'informs', 'godnode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                #('godnode', 'informs', 'phylonodes_down'):SAGEConv((-1,-1),  int( hidden_channels) ),\n",
    "                #('godnode', 'informs', 'phylonodes_up'):SAGEConv((-1,-1),  int( hidden_channels) ),\n",
    "            \n",
    "            } , aggr='sum')\n",
    "            \n",
    "            self.convs.append(conv)\n",
    "\n",
    "            for vectype in  ['phylonodes_up', 'phylonodes_down' , 'sectornode' , 'godnode' ]:\n",
    "                lin1 = Linear(-1 , int( hidden_channels))\n",
    "                self.lins.append( lin1 )\n",
    "                \n",
    "            print( 'hidden units' , int( hidden_channels) )\n",
    "            print( 'layer' , i )\n",
    "\n",
    "        for vectype in ['phylonodes_up', 'phylonodes_down' , 'sectornode' , 'godnode' ]:\n",
    "            lin2 = Linear(-1 , out_channels)\n",
    "            self.lins2.append( lin2 ) \n",
    "\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        lins = iter(self.lins)\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            x_dict = conv(x_dict , edge_index_dict)\n",
    "            x_dict = {key: F.dropout(x , p = .5 , training = self.training ) for key, x in x_dict.items()}\n",
    "\n",
    "            for key, x in x_dict.items():\n",
    "                x_dict[key] = next(lins)(x)\n",
    "        lins2 = iter(self.lins2)\n",
    "        \n",
    "        for key, x in x_dict.items():\n",
    "            x_dict[key] =  next(lins2)(x)\n",
    "        \n",
    "        return {key: F.tanh(x) for key, x in x_dict.items()}\n",
    "        #return x_dict\n",
    "\n",
    "model = HeteroGCN(hidden_channels=50 , out_channels=2, num_layers=2)\n",
    "model = model.double()\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict , data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader( trainingdata , batch_size = 50 , shuffle=True)\n",
    "testloader = DataLoader(testingdata , batch_size = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start training')\n",
    "import warnings\n",
    "#lastauc = 0.7751991361963\n",
    "lastauc = 0\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = model.to(device)\n",
    "    for epoch in range(10000):\n",
    "        model.train()\n",
    "        truths = []\n",
    "        preds = []\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        losses3 = []\n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss3 =  F.smooth_l1_loss(out['godnode'].double(), data['godnode'].y.double())\n",
    "            loss = loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            truth = data['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  out['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_n = auc(fpr, tpr)\n",
    "        print('losses', np.mean(losses3) )\n",
    "        print('train auc', auc_n)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        truths = []\n",
    "        preds = []\n",
    "\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        \n",
    "        \n",
    "        for i,testdata in enumerate(testloader):\n",
    "            testdata = testdata.to(device)\n",
    "            pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "            truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_gn = auc(fpr, tpr)\n",
    "        print('test auc',auc_gn)\n",
    "        if auc_gn > lastauc:\n",
    "            lastauc = auc_gn\n",
    "            print('saving')\n",
    "            torch.save(model, './phylographnet_job_finalhumap.torch')\n",
    "            print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./phylographnet_job_finalhumap.torch')\n",
    "model = model.double()\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "truths = []\n",
    "preds = []\n",
    "\n",
    "truths_n = []\n",
    "preds_n = []\n",
    "\n",
    "testloader = DataLoader(testingdata , batch_size = 10 )\n",
    "for i,testdata in enumerate(testloader):\n",
    "    testdata = testdata.to(device)\n",
    "    pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "    pred = {idx:x.to('cpu') for idx,x in pred.items()}\n",
    "    \n",
    "    truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "    predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "    truths.append(truth)\n",
    "    preds.append(predy)\n",
    "\n",
    "truth = np.hstack(truths)\n",
    "predy = np.hstack(preds)\n",
    "\n",
    "print(predy)\n",
    "print(truth)\n",
    "ROC_curve_single( truth , predy)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "auc_gn = auc(fpr, tpr)\n",
    "print(auc_gn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./humap_ROCdata.pkl' , 'rb') as ydata_in:\n",
    "    ydata   = pickle.loads(ydata_in.read())\n",
    "\n",
    "ydata['CGN'] ={ 'Ypred': predy , 'Ytrue':truth}\n",
    "#plot ROC\n",
    "print(ydata)\n",
    "ROC_curve(ydata , label = 'humap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
