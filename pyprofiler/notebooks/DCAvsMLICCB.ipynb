{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprofiler.utils import hashutils\n",
    "import ete3\n",
    "import random\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.utils.hashutils as hashutils\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pyprofiler.profiler as profiler\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import redis\n",
    "##to get the mapping of oma hogs to cogs to interactions in specific species I used dask distributed and a redis server\n",
    "#you may need to get these up and running for you own cluster configuration before this notebook will work for you\n",
    "import dask\n",
    "import scipy\n",
    "from dask import dataframe as dd\n",
    "import pickle\n",
    "from bloom_filter2 import BloomFilter\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load a compiled db containing the OMA root HOGs into a profiler oject \n",
    "p = profiler.Profiler(lshforestpath = '/scratch/dmoi/datasets/all/newlshforest.pkl' , hashes_h5='/scratch/dmoi/datasets/birds/all/hashes.h5' , mat_path= None, oma = '/scratch/dmoi/datasets/OMA/apr2021/OmaServer.h5', tar= None , nsamples = 256 , mastertree = '/scratch/dmoi/datasets/birds/all_test_master_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabHog(ID, verbose = True):\n",
    "    try:\n",
    "        entry = p.db_obj.entry_by_entry_nr(p.db_obj.id_resolver.resolve(ID))\n",
    "        return entry[4].decode() , entry\n",
    "    except:\n",
    "        return np.nan,np.nan\n",
    "#map to OMA HOGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap = '/scratch/dmoi/datasets/humap_PPI/humap2_ppis_ACC_20200821.pairsWprob'\n",
    "calc_humap = False\n",
    "if calc_humap == True:\n",
    "    #load humap data\n",
    "    humap_df = pd.read_table(humap, header = None)\n",
    "    print(humap_df)\n",
    "    humap_df = humap_df[humap_df[2] > .75 ]\n",
    "    mapper = set( list(humap_df[1]) + list(humap_df[0]) )\n",
    "    mapper = { protid: grabHog(protid) for protid in mapper }\n",
    "    humap_df['hog1'] = humap_df[1].map(mapper)\n",
    "    humap_df['hog2'] = humap_df[0].map(mapper)\n",
    "    humap_df['hogid_1'] = humap_df['hog1'].map(lambda x:x[0])\n",
    "    humap_df['hogid_2'] = humap_df['hog2'].map(lambda x:x[0])\n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df['fam1'] = humap_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    humap_df['fam2'] = humap_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df.fam1 = humap_df.fam1.map(int)\n",
    "    humap_df.fam2 = humap_df.fam2.map(int)\n",
    "    print(len(humap_df))\n",
    "    humap_df.to_csv(humap+'hogmapped.csv')\n",
    "else:\n",
    "    humap_df = pd.read_csv(humap+'hogmapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap_pairs = humap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "calc_hogs_humap = False\n",
    "if calc_hogs_humap == True:\n",
    "    allhogs = set([])\n",
    "    allhogs = allhogs.union( set(humap_df.fam1.unique() ) )\n",
    "    allhogs = allhogs.union( set(humap_df.fam2.unique() ) )\n",
    "    print(len(allhogs))\n",
    "    for fam in allhogs:\n",
    "        print(fam)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "        except:\n",
    "            print('err', fam)\n",
    "        print(prof)\n",
    "        profiles.update(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Hogs_humap = False\n",
    "if save_Hogs_humap == True:\n",
    "    with open(humap + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(humap + 'gold_standard_profiles.pkl' , 'rb') as profiles_out:\n",
    "    humap_profiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap_df = pd.DataFrame.from_dict(humap_profiles , orient = 'index')\n",
    "print(humap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################begin building the string dataset ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_coglinks = False\n",
    "if filter_coglinks == True:\n",
    "    coglink_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')\n",
    "    print(coglink_df)\n",
    "    print(coglink_df.columns)\n",
    "    dropcols = ['neighborhood', 'fusion', 'cooccurence', 'combined_score' ]\n",
    "    coglink_df = coglink_df.drop(columns = dropcols)\n",
    "    coglink_df['score'] = coglink_df.coexpression + coglink_df.experimental +coglink_df.database+ coglink_df.textmining\n",
    "    coglink_df= coglink_df[coglink_df.score>1000]\n",
    "    coglink_df = coglink_df.compute()\n",
    "    print(coglink_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the interacting cogs to the proteins\n",
    "compute_grabcogs = False\n",
    "if compute_grabcogs == True:\n",
    "    grabcogs = set( list(coglink_df.group1.unique()) + list(coglink_df.group2.unique()) )\n",
    "    grabcogs= list(grabcogs)\n",
    "    COGmapings_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt', blocksize=25e6 , header = 0, sep = '\\t')\n",
    "    COGmapings_df = COGmapings_df.set_index('orthologous_group')\n",
    "    COGmapings_df.astype(str)\n",
    "    COGmapings_df['##protein'].map( lambda x : x.strip() )\n",
    "    COGmapings_df['species'] = COGmapings_df['##protein'].map( lambda x : x.split('.')[0] )\n",
    "    COGmapings_df['COG'] = COGmapings_df.index\n",
    "    COGmapings_df = COGmapings_df.loc[grabcogs]\n",
    "    COGmapings_df = COGmapings_df.compute()\n",
    "    print(COGmapings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take the proteins in our cogs of interest\n",
    "if compute_grabcogs == True:\n",
    "    grabprots =list(COGmapings_df['##protein'].unique())\n",
    "    print(len(grabprots))\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabcogs ]) )\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabprots ]) )\n",
    "else:\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'r') as protsout:\n",
    "        grabcogs = [ cog for cog in protsout.readlines()]\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'r') as protsout:\n",
    "        grabprots = [ prot for prot in protsout.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_mappers = False\n",
    "rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "if calc_mappers == True:\n",
    "    count = 0\n",
    "    for i,r in COGmapings_df.iterrows():\n",
    "        rdb.set(r['##protein'], i)\n",
    "        count+=1\n",
    "        if count < 10:\n",
    "            print(i+'\\n',r)\n",
    "        if count%1000000==0:\n",
    "            print(count/len(COGmapings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maphogs = False\n",
    "if maphogs == True:\n",
    "    #mapping each string cog to an oma hog by selecting a member of the cog\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    hogmap = {}\n",
    "    for i,prot in enumerate(grabprots):\n",
    "        if i % 100000 == 0 :\n",
    "            print(i/len(grabprots))\n",
    "        cog = rdb.get(prot)\n",
    "        if cog not in hogmap:\n",
    "            mapped =  grabHog(prot)\n",
    "            #retry until something maps\n",
    "            if mapped[0] != np.nan and type(mapped[0]) == str :\n",
    "                if len(mapped[0])>1 :\n",
    "                    hogmap[cog] = mapped\n",
    "    with open('stringhogmap.pkl' , 'wb')as hogmapout:\n",
    "        hogmapout.write(pickle.dumps(hogmap))\n",
    "else:\n",
    "    with open('stringhogmap.pkl' , 'rb')as hogmapout:\n",
    "        hogmap = pickle.loads(hogmapout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hogmap))\n",
    "for i, key in enumerate(hogmap):\n",
    "    if i < 10:\n",
    "        print(key, hogmap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the HOGs to the COGdf\n",
    "#grab the corresponding profiles\n",
    "compile_final_cogdf = False\n",
    "if compile_final_cogdf == True:\n",
    "    print(len(coglink_df))\n",
    "    try:\n",
    "        coglink_df.group1  = coglink_df.group1.map( lambda x : x.encode())\n",
    "        coglink_df.group2  = coglink_df.group2.map( lambda x : x.encode())\n",
    "    except:\n",
    "        pass\n",
    "    coglink_df['hog1'] = coglink_df.group1.map(hogmap)\n",
    "    coglink_df['hog2'] = coglink_df.group2.map(hogmap)\n",
    "    coglink_df=coglink_df.dropna()\n",
    "    print(len(coglink_df))\n",
    "    print(coglink_df.head())\n",
    "    coglink_df['hogid_1'] = coglink_df['hog1'].map(lambda x:x[0])\n",
    "    coglink_df['hogid_2'] = coglink_df['hog2'].map(lambda x:x[0])\n",
    "    coglink_df['fam1'] = coglink_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    coglink_df['fam2'] = coglink_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    coglink_df.fam1 = coglink_df.fam1.map(int)\n",
    "    coglink_df.fam2 = coglink_df.fam2.map(int)\n",
    "    stringHOGs = set(coglink_df.fam1.unique()).union(set(coglink_df.fam2.unique()))\n",
    "    print(len(stringHOGs))\n",
    "    print(coglink_df)\n",
    "    coglink_df.to_csv('STRINGCOGS2OMAHOGS.csv')\n",
    "else:\n",
    "    coglink_df = pd.read_csv('STRINGCOGS2OMAHOGS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringPairs = coglink_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive explicit profiles for our hogs of interest in string\n",
    "calc_hogs_string = False\n",
    "stringprofiles = {}\n",
    "if calc_hogs_string == True:\n",
    "    print('profiles to calclulate',len(stringHOGs))\n",
    "    for i,fam in enumerate(stringHOGs):\n",
    "        if i % 100 ==0:\n",
    "            print(i)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "            stringprofiles.update(prof)\n",
    "        except:\n",
    "            print('err',fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_hogs_string == True:\n",
    "    with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(stringprofiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'rb' )as profiles_out:\n",
    "    stringprofiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = pd.DataFrame.from_dict(stringprofiles , orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the profiles for this small set of HOGs\n",
    "for i, key in enumerate(stringprofiles):\n",
    "    if i < 10:\n",
    "        print(key,stringprofiles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have profiles for all HUMAP and COG interactions\n",
    "#String has interactions from each COG in different species.\n",
    "#We need a way to check for the presence of interaction within a species for a COG\n",
    "#for this we will create a bloom filter with all the interactions between our cogs\n",
    "\n",
    "calc_filter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "if calc_filter == True:\n",
    "    from dask.distributed import fire_and_forget\n",
    "    from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    from dask.distributed import  utils_perf\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    import dask\n",
    "    import redis\n",
    "    from bloom_filter2 import BloomFilter\n",
    "    import lzma\n",
    "    from dask import dataframe as dd\n",
    "    distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying cluster\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=4\n",
      "#SBATCH --mem=112G\n",
      "#SBATCH -t 4:00:00\n",
      "source /scratch/dmoi/miniconda/etc/profile.d/conda.sh\n",
      "conda activate ML2\n",
      "/scratch/dmoi/condaenvs/ML2/bin/python -m distributed.cli.dask_worker tcp://10.203.100.107:38831 --nthreads 1 --nprocs 4 --memory-limit 27.94GiB --name dummy-name --nanny --death-timeout 60 --interface ib0 --protocol tcp://\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        NCORE = 4\n",
    "        print('deploying cluster')\n",
    "        cluster = SLURMCluster(\n",
    "            walltime='4:00:00',\n",
    "            n_workers = NCORE,\n",
    "            cores=NCORE,\n",
    "            processes = NCORE,\n",
    "            interface='ib0',\n",
    "            memory=\"120GB\",\n",
    "            env_extra=[\n",
    "            'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "            'conda activate ML2'\n",
    "            ],\n",
    "            scheduler_options={'interface': 'ens2f0' },\n",
    "            #extra=[\"--lifetime\", \"3h55m\", \"--lifetime-stagger\", \"4m\"]\n",
    "        )\n",
    "        print(cluster.job_script())\n",
    "\n",
    "    else:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURMCluster(13da5ef2, 'tcp://10.203.100.107:38831', workers=0, threads=0, memory=0 B)\n",
      "http://10.203.100.107:8787/status\n"
     ]
    }
   ],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        print(cluster)\n",
    "        cluster.scale(jobs = 100)\n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                  protein1 protein2 neighborhood fusion cooccurence coexpression experimental database textmining combined_score\n",
      "npartitions=12990                                                                                                               \n",
      "                    object   object        int64  int64       int64        int64        int64    int64      int64          int64\n",
      "                       ...      ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "...                    ...      ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                       ...      ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                       ...      ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "Dask Name: read-csv, 12990 tasks\n"
     ]
    }
   ],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "if calc_filter == True:\n",
    "    \n",
    "    #link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.physical.links.detailed.v11.5.txt', blocksize=100e6 , header = 0, sep = ' ')\n",
    "    link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.links.detailed.v11.5.txt',  blocksize=100e6 , header = 0, sep = ' ')\n",
    "    print(link_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "@dask.delayed\n",
    "def mapcogs(df ):\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    if type( df ) == tuple:\n",
    "        df = df[0]\n",
    "    protlist1 = list(df.protein1.map(lambda x:str(x).strip()))\n",
    "    protlist2 = list(df.protein2.map(lambda x:str(x).strip()))\n",
    "    protlist = list(set(protlist1+protlist2))\n",
    "    data = rdb.mget(protlist)\n",
    "    mapper = dict(zip(protlist, data) )\n",
    "    df['COG1'] = df.protein1.map(mapper)\n",
    "    df['COG2'] = df.protein2.map(mapper)\n",
    "    df = df.dropna()\n",
    "    df['COG1'] = df.COG1.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['COG2'] = df.COG2.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['species'] = df.protein1.map(lambda x:x.split('.')[0])\n",
    "    df['coglinks'] = df.COG1 + '_' + df.COG2 + '_' + df.species\n",
    "    ret = set(df.coglinks.unique())\n",
    "    return ret\n",
    "@dask.delayed\n",
    "def return_filter(coglinks, verbose = True):\n",
    "    if type( coglinks ) == tuple:\n",
    "        coglinks = coglinks[0]\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in coglinks:\n",
    "        b.add( p )\n",
    "    retlen = len(coglinks)\n",
    "    return   b , retlen\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    f3 = f1.__ior__(f2)\n",
    "    return f3 , total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treesum(totalfilter):\n",
    "    print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        print(len(totalfilter))\n",
    "    return totalfilter\n",
    "\n",
    "if calc_filter == True:\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    partitions  = link_df.to_delayed()\n",
    "    print('map cogs')\n",
    "    res1 = [ mapcogs(p) for p in partitions ]\n",
    "    print('done')\n",
    "    print('make filters')\n",
    "    res2 = [ return_filter(p) for p in res1 ]\n",
    "    finals =[]\n",
    "    for chunk in range(int(len(res2)/1024)+1):\n",
    "        print(chunk*1024)\n",
    "        res3 = res2[chunk*1024:(chunk+1)*1024]\n",
    "        res4 = treesum(res3)\n",
    "        res4 = dask.compute(res4)\n",
    "        print(res4)\n",
    "        finals.append(res4[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 122010548)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 220957577)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 169856375)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 119428878)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 126517031)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 185650531)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 199204385)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 241435144)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 228547216)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 191150555)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 93170677)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 0)], [(BloomFilter(ideal_num_elements_n=100000000, error_rate_p=0.001000, num_bits_m=1437758757), 0)]]\n",
      "13\n",
      "7\n",
      "4\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(finals)\n",
    "peel_finals = [r[0] for r in finals]\n",
    "resfinal = treesum(peel_finals)\n",
    "resfinal = dask.compute(peel_finals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    resfinal = dask.compute(totalfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cancel(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    import pickle\n",
    "    with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(resfinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bloomfinal_big.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "cogfilter =  resfinal[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('COG1756_COG0088_4113' in cogfilter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try it out. we should be able to find in which species two cogs interact with the bloom filter\n",
    "cog1='COG0088'\n",
    "cog2 ='COG1756'\n",
    "coglink = cog1 + '_' + cog2 + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out to find the species for a cog pair\n",
    "coglinks_species = [ coglink+spec.name  for spec in p.tree.get_leaves()]\n",
    "species = [ spec.name  for spec in p.tree.get_leaves()]\n",
    "checklinks = [ coglink+spec in cogfilter for spec in coglinks_species ]\n",
    "species_set = [s for s in species]\n",
    "links = [ l for l,c in list(zip(coglinks_species,checklinks))  if c== True  ]\n",
    "species_set = [ s for s,c in list(zip(species_set,checklinks))  if c== True  ]\n",
    "print(links)\n",
    "print(species_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapfam to matrow\n",
    "humap_fam_map= { f:i for i,f in enumerate(humap_df.index)}\n",
    "humap_profilemat = np.vstack(humap_df.mat)\n",
    "print(humap_profilemat.shape)\n",
    "string_fam_map = { f:i for i,f in enumerate(string_df.index)}\n",
    "string_mat = np.vstack(string_df.mat)\n",
    "print(string_mat.shape)\n",
    "#train test split\n",
    "Datasets = {}\n",
    "for label,df,mapping,profilemat in [  ('string', stringPairs, string_fam_map ,string_mat ) , ('humap',humap_pairs, humap_fam_map , humap_profilemat) ]: \n",
    "    keys = set(mapping.keys())\n",
    "    entry1 = [ f in keys for f in df.fam1]\n",
    "    df = df.iloc[entry1]\n",
    "    entry2 = [ f in keys for f in df.fam2]\n",
    "    df = df.iloc[entry2]\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    df_train = df.iloc[msk]\n",
    "    df_test = df.iloc[~msk]\n",
    "    Datasets[label]={'Train':df_train,'Test':df_test , 'mapping': mapping , 'mat':profilemat }\n",
    "    print(label)\n",
    "    print('train',len(df_train))\n",
    "    print('test',len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(df, n):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i + n]\n",
    "def generateXYchunk(explicit_profiles, goldstandardDF,fam_map,  nsamples=100, posi_percent = .5):\n",
    "    #shuffle\n",
    "    goldstandardDF = goldstandardDF.sample(frac=1)\n",
    "    for chunkdf in chunks(goldstandardDF , int( nsamples*posi_percent)):\n",
    "        #negatives drawn from the overall dataset\n",
    "        X = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam1]) , np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam2]) ] )\n",
    "        Y = [1]* X.shape[0]\n",
    "        neg1 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam1)\n",
    "        neg2 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam2)\n",
    "        \n",
    "        if len(neg1)>0:\n",
    "            mixchunk = np.hstack([np.vstack([profilemat[fam_map[f]] for f in neg1]),np.vstack([profilemat[fam_map[f]] for f in neg2])])\n",
    "            Y =np.hstack([[0]* mixchunk.shape[0] , Y])\n",
    "            X= np.vstack([mixchunk,X])    \n",
    "        #positive samples\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def ROC_curve(y_data, label = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot( recall, precision , label= l )\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a vanilla deep NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "overwrite = True\n",
    "train_dnn = False\n",
    "\n",
    "if train_dnn == True:\n",
    "    for dataset in Datasets:\n",
    "        modelpath = './'+dataset+'_dropout_DNN.h5'\n",
    "        callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= modelpath+'.logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=modelpath)\n",
    "        ]\n",
    "        print(dataset)\n",
    "        df_train = Datasets[label]['Train']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        if os.path.exists(modelpath) and overwrite == False:\n",
    "            model = load_model(modelpath)\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=100, activation='sigmoid', input_dim=profilemat.shape[1]*2))\n",
    "            model.add(tf.keras.layers.Dropout( .2 , seed=42 ))\n",
    "            model.add(Dense(units=30, activation='sigmoid' ) )\n",
    "            model.add(Dense(units=1, activation='sigmoid' ) )\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        generator = generateXYchunk(profilemat, df_train, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        sample = next(generator)\n",
    "        model.fit(itertools.cycle(generator) , steps_per_epoch = 300 , epochs = 50, callbacks=callbacks)\n",
    "        # Save the model\n",
    "        model.save(modelpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal distance metrics to try\n",
    "\n",
    "from scipy.spatial.distance import euclidean , hamming, jaccard\n",
    "from sklearn.covariance import empirical_covariance\n",
    "from scipy.stats import pearsonr\n",
    "from keract import get_activations\n",
    "def pearsonR(v1,v2):\n",
    "        return -pearsonr(v1,v2)[0]\n",
    "\n",
    "visualize = False    \n",
    "if visualize == True:\n",
    "    for label in Datasets:\n",
    "        print(label)\n",
    "        df_test = Datasets[label]['Test']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        ydata =  {}\n",
    "\n",
    "        for func, name in [ (euclidean, 'Euclidean' ), (hamming,'Hamming') , (jaccard,'Jaccard') , (pearsonR,'Pearson') ]:\n",
    "            print(name)\n",
    "            generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "            #test all the easy metrics\n",
    "            ypreds = []\n",
    "            ytruth = []\n",
    "            for X,y in generator:\n",
    "                #distances\n",
    "                x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                x2 = X[:,int(X.shape[1]/2):]\n",
    "                predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                ypreds.append(predictions)\n",
    "                ytruth.append(y)\n",
    "            ytest= np.hstack(ytruth)\n",
    "            ypred = np.hstack(ypreds)\n",
    "            ydata[name] = { 'Ypred': ypred , 'Ytrue':ytest} \n",
    "        #get DNN values\n",
    "        print('DNN')\n",
    "        generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        mats = [(x,y) for x,y in generator ]\n",
    "        y_test = np.hstack([y for x,y in mats])\n",
    "        modelpath = './'+label+'_dropout_DNN.h5'\n",
    "        if os.path.exists(modelpath):\n",
    "            model = load_model(modelpath)\n",
    "\n",
    "\n",
    "        ypred = np.vstack([ model.predict(x) for x,y in mats])\n",
    "\n",
    "\n",
    "        #activations = [ get_activations(model, x , auto_compile=True) for x,y in mats] \n",
    "        #activations = activations[list(activations.keys())[0]]\n",
    "        #print( activations.shape)\n",
    "        #n_samples = activations.shape[0]\n",
    "        #activations = np.sum(activations,axis =0)/n_samples\n",
    "        #representations = np.sum(X_test, axis = 0)\n",
    "\n",
    "        #print(activations.shape)\n",
    "\n",
    "        #np.save(modelpath + 'activation.np' , activations)\n",
    "        #np.save(modelpath + 'representation.np', representations)\n",
    "\n",
    "        ydata['DNN'] ={ 'Ypred': ypred , 'Ytrue':ytest}\n",
    "        #plot ROC\n",
    "        print(ydata)\n",
    "        ROC_curve(ydata , label = label)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################being the graph NN part of the paper #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "#create graphs on the fly to represent pairs of profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dendropy\n",
    "taxnwk = '/scratch/dmoi/datasets/birds/all_test_master_tree.nwk'\n",
    "with open( 'taxtree.nwk' , 'w') as treeout:\n",
    "    treeout.write(p.tree.write())\n",
    "dendrotree = dendropy.Tree.get(\n",
    "        data=p.tree.write(format=3),\n",
    "        rooting=\"force-rooted\",\n",
    "        suppress_internal_node_taxa=False,\n",
    "        schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set([ n.name for n in p.tree.traverse() ])\n",
    "dendrotree_nodes = set([str(n.taxon.label) if n.taxon else '-1' for n in dendrotree.nodes()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(dendrotree_nodes))\n",
    "print(len(nodes.intersection(dendrotree_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { n.name:i for i,n in enumerate(p.tree.traverse()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(dendrotree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { (n.taxon.label if n.taxon else '-1'):n.matrow for n in  dendrotree.nodes() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smallpars\n",
    "#we're checking for interaction in a subset of species and propagating up\n",
    "allowed_symbols =set([0,1])\n",
    "transition_dict = { (c[0],c[1]):i for i,c in enumerate(itertools.permutations(allowed_symbols,2) ) }\n",
    "def calc_interaction_on_taxonomy(cog1,cog2,treein, bloom , verbose = False):\n",
    "    #set interaction states\n",
    "    #look for interactions in bloom\n",
    "    tree = copy.deepcopy(treein)\n",
    "    for i,l in enumerate(tree.leaf_nodes()):\n",
    "        l.event = {}\n",
    "        l.scores = {}\n",
    "        l.symbols = {}\n",
    "        l.scores = { c:10**10 for c in allowed_symbols }\n",
    "        if cog1+'_'+cog2+'_'+l.taxon.label in bloom:\n",
    "            if verbose == True:\n",
    "                print(l.taxon.label)\n",
    "            l.char= 1\n",
    "            l.symbols = {1}\n",
    "        else:\n",
    "            l.char= 0\n",
    "            l.symbols = {0}\n",
    "        l.scores[l.char] = 0\n",
    "    t = smallpars.calculate_small_parsimony(tree ,allowed_symbols, transition_dict)\n",
    "    labels = np.array( [ n.char for n in t.nodes() ] )\n",
    "    return  labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2Single_sparse_graph_updown(tree):\n",
    "    N = len(tree.nodes())\n",
    "    #mimic the fitch algo\n",
    "    #propagate up and down in separate graphs\n",
    "    index_up = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    index_down = np.vstack([ [c.matrow, n.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    \n",
    "    connectmat_up = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_down = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    \n",
    "    connectmat_up[index_up[:,0],index_up[:,1]] = 1 \n",
    "    connectmat_down[index_down[:,0],index_down[:,1]] = 1 \n",
    "\n",
    "    diag = [[n,n] for n in range(N)]\n",
    "    connectmat_diag=scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_diag[diag,diag] = 1 \n",
    "    \n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    \n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    \n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    \n",
    "    Norm_nsister= np.array( [ len(n.sister_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    msis =np.amax(Norm_nsister)\n",
    "    Norm_nsister/=msis    \n",
    "    \n",
    "    template_features = np.stack([ntime ,  Norm_nchild , Norm_nsister ]).T    \n",
    "    return connectmat_up, connectmat_down, connectmat_diag, template_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def getmrca(treein,taxset):\n",
    "    tree = copy.deepcopy(treein)\n",
    "    n = tree.mrca(taxon_labels=taxset)\n",
    "    subtree = dendropy.Tree(seed_node=n)\n",
    "    taxset = set([ t.taxon.label if n.taxon else '-1'  for t in subtree.nodes()])\n",
    "    matrows = [ t.matrow for t in subtree.nodes()]\n",
    "    return taxset, matrows , n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse2pairs(sparsemat, matrows = None):\n",
    "    if matrows :\n",
    "        sparsemat = sparsemat[matrows,:]\n",
    "        sparsemat = sparsemat[:,matrows]\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_updown( q, iolock, tree, coglinkdf, bloom, profiles , taxindex , posi_percent = .5 , verbose = False, loop= True):\n",
    "        connectmat_up, connectmat_down, connectmat_diag, template_features = tree2Single_sparse_graph_updown(tree)\n",
    "        \n",
    "        num_total_nodes = template_features.shape[0]\n",
    "        with iolock:\n",
    "            print('dataset size' , len(coglinkdf))\n",
    "        \n",
    "        lineiterator = coglinkdf.iterrows()\n",
    "        N = len(tree.nodes())\n",
    "        \n",
    "        #grab signal from the downward propagation\n",
    "        leafnodes = np.array([n.matrow for n in tree.leaf_nodes()])\n",
    "        overview_connect = scipy.sparse.lil_matrix(( N , 2 ) )\n",
    "        overview_connect[leafnodes,0] = 1\n",
    "        allfams = list(set(coglinkdf.fam1.unique()).union( set(coglinkdf.fam2.unique() ) ))\n",
    "        while True:\n",
    "            toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "            if verbose == True:\n",
    "                print('posi/nega',toss)\n",
    "            if toss == 0:\n",
    "                fam1 = random.choice(allfams)\n",
    "                fam2 = fam1\n",
    "                while fam1 == fam2:\n",
    "                    fam2 = random.choice(allfams)\n",
    "                labels = np.zeros((template_features.shape[0],))\n",
    "            else:\n",
    "                #positive sample\n",
    "                dfline = coglinkdf.sample(n=1).iloc[0]\n",
    "                cog1= str(dfline.group1).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                cog2= str(dfline.group2).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                fam1 = dfline.fam1\n",
    "                fam2 = dfline.fam2\n",
    "                #get the features from fitch and profile\n",
    "                labels = calc_interaction_on_taxonomy(cog1,cog2,tree, bloom)\n",
    "                \n",
    "            \n",
    "            profile1 = profiles[fam1]['tree']\n",
    "            profile2 = profiles[fam2]['tree']\n",
    "            presence1 = set([ n.name  for n in profile1.get_leaves() if n.nbr_genes > 0  ])\n",
    "            presence2 = set([ n.name  for n in profile2.get_leaves() if n.nbr_genes > 0  ])\n",
    "            #restrict graph to clades where both the proteins are found\n",
    "            skip = False\n",
    "            try:\n",
    "                taxset,matrows,n = getmrca(tree,presence1.intersection(presence2))\n",
    "            except ValueError:\n",
    "                #no species overlap\n",
    "                skip = True    \n",
    "            if skip == False:\n",
    "                subconnect_up = sparse2pairs(connectmat_up, matrows)\n",
    "                subconnect_down = sparse2pairs(connectmat_down, matrows)\n",
    "                subdiag = sparse2pairs(connectmat_diag, matrows)\n",
    "                suboverview = overview_connect[matrows,:]                \n",
    "                suboverview = sparse2pairs(suboverview)\n",
    "                \n",
    "                nodefeatures = []\n",
    "                for i,tp in enumerate([profile1,profile2]):    \n",
    "                    profilefeatures = np.zeros((template_features.shape[0],3) )\n",
    "                    #find on which nodes the events happened\n",
    "                    losses = [ taxindex[n.name]  for n in tp.traverse() if n.lost and n.name in taxset ]\n",
    "                    dupl = [ taxindex[n.name]  for n in tp.traverse() if n.dupl and n.name in taxset  ]\n",
    "                    presence = [ taxindex[n.name]  for n in tp.traverse() if n.nbr_genes > 0  and n.name in taxset ]\n",
    "                    profilefeatures[losses, 0] = 1\n",
    "                    profilefeatures[dupl, 1] = 1\n",
    "                    profilefeatures[presence, 2] = 1\n",
    "                    nodefeatures.append(profilefeatures)\n",
    "                \n",
    "                nodefeatures=np.hstack(nodefeatures)\n",
    "                sub_template_features= template_features[matrows,:]\n",
    "                sub_node_features= nodefeatures[matrows,:]\n",
    "                \n",
    "                sub_node_features = np.hstack([sub_template_features , sub_node_features])\n",
    "                \n",
    "                labels = labels[matrows]\n",
    "                \n",
    "                \n",
    "                if verbose == True:\n",
    "                    print('features',nodefeatures)\n",
    "                    print( 'labels' , labels)\n",
    "                \n",
    "                data = HeteroData()            \n",
    "                data['phylonodes_up'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                data['phylonodes_down'].x = torch.tensor(sub_node_features, dtype=torch.double )\n",
    "                data['godnode'].x =torch.tensor(  np.zeros((1,sub_node_features.shape[1]))  ,  dtype=torch.double )\n",
    "\n",
    "                data['phylonodes_up', 'phylolink_up', 'phylonodes_up'].edge_index = torch.tensor(subconnect_up ,  dtype=torch.long )\n",
    "                data['phylonodes_down', 'phylolink_down', 'phylonodes_down'].edge_index = torch.tensor(subconnect_down ,  dtype=torch.long )             \n",
    "                data['phylonodes_up', 'phylolink_up_down', 'phylonodes_down'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "                data['phylonodes_down', 'phylolink_down_up', 'phylonodes_up'].edge_index = torch.tensor( subdiag ,  dtype=torch.long )\n",
    "                \n",
    "                data['phylonodes_down', 'informs', 'godnode'].edge_index = torch.tensor(suboverview,  dtype=torch.long )\n",
    "                data['phylonodes_up', 'informs', 'godnode'].edge_index = torch.tensor(suboverview,  dtype=torch.long )\n",
    "                                \n",
    "                #add 2 label classes                \n",
    "                data['phylonodes_up'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "                data['phylonodes_down'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "                data['godnode'].y =torch.tensor( np.ones((1,))*toss  ,  dtype=torch.long )\n",
    "                \n",
    "                data = T.AddSelfLoops()(data)\n",
    "                data = T.NormalizeFeatures()(data)\n",
    "                q.put(data)\n",
    "                #yield data\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traindata = create_data_updown(dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,   profile_mapper, posi_percent= .5  )\n",
    "#data = next(Traindata)\n",
    "#Testdata = create_data_updown(dendrotree, Datasets['string']['Test'] ,cogfilter , stringprofiles ,  profile_mapper , posi_percent= .5  )\n",
    "#testchunk = [next(Testdata) for i in range(100) ]\n",
    "#test_dataset = DataLoader(testchunk , batch_size = 1)\n",
    "\n",
    "import multiprocessing as mp\n",
    "NCORE = 4\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainq = mp.Queue(maxsize=10 * NCORE)\n",
    "    testq = mp.Queue(maxsize=10 * NCORE)\n",
    "    iolock = mp.Lock()\n",
    "    pool_train = mp.Pool(NCORE, initializer=create_data_updown, initargs=(trainq, iolock  , \n",
    "                dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,  \n",
    "                 profile_mapper, .5  ) )\n",
    "    \n",
    "    pool_test = mp.Pool(NCORE, initializer=create_data_updown, initargs=(testq, iolock  , \n",
    "                dendrotree, Datasets['string']['Test'] ,cogfilter , stringprofiles ,  \n",
    "                 profile_mapper, .5  ) )\n",
    "    \n",
    "    \n",
    "    data = trainq.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                \n",
    "                ('phylonodes_up', 'phylolink', 'phylonodes_up'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                ('phylonodes_down', 'phylolink', 'phylonodes_down'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                \n",
    "                ('phylonodes', 'phylolink', 'phylonodes'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                ('phylonodes', 'informs', 'godnode'):SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            \n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            \n",
    "            x_dict = {key: self.lin(x) for key, x in x_dict.items()}\n",
    "        #1d output for each node\n",
    "        return {key: F.log_softmax(x) for key, x in x_dict.items()}\n",
    "\n",
    "model = HeteroGNN(hidden_channels=50, out_channels=20, num_layers=2)\n",
    "model = model.double()\n",
    "model = model.to(device)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('phylonodes_up', 'phylolink_up', 'phylonodes_up'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                ('phylonodes_down', 'phylolink_down', 'phylonodes_down'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                \n",
    "                ('phylonodes_down', 'phylolink_down_up', 'phylonodes_up'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                ('phylonodes_up', 'phylolink_up_down', 'phylonodes_down'):SAGEConv((-1,-1), hidden_channels ),\n",
    "                \n",
    "                ('phylonodes_up', 'informs', 'godnode'):SAGEConv((-1, -1), hidden_channels),\n",
    "                ('phylonodes_down', 'informs', 'godnode'):SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            \n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            \n",
    "            x_dict = {key: self.lin(x) for key, x in x_dict.items()}\n",
    "        #1d output for each node\n",
    "        return {key: F.log_softmax(x, dim=1) for key, x in x_dict.items()}\n",
    "\n",
    "model = HeteroGNN(hidden_channels=100, out_channels=20, num_layers=1)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict ,data.edge_index_dict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=10**-4)\n",
    "#optimizer=torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=10**-4, momentum=0.01, centered=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('torch_chkpt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = 50\n",
    "for epoch in range(200):\n",
    "    \n",
    "    while trainq.qsize()<100:\n",
    "        sleep(10)\n",
    "    datachunk = [trainq.get() for i in range(100) ]\n",
    "    dataset = DataLoader(datachunk , batch_size =10)\n",
    "    model.train()\n",
    "    losses1=[]\n",
    "    losses2 =[]\n",
    "    losses3 =[]\n",
    "    \n",
    "    for i,data in enumerate(dataset):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        data = data.to(device)\n",
    "        for loop in range(loops):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            #loss = F.nll_loss(out, data.y)\n",
    "            loss1 =  F.nll_loss(out['phylonodes_up'], data['phylonodes_up'].y)\n",
    "            loss2 =  F.nll_loss(out['phylonodes_down'], data['phylonodes_down'].y)\n",
    "            loss3 =  F.nll_loss(out['godnode'], data['godnode'].y)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            losses3.append(float(loss3))\n",
    "            losses2.append(float(loss2))\n",
    "            losses1.append(float(loss1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step(np.mean(losses2))\n",
    "    print(f'loss1: {np.mean(losses1):.4f}')\n",
    "    print(f'loss2: {np.mean(losses2):.4f}')\n",
    "    print(f'loss3: {np.mean(losses3):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    gncorrects=[]\n",
    "    ncorrects=[]\n",
    "    composition_gn = []\n",
    "    composition_n = []\n",
    "    \n",
    "    while trainq.qsize()<100:\n",
    "        sleep(10)\n",
    "    testdatachunk = [testq.get() for i in range(100) ]\n",
    "    testdataset = DataLoader(datachunk , batch_size =10)\n",
    "\n",
    "    for testdata in test_dataset:\n",
    "        testdata = testdata.to(device)\n",
    "        pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "        pred = {idx:x.to('cpu') for idx,x in pred.items()}\n",
    "        correct = (pred['godnode'].argmax(dim=1) == testdata['godnode'].y.cpu()).sum()\n",
    "        \n",
    "        composition_gn.append(testdata['godnode'].y.cpu().sum() / testdata['godnode'].y.cpu().shape[0] )\n",
    "        gncorrects.append(correct)\n",
    "        \n",
    "        correct = (pred['phylonodes_down'].argmax(dim=1) == testdata['phylonodes_down'].y.cpu()).sum()\n",
    "        ncorrects.append(correct)\n",
    "        composition_n.append(testdata['phylonodes_down'].y.cpu().sum() / testdata['phylonodes_down'].y.cpu().shape[0] )\n",
    "    '''torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'torch_chkpt.pt')'''\n",
    "    \n",
    "    acc = np.mean(gncorrects) / int(pred['godnode'].cpu().shape[0])\n",
    "    print(f'gn Accuracy: {acc:.4f}', len(gncorrects), pred['godnode'].shape[0] )\n",
    "    print(f'gn composition: {np.mean(composition_gn) :.4f}', )\n",
    "    \n",
    "    acc = np.mean(ncorrects) / int(pred['phylonodes_down'].cpu().shape[0])\n",
    "    print(f'n Accuracy: {acc:.4f}', len(ncorrects) , pred['phylonodes_down'].shape[0] )\n",
    "    print(f'n composition: {np.mean(composition_n) :.4f}', )\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "truths = []\n",
    "for testdata in test_dataset:\n",
    "    pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "    pred = {idx:x.to('cpu') for idx,x in pred.items()}\n",
    "    \n",
    "    preds.append(np.array(pred['godnode'] ))\n",
    "    truths.append(np.array(testdata['godnode'].y.cpu() ))\n",
    "\n",
    "ROC_curve_single(preds, truths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
