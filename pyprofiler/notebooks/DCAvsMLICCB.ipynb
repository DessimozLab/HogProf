{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/dmoi/projects/HogProf/pyprofiler/notebooks', '/scratch/dmoi/condaenvs/ML2/lib/python39.zip', '/scratch/dmoi/condaenvs/ML2/lib/python3.9', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/lib-dynload', '', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages', '/scratch/dmoi/software/pyham', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/IPython/extensions', '/users/dmoi/.ipython', '../..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dir': {'datadir': '/scratch/dmoi/datasets/birds/', 'omadir': '/scratch/dmoi/datasets/birds/'}, 'orthoxmltar': '', 'email': 'dmoi@unil.ch'}\n"
     ]
    }
   ],
   "source": [
    "#using the profiler. It's easy!\n",
    "#lets import the profiler, configuration file and some tools for GO analysis to look at our returned results\n",
    "from pyprofiler.utils import hashutils\n",
    "import ete3\n",
    "import random\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.utils.hashutils as hashutils\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pyprofiler.profiler as profiler\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import redis\n",
    "##to get the mapping of oma hogs to cogs to interactions in specific species I used dask distributed and a redis server\n",
    "#you may need to get these up and running for you own cluster configuration before this notebook will work for you\n",
    "\n",
    "import dask\n",
    "import scipy\n",
    "from dask import dataframe as dd\n",
    "import pickle\n",
    "from bloom_filter2 import BloomFilter\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lsh\n",
      "indexing lsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/tables/leaf.py:367: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  warnings.warn(\"\"\"\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#lets load a compiled db containing the OMA root HOGs into a profiler oject \n",
    "p = profiler.Profiler(lshforestpath = '/scratch/dmoi/datasets/all/newlshforest.pkl' , hashes_h5='/scratch/dmoi/datasets/birds/all/hashes.h5' , mat_path= None, oma = '/scratch/dmoi/datasets/OMA/apr2021/OmaServer.h5', tar= None , nsamples = 256 , mastertree = '/scratch/dmoi/datasets/birds/all_test_master_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabHog(ID, verbose = True):\n",
    "    try:\n",
    "        entry = p.db_obj.entry_by_entry_nr(p.db_obj.id_resolver.resolve(ID))\n",
    "        return entry[4].decode() , entry\n",
    "    except:\n",
    "        return np.nan,np.nan\n",
    "#map to OMA HOGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap = '/scratch/dmoi/datasets/humap_PPI/humap2_ppis_ACC_20200821.pairsWprob'\n",
    "calc_humap = False\n",
    "if calc_humap == True:\n",
    "    #load humap data\n",
    "    humap_df = pd.read_table(humap, header = None)\n",
    "    print(humap_df)\n",
    "    humap_df = humap_df[humap_df[2] > .75 ]\n",
    "    mapper = set( list(humap_df[1]) + list(humap_df[0]) )\n",
    "    mapper = { protid: grabHog(protid) for protid in mapper }\n",
    "    humap_df['hog1'] = humap_df[1].map(mapper)\n",
    "    humap_df['hog2'] = humap_df[0].map(mapper)\n",
    "    humap_df['hogid_1'] = humap_df['hog1'].map(lambda x:x[0])\n",
    "    humap_df['hogid_2'] = humap_df['hog2'].map(lambda x:x[0])\n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df['fam1'] = humap_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "    humap_df['fam2'] = humap_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "    humap_df = humap_df.dropna()\n",
    "    humap_df.fam1 = humap_df.fam1.map(int)\n",
    "    humap_df.fam2 = humap_df.fam2.map(int)\n",
    "    print(len(humap_df))\n",
    "    humap_df.to_csv(humap+'hogmapped.csv')\n",
    "else:\n",
    "    humap_df = pd.read_csv(humap+'hogmapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "humap_pairs = humap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "calc_hogs_humap = False\n",
    "if calc_hogs_humap == True:\n",
    "    allhogs = set([])\n",
    "    allhogs = allhogs.union( set(humap_df.fam1.unique() ) )\n",
    "    allhogs = allhogs.union( set(humap_df.fam2.unique() ) )\n",
    "    print(len(allhogs))\n",
    "    for fam in allhogs:\n",
    "        print(fam)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "        except:\n",
    "            print('err', fam)\n",
    "        print(prof)\n",
    "        profiles.update(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Hogs_humap = False\n",
    "if save_Hogs_humap == True:\n",
    "    with open(humap + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(humap + 'gold_standard_profiles.pkl' , 'rb') as profiles_out:\n",
    "    humap_profiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      mat  \\\n",
      "524288  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "516097  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "516099  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "786438  [[1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0,...   \n",
      "573452  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                     tree  \n",
      "524288  (((\\n--7757)), ((\\n--7764)), ((\\n--9597)), ((\\...  \n",
      "516097  (((\\n--9597)), ((\\n--9598)), ((\\n--9606)), ((\\...  \n",
      "516099  (((\\n--9597)), ((\\n--9598)), ((\\n--9606)), ((\\...  \n",
      "786438  (((\\n--868595)), ((\\n--349161)), ((\\n--696281)...  \n",
      "573452  (((\\n--120017)), ((\\n--237631)), ((\\n--1230383...  \n"
     ]
    }
   ],
   "source": [
    "humap_df = pd.DataFrame.from_dict(humap_profiles , orient = 'index')\n",
    "print(humap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coglink_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                 group1  group2 neighborhood fusion cooccurence coexpression experimental database textmining combined_score\n",
      "npartitions=195                                                                                                             \n",
      "                 object  object        int64  int64       int64        int64        int64    int64      int64          int64\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "...                 ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "Dask Name: read-csv, 195 tasks\n",
      "Index(['group1', 'group2', 'neighborhood', 'fusion', 'cooccurence',\n",
      "       'coexpression', 'experimental', 'database', 'textmining',\n",
      "       'combined_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(coglink_df)\n",
    "print(coglink_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['neighborhood', 'fusion', 'cooccurence', 'combined_score' ]\n",
    "coglink_df = coglink_df.drop(columns = dropcols)\n",
    "coglink_df['score'] = coglink_df.coexpression + coglink_df.experimental +coglink_df.database+ coglink_df.textmining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500428\n"
     ]
    }
   ],
   "source": [
    "coglink_df= coglink_df[coglink_df.score>1000]\n",
    "coglink_df = coglink_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coglink_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabcogs = set( list(coglink_df.group1.unique()) + list(coglink_df.group2.unique()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabcogs= list(grabcogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the interacting cogs to the proteins\n",
    "COGmapings_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt', blocksize=25e6 , header = 0, sep = '\\t')\n",
    "COGmapings_df = COGmapings_df.set_index('orthologous_group')\n",
    "COGmapings_df.astype(str)\n",
    "COGmapings_df['##protein'].map( lambda x : x.strip() )\n",
    "COGmapings_df['species'] = COGmapings_df['##protein'].map( lambda x : x.split('.')[0] )\n",
    "COGmapings_df['COG'] = COGmapings_df.index\n",
    "COGmapings_df = COGmapings_df.loc[grabcogs]\n",
    "print(COGmapings_df.head())\n",
    "COGmapings_df = COGmapings_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take the proteins in our cogs of interest\n",
    "compute_grabcogs = False\n",
    "if compute_grabcogs == True:\n",
    "    grabprots =list(COGmapings_df['##protein'].unique())\n",
    "    print(len(grabprots))\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabcogs ]) )\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'w') as protsout:\n",
    "        protsout.write(''.join([ p + '\\n' for p in grabprots ]) )\n",
    "else:\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'r') as protsout:\n",
    "        grabcogs = [ cog for cog in protsout.readlines()]\n",
    "    with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'r') as protsout:\n",
    "        grabprots = [ prot for prot in protsout.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_mappers = False\n",
    "rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "if calc_mappers == True:\n",
    "    count = 0 \n",
    "    for i,r in COGmapings_df.iterrows():\n",
    "        rdb.set(r['##protein'], i)\n",
    "        count+=1\n",
    "        if count < 10:\n",
    "            print(i+'\\n',r)\n",
    "        if count%1000000==0:\n",
    "            print(count/len(COGmapings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(grabcogs))\n",
    "print(len(COGmapings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maphogs = False\n",
    "if maphogs == True:\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    hogmap = {}\n",
    "    for i,prot in enumerate(grabprots):\n",
    "        if i % 100000 == 0 :\n",
    "            print(i/len(grabprots))\n",
    "        cog = rdb.get(prot)\n",
    "        if cog not in hogmap:\n",
    "            mapped =  grabHog(prot)\n",
    "            #retry until something maps\n",
    "            if mapped[0] != np.nan and type(mapped[0]) == str :\n",
    "                if len(mapped[0])>1 :\n",
    "                    hogmap[cog] = mapped\n",
    "    with open('stringhogmap.pkl' , 'wb')as hogmapout:\n",
    "        hogmapout.write(pickle.dumps(hogmap))\n",
    "else:\n",
    "    with open('stringhogmap.pkl' , 'rb')as hogmapout:\n",
    "        hogmap = pickle.loads(hogmapout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hogmap))\n",
    "for i, key in enumerate(hogmap):\n",
    "    if i < 10:\n",
    "        print(key, hogmap[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the HOGs to the COGdf\n",
    "#grab the corresponding profiles\n",
    "print(len(coglink_df))\n",
    "try:\n",
    "    coglink_df.group1  = coglink_df.group1.map( lambda x : x.encode())\n",
    "    coglink_df.group2  = coglink_df.group2.map( lambda x : x.encode())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "coglink_df['hog1'] = coglink_df.group1.map(hogmap)\n",
    "coglink_df['hog2'] = coglink_df.group2.map(hogmap)\n",
    "coglink_df=coglink_df.dropna()\n",
    "print(len(coglink_df))\n",
    "print(coglink_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coglink_df['hogid_1'] = coglink_df['hog1'].map(lambda x:x[0])\n",
    "coglink_df['hogid_2'] = coglink_df['hog2'].map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coglink_df['fam1'] = coglink_df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "coglink_df['fam2'] = coglink_df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "\n",
    "coglink_df.fam1 = coglink_df.fam1.map(int)\n",
    "coglink_df.fam2 = coglink_df.fam2.map(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coglink_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringHOGs = set(coglink_df.fam1.unique()).union(set(coglink_df.fam2.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stringHOGs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coglink_df.to_csv('STRINGCOGS2OMAHOGS.csv')\n",
    "stringPairs = coglink_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derive explicit profiles for our hogs of interest in string\n",
    "calc_hogs_string = False\n",
    "stringprofiles = {}\n",
    "if calc_hogs_string == True:\n",
    "    print('profiles to calclulate',len(stringHOGs))\n",
    "    for i,fam in enumerate(stringHOGs):\n",
    "        if i % 100 ==0:\n",
    "            print(i)\n",
    "        try:\n",
    "            prof = p.return_profile_OTF(fam)\n",
    "            stringprofiles.update(prof)\n",
    "        except:\n",
    "            print('err',fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_hogs_string == True:\n",
    "    with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "        profiles_out.write(pickle.dumps(stringprofiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/dmoi/datasets/STRING/' + 'gold_standard_profiles.pkl' , 'rb' )as profiles_out:\n",
    "    stringprofiles = pickle.loads(profiles_out.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = pd.DataFrame.from_dict(stringprofiles , orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the profiles for this small set of HOGs\n",
    "for i, key in enumerate(stringprofiles):\n",
    "    if i < 10:\n",
    "        print(key,stringprofiles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have profiles for all HUMAP and COG interactions\n",
    "#String has interactions from each COG in different species.\n",
    "#We need a way to check for the presence of interaction within a species for a COG\n",
    "#for this we will create a bloom filter with all the interactions between our cogs\n",
    "\n",
    "calc_filter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    from dask.distributed import fire_and_forget\n",
    "    from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    from dask.distributed import  utils_perf\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    import dask\n",
    "    import redis\n",
    "    from bloom_filter2 import BloomFilter\n",
    "    import lzma\n",
    "    from dask import dataframe as dd\n",
    "    distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        NCORE = 5\n",
    "        print('deploying cluster')\n",
    "        cluster = SLURMCluster(\n",
    "            walltime='4:00:00',\n",
    "            n_workers = NCORE,\n",
    "            cores=NCORE,\n",
    "            processes = NCORE,\n",
    "            interface='ib0',\n",
    "            memory=\"80GB\",\n",
    "            env_extra=[\n",
    "            'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "            'conda activate ML2'\n",
    "            ],\n",
    "            scheduler_options={'interface': 'ens2f0' },\n",
    "            #extra=[\"--lifetime\", \"3h55m\", \"--lifetime-stagger\", \"4m\"]\n",
    "        )\n",
    "        print(cluster.job_script())\n",
    "\n",
    "    else:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    if distributed == True:\n",
    "        print(cluster)\n",
    "        cluster.scale(jobs = 100)\n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "if calc_filter == True:\n",
    "    link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.physical.links.detailed.v11.5.txt', blocksize=100e6 , header = 0, sep = ' ')\n",
    "    print(link_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "@dask.delayed\n",
    "def mapcogs(df ):\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    if type( df ) == tuple:\n",
    "        df = df[0]\n",
    "    protlist1 = list(df.protein1.map(lambda x:str(x).strip()))\n",
    "    protlist2 = list(df.protein2.map(lambda x:str(x).strip()))\n",
    "    protlist = list(set(protlist1+protlist2))\n",
    "    data = rdb.mget(protlist)\n",
    "    mapper = dict(zip(protlist, data) )\n",
    "    df['COG1'] = df.protein1.map(mapper)\n",
    "    df['COG2'] = df.protein2.map(mapper)\n",
    "    df = df.dropna()\n",
    "    df['COG1'] = df.COG1.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['COG2'] = df.COG2.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['species'] = df.protein1.map(lambda x:x.split('.')[0])\n",
    "    df['coglinks'] = df.COG1 + '_' + df.COG2 + '_' + df.species\n",
    "    return  list(df.coglinks.unique())\n",
    "\n",
    "@dask.delayed\n",
    "def return_filter(coglinks, verbose = True):\n",
    "    if type( coglinks ) == tuple:\n",
    "        coglinks = coglinks[0]\n",
    "    #prots can only interact in one species...\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in coglinks:\n",
    "        b.add( p )\n",
    "    #b.close()\n",
    "    #b = lzma.compress( pickle.dumps(b) )\n",
    "    return   b , len(coglinks)\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    #f1 = pickle.loads(lzma.decompress( f1) )\n",
    "    #f2 = pickle.loads(lzma.decompress( f2 ) )\n",
    "    f3 = f1.__ior__(f2)\n",
    "    #f3 = lzma.compress( pickle.dumps(f3) )\n",
    "    return f3 , total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    partitions  = link_df.to_delayed()\n",
    "    #print('reduce partitions')\n",
    "    #res = [ reduce(p,remote_list) for p in partitions ] \n",
    "    #print('done')\n",
    "    print('map cogs')\n",
    "    res1 = [ mapcogs(p) for p in partitions ] \n",
    "    print('done')\n",
    "    print('make filters')\n",
    "    res2 = [ return_filter(p) for p in res1 ] \n",
    "\n",
    "    totalfilter = res2\n",
    "    print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        print(len(totalfilter))\n",
    "\n",
    "    #resfinal = dask.compute(totalfilter)\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    resfinal = dask.compute(totalfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cancel(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    with open('bloomfinal.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(resfinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bloomfinal.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "cogfilter =  resfinal[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('COG1756_COG0088_4113' in cogfilter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try it out. we should be able to find in which species two cogs interact with the bloom filter\n",
    "cog1='COG0088'\n",
    "cog2 ='COG1756'\n",
    "coglink = cog1 + '_' + cog2 + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out to find the species for a cog pair\n",
    "coglinks_species = [ coglink+spec.name  for spec in p.tree.get_leaves()]\n",
    "species = [ spec.name  for spec in p.tree.get_leaves()]\n",
    "checklinks = [ coglink+spec in cogfilter for spec in coglinks_species ]\n",
    "species_set = [s for s in species]\n",
    "links = [ l for l,c in list(zip(coglinks_species,checklinks))  if c== True  ]\n",
    "species_set = [ s for s,c in list(zip(species_set,checklinks))  if c== True  ]\n",
    "print(links)\n",
    "print(species_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapfam to matrow\n",
    "humap_fam_map= { f:i for i,f in enumerate(humap_df.index)}\n",
    "humap_profilemat = np.vstack(humap_df.mat)\n",
    "print(humap_profilemat.shape)\n",
    "string_fam_map = { f:i for i,f in enumerate(string_df.index)}\n",
    "string_mat = np.vstack(string_df.mat)\n",
    "print(string_mat.shape)\n",
    "#train test split\n",
    "Datasets = {}\n",
    "for label,df,mapping,profilemat in [  ('string', stringPairs, string_fam_map ,string_mat ) , ('humap',humap_pairs, humap_fam_map , humap_profilemat) ]: \n",
    "    keys = set(mapping.keys())\n",
    "    entry1 = [ f in keys for f in df.fam1]\n",
    "    df = df.iloc[entry1]\n",
    "    entry2 = [ f in keys for f in df.fam2]\n",
    "    df = df.iloc[entry2]\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    df_train = df.iloc[msk]\n",
    "    df_test = df.iloc[~msk]\n",
    "    Datasets[label]={'Train':df_train,'Test':df_test , 'mapping': mapping , 'mat':profilemat }\n",
    "    print(label)\n",
    "    print('train',len(df_train))\n",
    "    print('test',len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(df, n):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i + n]\n",
    "def generateXYchunk(explicit_profiles, goldstandardDF,fam_map,  nsamples=100, posi_percent = .5):\n",
    "    #shuffle\n",
    "    goldstandardDF = goldstandardDF.sample(frac=1)\n",
    "    for chunkdf in chunks(goldstandardDF , int( nsamples*posi_percent)):\n",
    "        #negatives drawn from the overall dataset\n",
    "        X = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam1]) , np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam2]) ] )\n",
    "        Y = [1]* X.shape[0]\n",
    "        neg1 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam1)\n",
    "        neg2 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam2)\n",
    "        \n",
    "        if len(neg1)>0:\n",
    "            mixchunk = np.hstack([np.vstack([profilemat[fam_map[f]] for f in neg1]),np.vstack([profilemat[fam_map[f]] for f in neg2])])\n",
    "            Y =np.hstack([[0]* mixchunk.shape[0] , Y])\n",
    "            X= np.vstack([mixchunk,X])    \n",
    "        #positive samples\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def ROC_curve(y_data, label = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot( recall, precision , label= l )\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a vanilla deep NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "overwrite = True\n",
    "train_dnn = False\n",
    "\n",
    "if train_dnn == True:\n",
    "    for dataset in Datasets:\n",
    "        modelpath = './'+dataset+'_dropout_DNN.h5'\n",
    "        callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir= modelpath+'.logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=modelpath)\n",
    "        ]\n",
    "        print(dataset)\n",
    "        df_train = Datasets[label]['Train']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        if os.path.exists(modelpath) and overwrite == False:\n",
    "            model = load_model(modelpath)\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=100, activation='sigmoid', input_dim=profilemat.shape[1]*2))\n",
    "            model.add(tf.keras.layers.Dropout( .2 , seed=42 ))\n",
    "            model.add(Dense(units=30, activation='sigmoid' ) )\n",
    "            model.add(Dense(units=1, activation='sigmoid' ) )\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        generator = generateXYchunk(profilemat, df_train, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        sample = next(generator)\n",
    "        model.fit(itertools.cycle(generator) , steps_per_epoch = 300 , epochs = 50, callbacks=callbacks)\n",
    "        # Save the model\n",
    "        model.save(modelpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal distance metrics to try\n",
    "\n",
    "from scipy.spatial.distance import euclidean , hamming, jaccard\n",
    "from sklearn.covariance import empirical_covariance\n",
    "from scipy.stats import pearsonr\n",
    "from keract import get_activations\n",
    "def pearsonR(v1,v2):\n",
    "        return -pearsonr(v1,v2)[0]\n",
    "\n",
    "visualize = False    \n",
    "if visualize == True:\n",
    "    for label in Datasets:\n",
    "        print(label)\n",
    "        df_test = Datasets[label]['Test']\n",
    "        fam_map = Datasets[label]['mapping']\n",
    "        profilemat = Datasets[label]['mat']\n",
    "        ydata =  {}\n",
    "\n",
    "        for func, name in [ (euclidean, 'Euclidean' ), (hamming,'Hamming') , (jaccard,'Jaccard') , (pearsonR,'Pearson') ]:\n",
    "            print(name)\n",
    "            generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "            #test all the easy metrics\n",
    "            ypreds = []\n",
    "            ytruth = []\n",
    "            for X,y in generator:\n",
    "                #distances\n",
    "                x1 = X[:,0:int(X.shape[1]/2)]\n",
    "                x2 = X[:,int(X.shape[1]/2):]\n",
    "                predictions = np.array([ -func(x1[r,:],x2[r,:] ) for r in range(x1.shape[0]) ])\n",
    "                ypreds.append(predictions)\n",
    "                ytruth.append(y)\n",
    "            ytest= np.hstack(ytruth)\n",
    "            ypred = np.hstack(ypreds)\n",
    "            ydata[name] = { 'Ypred': ypred , 'Ytrue':ytest} \n",
    "        #get DNN values\n",
    "        print('DNN')\n",
    "        generator = generateXYchunk(profilemat, df_test, fam_map , posi_percent= .5 , nsamples = 50 )\n",
    "        mats = [(x,y) for x,y in generator ]\n",
    "        y_test = np.hstack([y for x,y in mats])\n",
    "        modelpath = './'+label+'_dropout_DNN.h5'\n",
    "        if os.path.exists(modelpath):\n",
    "            model = load_model(modelpath)\n",
    "\n",
    "\n",
    "        ypred = np.vstack([ model.predict(x) for x,y in mats])\n",
    "\n",
    "\n",
    "        #activations = [ get_activations(model, x , auto_compile=True) for x,y in mats] \n",
    "        #activations = activations[list(activations.keys())[0]]\n",
    "        #print( activations.shape)\n",
    "        #n_samples = activations.shape[0]\n",
    "        #activations = np.sum(activations,axis =0)/n_samples\n",
    "        #representations = np.sum(X_test, axis = 0)\n",
    "\n",
    "        #print(activations.shape)\n",
    "\n",
    "        #np.save(modelpath + 'activation.np' , activations)\n",
    "        #np.save(modelpath + 'representation.np', representations)\n",
    "\n",
    "        ydata['DNN'] ={ 'Ypred': ypred , 'Ytrue':ytest}\n",
    "        #plot ROC\n",
    "        print(ydata)\n",
    "        ROC_curve(ydata , label = label)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################being the graph NN part of the paper #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dendropy\n",
    "taxnwk = '/scratch/dmoi/datasets/birds/all_test_master_tree.nwk'\n",
    "with open( 'taxtree.nwk' , 'w') as treeout:\n",
    "    treeout.write(p.tree.write())\n",
    "dendrotree = dendropy.Tree.get(\n",
    "        data=p.tree.write(format=3),\n",
    "        rooting=\"force-rooted\",\n",
    "        suppress_internal_node_taxa=False,\n",
    "        schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set([ n.name for n in p.tree.traverse() ])\n",
    "dendrotree_nodes = set([str(n.taxon.label) if n.taxon else '-1' for n in dendrotree.nodes()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(dendrotree_nodes))\n",
    "print(len(nodes.intersection(dendrotree_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { n.name:i for i,n in enumerate(p.tree.traverse()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(dendrotree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_mapper = { (n.taxon.label if n.taxon else '-1'):n.matrow for n in  dendrotree.nodes() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smallpars\n",
    "#we're checking for interaction in a subset of species and propagating up\n",
    "allowed_symbols =set([0,1])\n",
    "transition_dict = { (c[0],c[1]):i for i,c in enumerate(itertools.permutations(allowed_symbols,2) ) }\n",
    "\n",
    "def calc_interaction_on_taxonomy(cog1,cog2,tree, bloom , verbose = False):\n",
    "    #set interaction states\n",
    "    #look for interactions in bloom\n",
    "    for i,l in enumerate(tree.leaf_nodes()):\n",
    "        l.event = {}\n",
    "        l.scores = {}\n",
    "        l.symbols = {}\n",
    "        l.scores = { c:10**10 for c in allowed_symbols }\n",
    "        if cog1+'_'+cog2+'_'+l.taxon.label in bloom:\n",
    "            if verbose == True:\n",
    "                print(l.taxon.label)\n",
    "            l.char= 1\n",
    "            l.symbols = {1}\n",
    "        else:\n",
    "            l.char= 0\n",
    "            l.symbols = {0}\n",
    "        l.scores[l.char] = 0\n",
    "    t = smallpars.calculate_small_parsimony(tree ,allowed_symbols, transition_dict)\n",
    "    return  t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2Single_sparse_graph(tree):\n",
    "    N = len(tree.nodes())\n",
    "    index = np.array([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    #features that apply to every node\n",
    "    #lengths = np.array([ c.edge_length for n in tree.nodes() for c in n.child_nodes()])\n",
    "    #lengths /= np.amax(lengths)\n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    #totalnodes = len(tree.nodes())\n",
    "    #Norm_ndescendant = np.array([ n./totalnodes for n in tree.nodes() ])\n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    template_features = np.stack([ntime ,  Norm_nchild  ]).T\n",
    "    connectmat = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat[index[:,0],index[:,1]] = 1\n",
    "    diag = np.array([ i for i in range(connectmat.shape[0]) ] )\n",
    "    connectmat = scipy.sparse.find(connectmat)\n",
    "    edge_lens=connectmat[2]\n",
    "    c = np.vstack([connectmat[0],connectmat[1]])\n",
    "    \n",
    "    #get adjacency and node IDs\n",
    "    return c, template_features , edge_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "#create graphs on the fly to represent pairs of profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data( tree, coglinkdf, bloom, profiles ,  fam_map , taxindex , posi_percent= .5 , verbose = False, loop= True):     \n",
    "        connectmat, template_features, edge_lens = tree2Single_sparse_graph(tree)\n",
    "        num_total_nodes = template_features.shape[0]\n",
    "        print('dataset size' , len(coglinkdf))\n",
    "        lineiterator = coglinkdf.iterrows()\n",
    "        leafnodes = [n.matrow for n in tree.leaf_nodes()]\n",
    "        allnodes = [n.matrow for n in tree.leaf_nodes()]\n",
    "        overview_connect= np.zeros((2, 2* (len(leafnodes) + len(allnodes)) ))\n",
    "        overview_connect[0,0:len(leafnodes)] = leafnodes\n",
    "        overview_connect[1,0:len(leafnodes)] = 1\n",
    "        overview_connect[1,len(leafnodes):len(leafnodes)+len(allnodes)] = allnodes       \n",
    "        godnode_connect= np.array([[0,1],[1,0]])\n",
    "        if loop == True:\n",
    "            lineiterator = itertools.cycle(lineiterator)\n",
    "        allfams = list(set(coglinkdf.fam1.unique()).union( set(coglinkdf.fam2.unique() ) ))\n",
    "        #for each pair in the dataset / non-coev pair\n",
    "        #coin toss to decide whether the sample is positive or negative\n",
    "        while True:\n",
    "            toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "            if verbose == True:\n",
    "                print('posi/nega',toss)\n",
    "            #toss = 1\n",
    "            if toss == 0:\n",
    "                fam1 = random.choice(allfams)\n",
    "                fam2 = fam1\n",
    "                #make sure they arent the same\n",
    "                while fam1 == fam2:\n",
    "                    fam2 = random.choice(allfams)\n",
    "                #profile features\n",
    "                labels = np.zeros((template_features.shape[0],))\n",
    "            else:\n",
    "                #positive sample\n",
    "                idx,dfline = next(lineiterator)\n",
    "                cog1= str(dfline.group1).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                cog2= str(dfline.group2).replace(\"b\",'').replace(\"'\",'').strip()\n",
    "                fam1 = dfline.fam1\n",
    "                fam2 = dfline.fam2\n",
    "                #get the features from fitch and profile\n",
    "                t = calc_interaction_on_taxonomy(cog1,cog2,tree, bloom)\n",
    "                labels = [ n.char for n in t.nodes() ]\n",
    "            profile1 = profiles[fam1]['tree']\n",
    "            profile2 = profiles[fam2]['tree']\n",
    "            nodefeatures = np.zeros((template_features.shape[0],6) )\n",
    "            for i,tp in enumerate([profile1,profile2]):\n",
    "                #find on which nodes the events happened\n",
    "                losses = [ taxindex[n.name]  for n in tp.traverse() if n.lost and n.name in taxindex ]\n",
    "                dupl = [ taxindex[n.name]  for n in tp.traverse() if n.dupl  and n.name in taxindex  ]\n",
    "                presence = [ taxindex[n.name]  for n in tp.traverse() if n.nbr_genes > 0  and n.name in taxindex ]\n",
    "                nodefeatures[ losses, 2*i] = 1\n",
    "                nodefeatures[dupl, 2*i+1] = 1\n",
    "                nodefeatures[presence, 2*i+2] = 1\n",
    "            nodefeatures = np.hstack([nodefeatures,template_features])\n",
    "            \n",
    "            if verbose == True:\n",
    "                print('features',nodefeatures)\n",
    "                print( 'labels' , labels)\n",
    "            data = HeteroData()            \n",
    "            data['phylonodes'].x = torch.tensor(nodefeatures, dtype=torch.double )\n",
    "            data['phylonodes', 'phylolink', 'phylonodes'].edge_index = torch.tensor(connectmat ,  dtype=torch.long )\n",
    "            data['godnode', 'lordsover', 'phylonodes'].edge_index = torch.tensor(overview_connect,  dtype=torch.long )\n",
    "            data['godnode', 'integrates' , 'godnode'].edge_index = torch.tensor(godnode_connect,  dtype=torch.long )\n",
    "            data['godnode'].x =torch.tensor(  np.zeros((2,nodefeatures.shape[1]))  ,  dtype=torch.double )\n",
    "            data['phylonodes'].y =torch.tensor(labels  ,  dtype=torch.long )\n",
    "            data['godnode'].y =torch.tensor( np.ones((2,))*toss  ,  dtype=torch.long )\n",
    "            \n",
    "            data = T.ToUndirected()(data)\n",
    "            data = T.AddSelfLoops()(data)\n",
    "            data = T.NormalizeFeatures()(data)\n",
    "            \n",
    "            yield data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traindata = create_data(dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "data = next(Traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('phylonodes', 'phylolink', 'phylonodes'):SAGEConv(-1, hidden_channels , bias = False),\n",
    "                ('godnode', 'lordsover', 'phylonodes'): SAGEConv((-1, -1), hidden_channels, bias = False),\n",
    "                ('godnode', 'integrates' , 'godnode'):SAGEConv((-1, -1), hidden_channels, bias = False),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "        self.lin = Linear(hidden_channels, out_channels, bias = False)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            \n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            x_dict = {key: self.lin(x) for key, x in x_dict.items()}\n",
    "        #1d output for each node\n",
    "        return {key: F.log_softmax(x,dim=1) for key, x in x_dict.items()}\n",
    "\n",
    "model = HeteroGNN(hidden_channels=20, out_channels=20,\n",
    "                  num_layers=2)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict ,data.edge_index_dict)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0)\n",
    "#optimizer=torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=10**-4, momentum=0.01, centered=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata = create_data(dendrotree, Datasets['string']['Test'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "testchunk = [next(Testdata) for i in range(100) ]\n",
    "test_dataset = DataLoader(testchunk , batch_size = 10)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = 50\n",
    "\n",
    "for epoch in range(200):\n",
    "    try:\n",
    "        datachunk = [next(Traindata) for i in range(100) ]\n",
    "    except:\n",
    "        Traindata = create_data(dendrotree, Datasets['string']['Train'] ,cogfilter , stringprofiles ,  profile_mapper , profile_mapper, posi_percent= .5  )\n",
    "        datachunk = [next(Traindata) for i in range(100) ]\n",
    "    \n",
    "    dataset = DataLoader(datachunk , batch_size =100)\n",
    "    model.train()\n",
    "    losses1=[]\n",
    "    losses2 =[]\n",
    "    for loop in range(loops):\n",
    "        for data in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            #loss = F.nll_loss(out, data.y)\n",
    "            loss1 =  F.nll_loss(out['phylonodes'], data['phylonodes'].y)\n",
    "            loss2 =  F.nll_loss(out['godnode'], data['godnode'].y)\n",
    "            loss = loss1 + loss2\n",
    "            losses2.append(float(loss2))\n",
    "            losses1.append(float(loss1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(np.mean(losses2))\n",
    "        if loop%10 == 0 :\n",
    "            print(f'loss1: {np.mean(losses1):.4f}')\n",
    "            print(f'loss2: {np.mean(losses2):.4f}')\n",
    "    model.eval()\n",
    "    gncorrects=[]\n",
    "    ncorrects=[]\n",
    "    composition_gn = []\n",
    "    composition_n = []\n",
    "    \n",
    "    for testdata in test_dataset: \n",
    "        pred = model(data.x_dict ,data.edge_index_dict)\n",
    "        correct = (pred['godnode'].argmax(dim=1) == data['godnode'].y).sum()\n",
    "        composition_gn.append(data['godnode'].y.sum() / data['godnode'].y.shape[0] )\n",
    "        gncorrects.append(correct)\n",
    "        correct = (pred['phylonodes'].argmax(dim=1) == data['phylonodes'].y).sum()\n",
    "        ncorrects.append(correct)\n",
    "        composition_n.append(data['phylonodes'].y.sum() / data['phylonodes'].y.shape[0] )\n",
    "\n",
    "    \n",
    "    acc = np.mean(gncorrects) / int(pred['godnode'].shape[0])\n",
    "    print(f'gn Accuracy: {acc:.4f}', len(gncorrects), pred['godnode'].shape[0] )\n",
    "    print(f'gn composition: {np.mean(composition_gn) :.4f}', )\n",
    "    \n",
    "    acc = np.mean(ncorrects) / int(pred['phylonodes'].shape[0])\n",
    "    print(f'n Accuracy: {acc:.4f}', len(ncorrects) , pred['phylonodes'].shape[0] )\n",
    "    print(f'n composition: {np.mean(composition_n) :.4f}', )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
