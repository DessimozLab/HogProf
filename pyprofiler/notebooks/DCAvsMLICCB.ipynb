{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/dmoi/projects/HogProf/pyprofiler/notebooks', '/scratch/dmoi/condaenvs/ML2/lib/python39.zip', '/scratch/dmoi/condaenvs/ML2/lib/python3.9', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/lib-dynload', '', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages', '/scratch/dmoi/software/pyham', '/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/IPython/extensions', '/users/dmoi/.ipython', '../..']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idea! try graph net on the species tree with the same approach as covid\n",
    "#predict connection nodes and score on the human result\n",
    "\n",
    "\n",
    "#hilbert space analysis?\n",
    "\n",
    "#try blurring approach with species tree connectivity mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dir': {'datadir': '/scratch/dmoi/datasets/birds/', 'omadir': '/scratch/dmoi/datasets/birds/'}, 'orthoxmltar': '', 'email': 'dmoi@unil.ch'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/ete3/evol/parser/codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if line is '\\n':\n"
     ]
    }
   ],
   "source": [
    "#using the profiler. It's easy!\n",
    "#lets import the profiler, configuration file and some tools for GO analysis to look at our returned results\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.utils.hashutils as hashutils\n",
    "import seaborn as sns\n",
    "\n",
    "import pyprofiler.profiler as profiler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lsh\n",
      "indexing lsh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/tables/leaf.py:367: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  warnings.warn(\"\"\"\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#lets load a compiled db containing the OMA root HOGs into a profiler oject \n",
    "p = profiler.Profiler(lshforestpath = '/scratch/dmoi/datasets/all/newlshforest.pkl' , hashes_h5='/scratch/dmoi/datasets/birds/all/hashes.h5' , mat_path= None, oma = '/scratch/dmoi/datasets/OMA/apr2021/OmaServer.h5', tar= None , nsamples = 256 , mastertree = '/scratch/dmoi/datasets/birds/all_test_master_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabHog(ID, verbose = True):\n",
    "    try:\n",
    "        entry = p.db_obj.entry_by_entry_nr(p.db_obj.id_resolver.resolve(ID))\n",
    "        return entry[4].decode() , entry\n",
    "    except:\n",
    "        return np.nan,np.nan\n",
    "#map to OMA HOGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0                1             2\n",
      "0            P46926           Q8TDQ7  1.000000e+00\n",
      "1            P43631           Q14954  1.000000e+00\n",
      "2            P43631           Q14953  1.000000e+00\n",
      "3            P43631           P43629  1.000000e+00\n",
      "4         100287045           Q86YD7  1.000000e+00\n",
      "...             ...              ...           ...\n",
      "17526306     Q9H6Z4           Q7L1Q6  3.000000e-14\n",
      "17526307     Q9Y265           P54578  3.000000e-14\n",
      "17526308     P16152           P04406  3.000000e-14\n",
      "17526309     Q96AE4           P78417  3.000000e-14\n",
      "17526310     P20042  ENSG00000180574  3.000000e-14\n",
      "\n",
      "[17526311 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#load humap data\n",
    "humap = '/scratch/dmoi/datasets/humap_PPI/humap2_ppis_ACC_20200821.pairsWprob'\n",
    "df = pd.read_table(humap, header = None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/tables/leaf.py:367: PerformanceWarning: The Leaf ``/_i_XRef/XRefId/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  warnings.warn(\"\"\"\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0       1         2  \\\n",
      "0        P46926  Q8TDQ7  1.000000   \n",
      "1        P43631  Q14954  1.000000   \n",
      "2        P43631  Q14953  1.000000   \n",
      "3        P43631  P43629  1.000000   \n",
      "4     100287045  Q86YD7  1.000000   \n",
      "...         ...     ...       ...   \n",
      "8976     P12524  Q6P597  0.750250   \n",
      "8977     P28074  Q99460  0.750249   \n",
      "8978     P78537  Q96GS4  0.750214   \n",
      "8979     O14972  O60826  0.750171   \n",
      "8980     Q9Y244  P20618  0.750156   \n",
      "\n",
      "                                                   hog1  \\\n",
      "0     (HOG:A0786418.3b.3a.6b.7b.3a.5a.1a, [10509897,...   \n",
      "1     (HOG:A0495961.1b.4c.1b, [10480506, 4099522525,...   \n",
      "2     (HOG:A0495961.1b.4c.1a, [10480505, 4099522183,...   \n",
      "3     (HOG:A0495961.1a.1b, [10480515, 4099525487, 44...   \n",
      "4     (HOG:A0498062.5c, [10445481, 4087452873, 465, ...   \n",
      "...                                                 ...   \n",
      "8976  (HOG:A0552037.1b, [10478826, 4099019790, 505, ...   \n",
      "8977  (HOG:A0563496.5a.4a.3b.7a.8b, [10501895, 41083...   \n",
      "8978  (HOG:A0501712.1a, [10466111, 4094714680, 358, ...   \n",
      "8979  (HOG:A0569342.5a, [10535122, 4121527616, 628, ...   \n",
      "8980  (HOG:A0563478.24a.16b.6a, [10521953, 411636653...   \n",
      "\n",
      "                                                   hog2  \n",
      "0     (HOG:A0786418.3b.3a.6b.7b.3b.9b, [10515916, 41...  \n",
      "1     (HOG:A0495961.1b.4c.1a, [10480505, 4099522183,...  \n",
      "2     (HOG:A0495961.1b.4c.1a, [10480505, 4099522183,...  \n",
      "3     (HOG:A0495961.1b.4c.1a, [10480505, 4099522183,...  \n",
      "4                                            (nan, nan)  \n",
      "...                                                 ...  \n",
      "8976  (HOG:A0515766.2b.2b, [10483510, 4100723785, 36...  \n",
      "8977  (HOG:A0562650.2b.9b.11b.12b.8b.3a, [10452854, ...  \n",
      "8978  (HOG:A0567165.4b.6a.15c.5a.2a.2b, [10447752, 4...  \n",
      "8979  (HOG:A0566680.6a.7b, [10493022, 4104568645, 29...  \n",
      "8980  (HOG:A0554994.6a.4a.3a, [10451120, 4089385126,...  \n",
      "\n",
      "[8981 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#filter to 90%proba\n",
    "df = df[df[2] > .75 ]\n",
    "print(len(df))\n",
    "mapper = set( list(df[1]) + list(df[0]) )\n",
    "mapper = { protid: grabHog(protid) for protid in mapper }\n",
    "df['hog1'] = df[1].map(mapper)\n",
    "df['hog2'] = df[0].map(mapper)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hogid_1'] = df['hog1'].map(lambda x:x[0])\n",
    "df['hogid_2'] = df['hog2'].map(lambda x:x[0])\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0].hog1)\n",
    "df['fam1'] = df['hog1'].map( lambda x :   p.hogid2fam(x[1]) )\n",
    "df['fam2'] = df['hog2'].map( lambda x :   p.hogid2fam(x[1]) ) \n",
    "df = df.dropna()\n",
    "df.fam1 = df.fam1.map(int)\n",
    "df.fam2 = df.fam2.map(int)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(humap+'hogmapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with mapped ids\n",
    "import scipy\n",
    "import dendropy\n",
    "from matplotlib import pyplot as plt\n",
    "species_tree = p.tree\n",
    "with open('species_tree.nwk', 'w')as treeout:\n",
    "    treeout.write( species_tree.write())\n",
    "tree = dendropy.Tree.get( path='species_tree.nwk', schema='newick')\n",
    "for i,n in enumerate(tree.nodes()):\n",
    "    n.matrow = i\n",
    "    n.symbols = None\n",
    "    n.scores = None\n",
    "    n.event = None\n",
    "    n.char = None\n",
    "\n",
    "matsize = len(tree.nodes())\n",
    "print(matsize)\n",
    "print('nodes')\n",
    "#blur w connectivity mat\n",
    "blurfactor =  .25\n",
    "connectmat = scipy.sparse.lil_matrix((len(tree.nodes()), len(tree.nodes() ) ) )\n",
    "index = np.array([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "connectmat[index[:,0],index[:,1]] = 1\n",
    "connectmat[index[:,1],index[:,0]] = 1\n",
    "sumlengths = connectmat.sum(axis=1)\n",
    "#diag= scipy.sparse.diags(1,[0])\n",
    "#connectmat += diag\n",
    "connectmat = scipy.sparse.coo_matrix(connectmat)\n",
    "plt.figure( figsize=(10,10))\n",
    "plt.title( 'NCBI taxonomic tree connectivity matrix ' )\n",
    "plt.spy(connectmat, markersize= 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data for our explicit comparisons\n",
    "from pyprofiler.utils import hashutils\n",
    "import ete3\n",
    "allhogs = set([])\n",
    "[allhogs.add( h ) for h in df.fam1.unique()]\n",
    "[allhogs.add( h ) for h in df.fam2.unique()]\n",
    "print(len(allhogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "\n",
    "\n",
    "for fam in allhogs:\n",
    "    print(fam)\n",
    "    try:\n",
    "        prof = p.return_profile_OTF(fam)\n",
    "    except:\n",
    "        print('err', fam)\n",
    "    print(prof)\n",
    "    profiles.update(prof)\n",
    "    \n",
    "    \n",
    "\n",
    "'''\n",
    "explicit_profiles = p.retmat_mp_profiles( allhogs , nworkers= 20 , verbose = False )\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(humap + 'gold_standard_profiles.pkl' , 'wb') as profiles_out:\n",
    "    profiles_out.write(pickle.dumps(profiles))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explicit_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import dataframe as dd\n",
    "df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                 group1  group2 neighborhood fusion cooccurence coexpression experimental database textmining combined_score\n",
      "npartitions=195                                                                                                             \n",
      "                 object  object        int64  int64       int64        int64        int64    int64      int64          int64\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "...                 ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "                    ...     ...          ...    ...         ...          ...          ...      ...        ...            ...\n",
      "Dask Name: read-csv, 195 tasks\n",
      "Index(['group1', 'group2', 'neighborhood', 'fusion', 'cooccurence',\n",
      "       'coexpression', 'experimental', 'database', 'textmining',\n",
      "       'combined_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = ['neighborhood', 'fusion', 'cooccurence', 'combined_score' ]\n",
    "df = df.drop(columns = dropcols)\n",
    "\n",
    "df['score'] = df.coexpression + df.experimental +df.database+ df.textmining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, bins = dask.array.histogram(df.score,bins=20, range=[0, 5000])\n",
    "h = h.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115479844   7352936   2295808    955128    217648    108728     62692\n",
      "     41052     20172     15904     13516     10340      5112      3516\n",
      "      2460       760         0         0         0         0] [   0.  250.  500.  750. 1000. 1250. 1500. 1750. 2000. 2250. 2500. 2750.\n",
      " 3000. 3250. 3500. 3750. 4000. 4250. 4500. 4750. 5000.]\n"
     ]
    }
   ],
   "source": [
    "print(h,bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/4163888/ipykernel_708605/2148639131.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.bar(bins[:-1],np.log(h),width=500)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "plt.bar(bins[:-1],np.log(h),width=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71680\n"
     ]
    }
   ],
   "source": [
    "df= df[df.score>2000]\n",
    "print(len(df))\n",
    "df= df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabcogs = set( list(df.group1.unique()) + list(df.group2.unique()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabcogs= list(grabcogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4866 ['COG1163', 'COG2051', 'KOG0822', 'KOG2340', 'NOG258475', 'COG5133', 'NOG258020', 'COG5143', 'NOG06884', 'COG2023', 'NOG231554', 'NOG23381', 'KOG0598', 'COG3705', 'KOG2609', 'KOG3026', 'NOG283576', 'COG1588', 'KOG2265', 'NOG84241', 'NOG246086', 'NOG23389', 'KOG0297', 'KOG2669', 'NOG236509', 'NOG131411', 'NOG257719', 'KOG3202', 'NOG00701', 'KOG0608', 'KOG4748', 'COG3833', 'COG5084', 'NOG18079', 'KOG2279', 'NOG05807', 'NOG10940', 'COG4890', 'NOG17879', 'KOG0812', 'NOG04185', 'COG3515', 'NOG270167', 'NOG89573', 'COG3117', 'COG0821', 'COG2388', 'NOG02081', 'NOG02195', 'KOG1882', 'NOG07543', 'NOG38949', 'NOG01887', 'COG0377', 'NOG02389', 'NOG269946', 'COG0279', 'NOG244501', 'NOG14253', 'NOG22047', 'COG1380', 'COG3121', 'NOG273129', 'KOG1689', 'KOG3802', 'NOG250312', 'NOG282895', 'NOG04751', 'NOG254856', 'NOG38047', 'COG0153', 'COG2854', 'NOG17320', 'COG3736', 'NOG28786', 'COG4790', 'KOG4034', 'NOG272017', 'NOG55186', 'KOG3263', 'NOG08704', 'COG1282', 'COG0173', 'NOG07859', 'COG3497', 'KOG4068', 'COG0799', 'KOG2847', 'NOG02564', 'NOG258000', 'KOG3091', 'NOG01633', 'COG1938', 'KOG4524', 'NOG272349', 'KOG4609', 'KOG1937', 'NOG241387', 'NOG25036', 'KOG1413']\n"
     ]
    }
   ],
   "source": [
    "print(len(grabcogs), grabcogs[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       ##protein  start_position  \\\n",
      "orthologous_group                                                  \n",
      "COG0017                  1000570.HMPREF9966_0692               1   \n",
      "COG0017                  1000588.HMPREF9965_1653               1   \n",
      "COG0017                       1001240.GY21_12350               1   \n",
      "COG0017            1001530.BACE01000011_gene1967               1   \n",
      "COG0017                 1002367.HMPREF0673_01245               1   \n",
      "\n",
      "                   end_position  \\\n",
      "orthologous_group                 \n",
      "COG0017                     448   \n",
      "COG0017                     447   \n",
      "COG0017                     442   \n",
      "COG0017                     466   \n",
      "COG0017                     472   \n",
      "\n",
      "                                                  protein_annotation  \n",
      "orthologous_group                                                     \n",
      "COG0017            Asparagine--tRNA ligase; Identified by match t...  \n",
      "COG0017            Asparagine--tRNA ligase; Identified by match t...  \n",
      "COG0017            Aspartate--tRNA ligase; Catalyzes the attachme...  \n",
      "COG0017                                     annotation not available  \n",
      "COG0017            Asparagine--tRNA ligase; KEGG: pru:PRU_0054 7....  \n"
     ]
    }
   ],
   "source": [
    "#map the interacting cogs to the proteins\n",
    "COGmapings_df = dd.read_csv('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt', blocksize=25e6 , header = 0, sep = '\\t')\n",
    "COGmapings_df = COGmapings_df.set_index('orthologous_group')\n",
    "COGmapings_df = COGmapings_df.loc[grabcogs]\n",
    "print(COGmapings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4278995\n"
     ]
    }
   ],
   "source": [
    "#only take the proteins in our cogs of interest\n",
    "COGmapings_df = COGmapings_df.compute()\n",
    "grabprots =list(COGmapings_df['##protein'].unique())\n",
    "print(len(grabprots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/dmoi/datasets/STRING/COG.links.detailed.v11.5.txt' + '.grabcogs.txt', 'w') as protsout:\n",
    "    protsout.write(''.join([ p + '\\n' for p in grabcogs ]) )\n",
    "    \n",
    "with open('/scratch/dmoi/datasets/STRING/COG.mappings.v11.5.txt' + '.grabprots.txt' , 'w') as protsout:\n",
    "    protsout.write(''.join([ p + '\\n' for p in grabprots ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying cluster\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=10\n",
      "#SBATCH --mem=373G\n",
      "#SBATCH -t 24:00:00\n",
      "source /scratch/dmoi/miniconda/etc/profile.d/conda.sh\n",
      "conda activate ML2\n",
      "/scratch/dmoi/condaenvs/ML2/bin/python -m distributed.cli.dask_worker tcp://10.203.100.112:37567 --nthreads 1 --nprocs 10 --memory-limit 37.25GiB --name dummy-name --nanny --death-timeout 60 --interface ib0 --protocol tcp://\n",
      "\n",
      "SLURMCluster(4fd4807c, 'tcp://10.203.100.112:37567', workers=0, threads=0, memory=0 B)\n",
      "http://10.203.100.112:40047/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/dmoi/condaenvs/ML2/lib/python3.9/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40047 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "\n",
    "NCORE = 10\n",
    "njobs = 20\n",
    "print('deploying cluster')\n",
    "cluster = SLURMCluster(\n",
    "    walltime='24:00:00',\n",
    "    n_workers = NCORE,\n",
    "    cores=NCORE,\n",
    "    processes = NCORE,\n",
    "    interface='ib0',\n",
    "    memory=\"400GB\",\n",
    "    env_extra=[\n",
    "    'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "    'conda activate ML2'\n",
    "    ],\n",
    "    scheduler_options={'interface': 'ens2f0' }\n",
    ")\n",
    "print(cluster.job_script())\n",
    "#cluster.adapt(minimum=10, maximum=30)\n",
    "cluster.scale(jobs=20)\n",
    "print(cluster)\n",
    "print(cluster.dashboard_link)\n",
    "client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                  protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score\n",
      "npartitions=61621                                                                                                                                                                                                                                             \n",
      "                    object   object        int64                    int64  int64       int64    int64        int64                    int64       int64                   int64    int64                int64      int64                  int64          int64\n",
      "                       ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "...                    ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "                       ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "                       ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "Dask Name: read-csv, 61621 tasks\n"
     ]
    }
   ],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.links.full.v11.5.txt', blocksize=25e6 , header = 0, sep = ' ')\n",
    "print(link_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df['score'] = link_df.coexpression + link_df.experiments +link_df.database+ link_df.textmining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = link_df.set_index('protein1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = link_df.loc[grabprots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = link_df.set_index('protein2')\n",
    "link_df = link_df.loc[grabprots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(link_df))\n",
    "link_df = link_df.compute()\n",
    "#output the smallerdf\n",
    "link_df.to_csv('/scratch/dmoi/datasets/STRING/protein.links.full.v11.5.txt'+'filtered.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open up string data\n",
    "\n",
    "#map HOGs to string COGs\n",
    "\n",
    "#Find COGs with physicallink_dfression interaction data \n",
    "\n",
    "#filter for physical interaction quality and remove interlogs\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find min subtree for two interactors\n",
    "\n",
    "#only label the subtree as subgraph\n",
    "\n",
    "#fitch the ancestral states\n",
    "\n",
    "#when one is absent the state in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ete3\n",
    "taxonomy = p.tree\n",
    "\n",
    "\n",
    "taxindex ={ n:i for i,n in enumerate(taxonomy.traverse())}\n",
    "reverse ={ i:n for i,n in enumerate(taxonomy.traverse())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapfam to matrow\n",
    "fam_map= { f:i for i,f in enumerate(explicit_profiles.index)}\n",
    "profilemat = np.vstack(explicit_profiles.mat)\n",
    "print(profilemat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(allhogs))\n",
    "#filter out the entries in sub that don't have a profile\n",
    "\n",
    "sub=sub[ sub.fam1.isin(fam_map) & sub.fam2.isin(fam_map)]\n",
    "df = df[ df.fam1.isin(fam_map) & df.fam2.isin(fam_map)]\n",
    "\n",
    "print(len(sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use all of the gold standard to train and use the subnet to quantify\n",
    "import itertools\n",
    "\n",
    "def chunks(df, n):\n",
    "    for i in range(0, len(df), n):\n",
    "        yield df.iloc[i:i + n]\n",
    "        \n",
    "def generateXYchunk(explicit_profiles, goldstandardDF,  nsamples=100, posi_percent = .5):\n",
    "    #shuffle\n",
    "    goldstandardDF = goldstandardDF.sample(frac=1)\n",
    "    for chunkdf in chunks(goldstandardDF , int( nsamples*posi_percent)):\n",
    "        #negatives drawn from the overall dataset\n",
    "        X = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam1]) , np.vstack([profilemat[fam_map[f]] for f in chunkdf.fam2]) ] )\n",
    "        Y = [1]* X.shape[0]\n",
    "        neg1 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam1)\n",
    "        neg2 = list(goldstandardDF.sample(n = int(nsamples*(1-posi_percent))).fam2)\n",
    "        \n",
    "        if len(neg1)>0:\n",
    "            mixchunk = np.hstack([np.vstack([profilemat[fam_map[f]] for f in neg1]),np.vstack([profilemat[fam_map[f]] for f in neg2])])\n",
    "            Y =np.hstack([[0]* mixchunk.shape[0] , Y])\n",
    "            X= np.vstack([mixchunk,X])    \n",
    "        #positive samples\n",
    "        yield X, Y\n",
    "    \n",
    "\n",
    "def Testwsubnet(explicit_profiles, sub ):\n",
    "    \n",
    "    print(len(sub))\n",
    "    \n",
    "    Xposi = np.hstack([ np.vstack([profilemat[fam_map[f]] for f in sub.fam1]) , np.vstack([profilemat[fam_map[f]] for f in sub.fam2]) ] )\n",
    "    print(Xposi.shape)\n",
    "    pospairs = set(zip( list(sub.fam1) , list(sub.fam2)))\n",
    "    \n",
    "    allsubfams = set( list(sub.fam1) +list(sub.fam2) )\n",
    "    \n",
    "    nega = set( [(f1,f2) for f1,f2 in itertools.combinations(allsubfams,2) if (f1,f2) not in pospairs] )\n",
    "    Xnega = np.vstack( [ np.hstack( [ profilemat[fam_map[f1]] , profilemat[fam_map[f2]] ]  )  for f1,f2 in nega ] )\n",
    "    print(Xnega.shape)\n",
    "    X = np.vstack([Xnega,Xposi])\n",
    "    Y = np.hstack([[0]*Xnega.shape[0], [1]*Xposi.shape[0]])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def ROC_curve(y_data):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot(precision, recall , label= l )\n",
    "        plt.xlabel('Precision')\n",
    "        plt.ylabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fams = list(set( sub.fam1).union(set(sub.fam2)))\n",
    "\n",
    "connectmat = np.zeros((len(fams), len(fams)))\n",
    "connectmat_binary = np.zeros((len(fams), len(fams)))\n",
    "\n",
    "protindex={ p:i for i,p in enumerate(fams)}\n",
    "\n",
    "sub['famrow']= sub.fam1.map(protindex)\n",
    "sub['famcolumn'] = sub.fam2.map(protindex)\n",
    "\n",
    "#symmetric connectivity matrix\n",
    "connectmat[sub.famrow, sub.famcolumn] = sub[2]\n",
    "connectmat+= connectmat.T\n",
    "print(connectmat)\n",
    "\n",
    "connectmat_binary[sub.famrow, sub.famcolumn] = 1\n",
    "np.fill_diagonal( connectmat_binary , 1 )\n",
    "print(connectmat_binary)\n",
    "#connectmat_binary+= connectmat_binary.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest, Ytest = Testwsubnet(explicit_profiles, sub )\n",
    "print(Xtest.shape)\n",
    "print(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a vanilla deep NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "\n",
    "from keras.models import load_model\n",
    "modelpath = './human_profilepairs_DNN.h5'\n",
    "\n",
    "if os.path.exists(modelpath):\n",
    "    model = load_model(modelpath)\n",
    "else:\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='sigmoid', input_dim=Xtest.shape[1]))\n",
    "    model.add(Dense(units=50, activation='sigmoid' ) )\n",
    "    model.add(Dense(units=1, activation='sigmoid' ) )\n",
    "model.compile(loss='binary_crossentropy', optimizer='ADAGRAD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generateXYchunk(explicit_profiles, gold_standard , posi_percent= .5 , nsamples = 50 )\n",
    "model.fit_generator(itertools.cycle(generator) , steps_per_epoch = 3000 , epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('./human_profilepairs_DNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the model activation!\n",
    "\n",
    "\n",
    "\n",
    "from keract import get_activations\n",
    "\n",
    "#which taxa are the most informative for human\n",
    "activations = get_activations(model, x, auto_compile=True)\n",
    "#paint activations to each node of the species tree\n",
    "#visualize the import event categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with blurring using the connectivity map / graph laplacian\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profilemat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_nn = model.predict( Xtest)\n",
    "ydata.update( {'RBM':{ 'Ytest': ypred_nn , 'Ytrue':Ytest} } )\n",
    "ROC_curve_single( Ytest ,  ypred_nn )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "from sklearn.covariance import GraphicalLassoCV , GraphicalLassoCV , EmpiricalCovariance\n",
    "\n",
    "subprofiles = np.vstack([profilemat[fam_map[f]] for f in fams])\n",
    "print(subprofiles.shape)\n",
    "\n",
    "#use mean field\n",
    "mf = GraphicalLassoCV(n_jobs = -1)\n",
    "mf.fit(subprofiles.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap( mf._precision )\n",
    "plt.show()\n",
    "ROC_curve_single(  connectmat_binary.ravel() , mf.covariance_.ravel() )\n",
    "sns.heatmap(mf.covariance_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'mf_human.pkl' , 'wb') as mfout:\n",
    "    mfout.write(pickle.dumps(mf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use covariance\n",
    "\n",
    "ydata = { 'NN': {'Ytrue':connectmat_binary.ravel(), 'Ypred': ymatnn.ravel() } , 'RBM':{'Ytrue':connectmat_binary.ravel() , 'Ypred': ymatrbm.ravel()} , 'EMP_COV': {'Ytrue':connectmat_binary.ravel(),'Ypred':emp.covariance_.ravel() } , \n",
    "         'MF_DCA':{'Ytrue':connectmat_binary.ravel(),'Ypred': mf.covariance_.ravel()} , 'PL_DCA':{'Ytrue':connectmat_binary.ravel(),'Ypred': predmat.ravel() } , 'JACCARD':{'Ytrue':connectmat_binary.ravel() , 'Ypred': jkern.ravel()  } }\n",
    "\n",
    "\n",
    "ROC_curve(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "                        \n",
    "emp = EmpiricalCovariance()\n",
    "emp.fit(subprofiles.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_curve_single(  connectmat_binary.ravel(),   emp.covariance_.ravel()  )\n",
    "\n",
    "\n",
    "sns.heatmap( emp.covariance_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the interaction detection power of just the jkern first\n",
    "\n",
    "import multiprocessing as mp\n",
    "#generate submatrices for inversion\n",
    "\n",
    "hashes = p.pull_hashes(fams)\n",
    "\n",
    "\n",
    "def yieldpairs(hashes):\n",
    "    for i,h1 in enumerate(hashes):\n",
    "        for j,h2 in enumerate(hashes):\n",
    "            if i < j:\n",
    "                yield [i,j,h1,h2]\n",
    "def distance(work):\n",
    "    i,j,h1,h2 = work\n",
    "    return [i,j,h1.jaccard(h2)]\n",
    "\n",
    "pool = mp.Pool()\n",
    "hashes = list(hashes.values())\n",
    "res = pool.map_async( distance , yieldpairs(hashes)).get()\n",
    "i,j, dist = zip(*res)\n",
    "jkern = np.zeros( (len(hashes),len(hashes)))\n",
    "jkern[i,j]=dist\n",
    "jkern += jkern.T\n",
    "np.fill_diagonal(jkern, 1)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(jkern)\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(connectmat_binary)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "ROC_curve_single( connectmat_binary.ravel()  , jkern.ravel() )\n",
    "\n",
    "#ROC_curve_single(connectmat_binary.ravel() , predmat.ravel() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf2_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonus graph neural networks\n",
    "\n",
    "#encode phylogenies as graphs\n",
    "\n",
    "#make sandwhich layer\n",
    "\n",
    "\n",
    "#train graph nn receptive field\n",
    "\n",
    "\n",
    "#predict on human node\n",
    "\n",
    "#predict on several nodes\n",
    "#use string data and get all interaction interlogs\n",
    "\n",
    "#propagate the feature of interaction using fitch in sandwhich layer\n",
    "\n",
    "#compare AUC in different branches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
