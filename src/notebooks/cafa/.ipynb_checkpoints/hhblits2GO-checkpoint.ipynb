{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHHblits( aln , name, path , outdir, db , iterations , ncores , runName , SS  , oa3m , verbose = False):\n",
    "    if verbose == True:\n",
    "        print( [aln , name, path , outdir, db , iterations , ncores , runName] )\n",
    "    outhhr= outdir+name+runName+\".hhr\"\n",
    "    args = path + ' -cpu '+ str(ncores) +' -d ' + db + ' -i ' + aln  +' -o '+ outhhr + ' -n ' + str(iterations) + ' -norealign -add_cons -M 50'\n",
    "    if SS == True:\n",
    "         args += ' -ssm 2 -ssw .5 '\n",
    "    \n",
    "    if oa3m == True:\n",
    "        outa3m = outdir+name+runName+'.a3m'\n",
    "        args += ' -oa3m ' + outa3m\n",
    "    else: \n",
    "        outa3m = None\n",
    "    if verbose == True:\n",
    "        print(args)\n",
    "    \n",
    "    args = shlex.split(args)\n",
    "    p = subprocess.Popen(args )\n",
    "    return p , [outhhr,outa3m]\n",
    "\n",
    "\n",
    "def runHHmake( aln , name, path = 'hhmake' , outdir='./', verbose = False, SS = False):\n",
    "    if verbose == True:\n",
    "        print( [aln , name, path , outdir] )\n",
    "    outhhm= outdir+name+\".hhm\"\n",
    "    args = path + ' -i '+  aln  +' -o '+ outhhm + ' -M 50'\n",
    "    if SS == True:\n",
    "        #todo : make ss prediction here\n",
    "        pass\n",
    "    args = shlex.split(args)\n",
    "    p = subprocess.Popen(args )\n",
    "    return p , [outhhm]\n",
    "\n",
    "\n",
    "def hhrparse(hhr , coverage , proba ):\n",
    "    profile = HHOutputParser(alignments=False).parse_file(hhr)\n",
    "    qname = profile.query_name\n",
    "    for hit in profile:\n",
    "        proba = hit.probability\n",
    "        i = hit.id\n",
    "\n",
    "def query2PFAM_PDB(qaln, name , outdir= './' , path = 'hhblits' , verbose = False):\n",
    "    uniclust = '/home/cactuskid13/mntpt/HHBLITsdb/uniclust30_2018_08/uniclust30_2018_08'\n",
    "    pdb70 = '/home/cactuskid13/mntpt/HHBLITsdb/pdb70/pdb70'\n",
    "    pfam = '/home/cactuskid13/mntpt/HHBLITsdb/pfam'\n",
    "\n",
    "    p,ret = runHHmake(qaln, name)\n",
    "    qaln = ret[0]\n",
    "    \n",
    "    db = uniclust\n",
    "    iterations = 2\n",
    "    ncores = 4\n",
    "    \n",
    "    runName = 'UNI'\n",
    "    SS  = False\n",
    "    oa3m = True\n",
    "    \n",
    "    p, ret = runHHblits( qaln ,name, path , outdir, db , iterations , ncores , runName , SS  , oa3m , verbose)\n",
    "    p.wait()\n",
    "    UNI_HHR = ret[0]\n",
    "    '''\n",
    "    #todo_add SS to model here\n",
    "    \n",
    "    db = pfam\n",
    "    iterations = 1\n",
    "    ncores = 1\n",
    "    runName = 'PFAM'\n",
    "    SS  = False\n",
    "    oa3m = False\n",
    "    qaln = ret[1]\n",
    "    \n",
    "    p, ret = runHHblits( qaln , name,  path , outdir, db , iterations , ncores , runName , SS  , oa3m , verbose)\n",
    "    p.wait()\n",
    "    PFAM_HHR = ret[1]\n",
    "    \n",
    "    \n",
    "    db = pdb70\n",
    "    iterations = 1\n",
    "    ncores = 1\n",
    "    runName = 'PDB'\n",
    "    SS  = False\n",
    "    oa3m = False\n",
    "    \n",
    "    p, ret = runHHblits( qaln ,name , path , outdir, db , iterations , ncores , runName , SS  , oa3m , verbose  )\n",
    "    p.wait()\n",
    "    PDB_HHR = ret[1]\n",
    "    '''\n",
    "    return UNI_HHR #, PDB_HHR , PFAM_HHR\n",
    "\n",
    "def PFAM_HHR2GO(hhr):\n",
    "    #map thru interpro\n",
    "    \n",
    "    pass\n",
    "\n",
    "def PDB_HHR2GO(hhr):\n",
    "    #map from pdb\n",
    "    \n",
    "    pass\n",
    "\n",
    "def UNI_HHR2GO(hhr):\n",
    "    #map from uniprot gaf\n",
    "    \n",
    "    #map to uniclust cluster rep and grab all identifiers\n",
    "    \n",
    "    #check if there are any annotations in cluster\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make pandas df of cath dom chains\n",
    "import glob \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./pdb_chain_go.csv' , comment = '#' , error_bad_lines = False ,  warn_bad_lines = False )\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe of uniprot noniea annotations\n",
    "import pandas as pd\n",
    "gaf = './goa_noiea.gaf'\n",
    "unigaf = pd.read_csv( gaf , comment = '!' , sep = '\\t'  , header = None)\n",
    "print(unigaf.head())\n",
    "\n",
    "uniclust = '/home/cactuskid13/mntpt/HHBLITsdb/uniclust30_2018_08/uniclust30_2018_08.tsv'\n",
    "dfclust = pd.read_csv( uniclust , comment = '!' , sep = '\\t' , header = None )\n",
    "print(dfclust.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigaf = unigaf.merge( dfclust , how   = 'left' , left_on = 1 , right_on =  1  )\n",
    "print(unigaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a branching tree of uniclust mapping\n",
    "def lookup_clust(btree, ref):\n",
    "    c = iter(ref)\n",
    "    level = btree[next(c)]\n",
    "    while len(level)>1:\n",
    "        level = level[c]\n",
    "    return level[c]\n",
    "\n",
    "btree = {}\n",
    "\n",
    "for l in uniclustin:\n",
    "    clusterhead = l\n",
    "    prots = []    \n",
    "    for i,c in enumerate(clusterhead[0:-1]):\n",
    "        if i == 0:\n",
    "            if c not in btree:\n",
    "                btree[c] = {}\n",
    "            level = btree [c]\n",
    "        else:\n",
    "            if c not in level:\n",
    "                level[c] = {}\n",
    "            level[c] = {}\n",
    "    else:\n",
    "        #store refs in cluster head\n",
    "        level[ clusterhead[0:-1]] = prots\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe of interpro domains mapped to GO terms\n",
    "interpro_df = pd.read_csv('./interpro2go.txt' ,  sep = ';' , header = None, comment = '!' )\n",
    "interpro_df['IPID'] = interpro_df[0].map( lambda x : x.split()[0].split(':')[1] )\n",
    "pf_df = pd.read_csv('./ip2pfam.tsv' ,  sep = '\\t' ,comment = '!' )\n",
    "interpro_df = interpro_df.merge( pf_df , how='right' , right_on ='Accession' , left_on='IPID' )\n",
    "interpro_df = interpro_df[ interpro_df[1].notnull() ]\n",
    "#and map back to pfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query2PFAM_PDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2760a0b740d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mquery2PFAM_PDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestaln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hap2'\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'/home/cactuskid13/pyprofiler/pyprofiler/notebooks/cafa/'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hhblits'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query2PFAM_PDB' is not defined"
     ]
    }
   ],
   "source": [
    "testaln = './Hap2HOG.aln.fasta'\n",
    "import time\n",
    "t0 = time.time()\n",
    "query2PFAM_PDB(testaln, name='hap2' ,  outdir= '/home/cactuskid13/pyprofiler/pyprofiler/notebooks/cafa/' , path = 'hhblits' , verbose = True)\n",
    "print( time.time() - t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse hhr files for each query and filter for nice hits\n",
    "\n",
    "\n",
    "#map go from pdb chains and pfam domains\n",
    "\n",
    "\n",
    "#output final pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
