{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#profiler load \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from pyprofiler.utils import config_utils\n",
    "import pyprofiler.utils.goatools_utils as goa\n",
    "import pyprofiler.profiler as profiler\n",
    "\n",
    "#select rand profile w annotations\n",
    "#mask annotations\n",
    "#pull in coevolving Hogs\n",
    "#feed profile vectors and GO to NN\n",
    "#Try to recover annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from qtlsearch.OBOParser import OBO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf =  '/home/cactuskid13/mntpt/unil_backup/profilingbackup/gaf/oma-go.txt'\n",
    "obo = '/home/cactuskid13/mntpt/unil_backup/profilingbackup/gaf/go.obo'\n",
    "go = OBO(obo, store_as_int=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import itertools \n",
    "from scipy.stats import bernoulli\n",
    "import math\n",
    "import random\n",
    "\n",
    "def yeildBags(gaf):\n",
    "    with open(gaf , 'r') as gafin:\n",
    "        omaID = None\n",
    "        lastID = None\n",
    "        for l in gafin:\n",
    "            if l[0] != '#':\n",
    "                words = l.split()\n",
    "                omaID,GO,evi,ref = words[0:4]\n",
    "                if lastID is None:\n",
    "                    lastID = omaID\n",
    "                    dat = { 'ID': omaID , 'GO': ancestors(GO) , 'REF' : [ref] , 'EVI' : [evi]}\n",
    "                elif omaID != lastID:\n",
    "                    yield dat\n",
    "                    dat = { 'ID': omaID , 'GO': ancestors(GO) , 'REF' : [ref] , 'EVI' : [evi]}\n",
    "                else:\n",
    "                    #todo: yeild ancestors of terms\n",
    "                    dat['GO'] = dat['GO'].union( ancestors(GO) )\n",
    "                    dat['REF']+=[ref]\n",
    "                    dat['EVI']+=[evi]\n",
    "                \n",
    "def ancestors(term,verbose = False):\n",
    "    numterm = int(term.split(':')[1])\n",
    "    try:\n",
    "        ret = go.parents(numterm).union(set([numterm]))\n",
    "        return ret\n",
    "    except:\n",
    "        if verbose == True:\n",
    "            print(term)\n",
    "\n",
    "def yeild_annotations(gaf, verbose = False):\n",
    "     with open(gaf , 'r') as gafin:\n",
    "        for l in gafin:\n",
    "            if l[0] != '#':\n",
    "\n",
    "                #todo: yeild ancestors of terms\n",
    "                #for term in retgoterms(l.split()[1]):\n",
    "                #yield term\n",
    "                try:\n",
    "                    for t in ancestors(l.split()[1]):\n",
    "                        yield t\n",
    "                except:\n",
    "                    if verbose == True:\n",
    "                        print(l)\n",
    "\n",
    "def makeGOdict(gaf , sampling_factor= 1e-05):\n",
    "    #return some descriptors of the dataset to be \n",
    "    #used for embedding NN\n",
    "    c = collections.Counter(yeild_annotations(gaf))\n",
    "    #count all the go terms in the OMA corpus    \n",
    "    nannot = sum(c.values())\n",
    "    nterms = len(c.keys())\n",
    "    #info = np.log(np.asarray(c.values())/nannot)\n",
    "    #infocontent = dict(zip( c.keys(), list(info)))\n",
    "    index = dict(zip(c.keys(), list(np.arange(nterms) )))\n",
    "    reverse_index = dict( zip( index.values(), index.keys() ))\n",
    "    freq = list(np.array(list(c.values()))/nannot)\n",
    "    freq = [  (min(1, math.sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))) for word_frequency in freq ]\n",
    "    sampling = dict(zip(c.keys(),freq))\n",
    "    return nterms , c , index , reverse_index , sampling\n",
    "\n",
    "def prunesamples(samples , sampling ):\n",
    "    #remove samples in probabilistic way\n",
    "    #thin out pairs with overrepresented go terms in them\n",
    "    ar1 = [ bernoulli.rvs(p, size=1)[0] == 1  for p in [ sampling[s[0]] for s in samples ] ]\n",
    "    ar2 = [ bernoulli.rvs(p, size=1)[0] == 1  for p in [ sampling[s[1]] for s in samples ] ]\n",
    "    select = np.bitwise_and(ar1,ar2)\n",
    "    samples = np.array(samples)[select,:]\n",
    "    return samples\n",
    "    \n",
    "def makesamples( gaf , index ):\n",
    "    #generator function to loop through gaf generating samples...\n",
    "    gafreader = yeildBags(gaf)\n",
    "    terms = list(sampling.keys())\n",
    "    infinite_gaf = itertools.cycle(gafreader)\n",
    "    for dat in infinite_gaf:\n",
    "        try:\n",
    "            posi = [  list(c)+[1]  for c in itertools.combinations( dat['GO'] , 2 ) ]\n",
    "            #bernoulli to prune positive data using sampling proba\n",
    "            posi = prunesamples(posi, sampling)\n",
    "            neg1 = [ random.choice(terms) for r in range(posi.shape[0]) ]\n",
    "            neg2 = [ random.choice(terms) for r in range(posi.shape[0]) ]\n",
    "            #we dont care so much about sampling in the negative dataset\n",
    "            nega =  np.array([ list(l) for l in zip( neg1 , neg2  , [0]*posi.shape[0] ) ])\n",
    "            #shuffle and separate\n",
    "            samples =  np.vstack([posi,nega])        \n",
    "            x1 = samples[:,0]\n",
    "            x2 = samples[:,1]\n",
    "            y = samples[:,2]\n",
    "\n",
    "            yield [x1,x2],y\n",
    "        except ValueError:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'makeGOdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e163b7235469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnterms\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreverse_index\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeGOdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'makeGOdict' is not defined"
     ]
    }
   ],
   "source": [
    "nterms , c , index , reverse_index , sampling = makeGOdict(gaf)\n",
    "print(nterms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open( './models/gafobects.pkl' , 'wb' ) as gafstats:\n",
    "    gafstats.write(pickle.dumps([nterms , c , index , reverse_index , sampling]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([  35586,   35587,   51716,   51716,   51716, 1901576,   23052,\n",
      "          6412,    6418,    6418,    6418,    6418,   71704,    4888,\n",
      "         19740,   46872,   46872,   43038,   43038,   43038,   43039,\n",
      "         43043,    6435,    6435,    6435,    6435,    6435,    6435,\n",
      "          6435,    6435,    6435,   32559,      49,      49,      49,\n",
      "            49,      49, 1901363,   34613,   35639,   70972,  140098,\n",
      "         46914,   46914,    8270,   43603,   43604,   43604,   43604,\n",
      "         33365,   34645,   30554,    3676,   36094,   34660,   50789,\n",
      "         16502,    6518,    6518,    6518,   45184,    3723,    5524,\n",
      "          5524,    6810,   43170,     166,   17076,   51641,    4812,\n",
      "         44237,   72657,   15833,    4829,    4829,    4829,    4829,\n",
      "         16875,   16875,   60821,   36105,   34074,   34130,   70359,\n",
      "         52922, 1905824,   97384,    2153,   19527,   99565,    9307,\n",
      "         35344,    1821,    1674,   60438,   60990,   15501,   45470,\n",
      "          1934, 1990357,    2067,   30951, 1903852,    9971,   42589,\n",
      "       1990883,   39694,   70974,   52665,   51775,   34750,   97089,\n",
      "         46430,   33275,    1780,     378, 1905961, 1990245, 1900080,\n",
      "          8748,    1962,   60288,   52917,   32143,   48865,   60710,\n",
      "       1903747,   16213,    7286,    9314,   42412, 1901655,    6406,\n",
      "         90303,   30538,   43405, 1903475,   19048,   61513,   46292,\n",
      "         50043, 2000707, 1990015,   72266,   21763,   71453,   30706,\n",
      "          4993,   32953,   35872,   70099,    8912,   19574,   47145,\n",
      "         34389,   19935,   15195,   42706]), array([   8144,   19752,      49,   60089,    4829,    4829,   16874,\n",
      "          6886,   32553,   43604,   97367,    6518,    6435,   42886,\n",
      "          8270,      49,    7166,   90150,   45184,    6399,   51234,\n",
      "          4829, 1901360,   34641,   19538,   30554,    6520,    6614,\n",
      "          4829,   16875,    5622,  140101,  140098,   36094,    4829,\n",
      "         16874,   44271,     166,   45047,   15031,    4829,    4829,\n",
      "         72594,    6612,   10467,   65007,   34660,   38023,    6399,\n",
      "         30554,   16875,    3723, 1901265,    4812,   50789,   72599,\n",
      "          4829,   16070,    4829,    7154,   15833,   16070,    6614,\n",
      "         16875,   72657,    6886,   16874,    8152,    4812,   16874,\n",
      "          6886,   10467,    4829,   16874,   44267,   65007,    6399,\n",
      "         44271,    5622,   43527,   48576, 1903709,   34229,   21899,\n",
      "         18530,  102499,   35305, 1990976,    9909,   51972,    4102,\n",
      "         61052,   19450,    4418,   35773,     411,    6857,   48736,\n",
      "         71362,   42667, 1902282,   47497,   42231,   61411,   10224,\n",
      "       1903747,    7374, 1904914,   20026,    2838,   45461, 1900560,\n",
      "       1902669, 2000097, 1905603,    6198,   38190,   46638,   80188,\n",
      "          9338,   19438, 2000574,    9997,  102293,   98892,   48212,\n",
      "          9118, 1900826,   52174,    6222,   17046,   19298,   61547,\n",
      "         43961,   45671,   70315,   61659,   90212,    1102,   86009,\n",
      "       1990097,   60484,   32802,   42020,    1178,   46383,   48136,\n",
      "         97255,   32373,   90560,  110118,    4514,   46383,   16115,\n",
      "        102522,    9312, 1903228,    5887])], array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print( next(makesamples( gaf , index )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model..\n",
      "Epoch 1/100\n",
      " 1552/10000 [===>..........................] - ETA: 24:05 - loss: 0.5653 - binary_accuracy: 0.6616"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f91c5f61f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m history = model.fit_generator(makesamples( gaf , index ), steps_per_epoch=10000, epochs=100, verbose=1, \n\u001b[0;32m--> 102\u001b[0;31m callbacks=[ mc, es ], max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyprofiler3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.metrics import *\n",
    "\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "with open( './models/gafobects.pkl' , 'rb' ) as gafstats:\n",
    "    nterms , c , index , reverse_index , sampling = pickle.loads(gafstats.read())\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction= 0.95\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "retrain = True\n",
    "\n",
    "if retrain == False:\n",
    "    #dimensionality of GO space\n",
    "    vector_dim = 750\n",
    "    #word2vec model to be trained\n",
    "    input_target = Input((1,) , name='target_in')\n",
    "    input_context = Input((1,) , name='context_in')\n",
    "\n",
    "    embedding = Embedding(nterms, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "    target = embedding(input_target)\n",
    "    target = Reshape((vector_dim, 1), name='target')(target)\n",
    "    context = embedding(input_context)\n",
    "    context = Reshape((vector_dim, 1) , name='context' )(context)\n",
    "\n",
    "    similarity = dot([target, context], axes=0 , normalize = True )\n",
    "\n",
    "    # now perform the dot product operation to get a similarity measure\n",
    "    dot_product = dot([target, context] , axes=1)\n",
    "    dot_product = Reshape((1,))(dot_product)\n",
    "    # add the sigmoid output layer\n",
    "    output = Dense(1, activation='sigmoid' , name = 'out')(dot_product)\n",
    "\n",
    "    # create the primary training model\n",
    "    o = RMSprop(lr=0.006125, rho=0.9)\n",
    "    \n",
    "    model = Model(inputs=[input_target,input_context], outputs=[output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=o , metrics = [ 'binary_accuracy'])    \n",
    "    \n",
    "    #embedder = Model(inputs=[input_target], outputs=[target])\n",
    "    \n",
    "    \n",
    "    validation_model = Model(input=[input_target, input_context], output=similarity)\n",
    "\n",
    "    class SimilarityCallback:\n",
    "        def run_sim(self):\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                sim = self._get_sim(valid_examples[i])\n",
    "                nearest = (-sim).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "\n",
    "        @staticmethod\n",
    "        def _get_sim(valid_word_idx):\n",
    "            sim = np.zeros((vocab_size,))\n",
    "            in_arr1 = np.zeros((1,))\n",
    "            in_arr2 = np.zeros((1,))\n",
    "            for i in range(vocab_size):\n",
    "                in_arr1[0,] = valid_word_idx\n",
    "                in_arr2[0,] = i\n",
    "                out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "                sim[i] = out\n",
    "            return sim\n",
    "\n",
    "        \n",
    "    sim_cb = SimilarityCallback()\n",
    "\n",
    "    ###modify this\n",
    "    batchiter = 10000\n",
    "    epochs = 100\n",
    "    \n",
    "    \n",
    "if retrain == True:\n",
    "    model = print('Load the model..')\n",
    "    modelfile = './models/GO2vec'\n",
    "    model = load_model(modelfile)\n",
    "    #o = RMSprop(lr=0.01, rho=0.9)\n",
    "    o = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=o , metrics = [ 'binary_accuracy'])\n",
    "\n",
    "    \n",
    "mc = ModelCheckpoint('./models/GO2vec750', monitor = 'loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "es = EarlyStopping(monitor = 'loss', mode = 'min', verbose = 2, patience = 50)\n",
    "\n",
    "history = model.fit_generator(makesamples( gaf , index ), steps_per_epoch=10000, epochs=100, verbose=1, \n",
    "callbacks=[ mc, es ], max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.metrics import *\n",
    "\n",
    "model = print('Load the model..')\n",
    "modelfile = './models/GO2vec'\n",
    "model = load_model(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "target_in (InputLayer)       (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1, 3)              96069     \n",
      "=================================================================\n",
      "Total params: 96,069\n",
      "Trainable params: 96,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputlayer = Input((1,) , name='target_in')\n",
    "layer = model.get_layer('embedding')\n",
    "x = layer(inputlayer)\n",
    "embedder = Model( inputs=[inputlayer] , outputs =[x]  )\n",
    "embedder.build( input_shape = (1,) )\n",
    "print(embedder.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cactuskid13/mntpt/unil_backup/profilingbackup/gaf/go.obo: fmt(1.2) rel(2019-11-02) 47,242 GO Terms\n",
      "['GO:0000001', 'GO:0000002', 'GO:0000003', 'GO:0000006', 'GO:0000007', 'GO:0000009', 'GO:0000010', 'GO:0000011', 'GO:0000012', 'GO:0000014']\n",
      "GO:0000002\tlevel-05\tdepth-05\tmitochondrial genome maintenance [biological_process]\n"
     ]
    }
   ],
   "source": [
    "from goatools import obo_parser\n",
    "obo = '/home/cactuskid13/mntpt/unil_backup/profilingbackup/gaf/go.obo'\n",
    "obo = obo_parser.GODag(obo)\n",
    "print(list(obo.keys())[0:10])\n",
    "print(obo['GO:0000002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32023\n",
      "[[ 4.5602007e+00  4.4778681e+00 -4.5426621e+00]\n",
      " [ 3.5615582e+00  3.4942293e+00 -3.5044131e+00]\n",
      " [ 3.5528460e+00  3.5645812e+00 -3.4729040e+00]\n",
      " ...\n",
      " [ 3.7296299e-02  1.6180174e-03 -6.0877856e-02]\n",
      " [ 4.5762777e-02 -5.3709585e-02 -1.8680591e-02]\n",
      " [ 8.7923789e-03  3.1755388e-02 -4.5389082e-02]]\n",
      "(32023, 3)\n",
      "<sklearn.neighbors.kd_tree.KDTree object at 0x5578e81814b8>\n"
     ]
    }
   ],
   "source": [
    "#create a KD tree to retreive pts\n",
    "#inverse direction from embedding\n",
    "import pickle\n",
    "with open( './models/gafobects.pkl' , 'rb' ) as gafstats:\n",
    "    nterms , c , index , reverse_index , sampling = pickle.loads(gafstats.read())\n",
    "print(nterms)\n",
    "embedmat = embedder.predict( [np.array(list(index.values()) ) ] )\n",
    "embedmat = embedmat.reshape(nterms,-1)\n",
    "print(embedmat)\n",
    "print(embedmat.shape)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import BallTree as BT\n",
    "from sklearn.neighbors import KDTree as KD\n",
    "tree = KD( embedmat )\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106112\n",
      "34\n",
      "[[15724  8739 28552 14768 26143 20430 12502 16447  4988  6890]]\n",
      "[106112, 19649, 120060, 955, 1903416, 21588, 90579, 1900593, 8536, 4319]\n",
      "[34, 20, 6, 223, 65, 48, 377, 1, 8298, 158]\n",
      "[[0.         0.00179405 0.00203943 0.00232785 0.00283212 0.00289521\n",
      "  0.00298394 0.00354521 0.00356996 0.00369736]]\n",
      "48291\n",
      "21\n",
      "[[27195 10437 31069  1711 20167 15789 10000 11823 21347 23446]]\n",
      "[48291, 33994, 9660, 33386, 1904533, 60967, 15546, 33192, 1901389, 2849]\n",
      "[21, 1, 36, 1453, 128, 779, 3, 492, 63, 33]\n",
      "[[0.         0.00140686 0.0022208  0.00234978 0.00243937 0.00290528\n",
      "  0.00303199 0.00305909 0.00306639 0.00308842]]\n",
      "15577\n",
      "269\n",
      "[[ 1913 22120 22012  8523   660  7849 18525 22640 13011 27665]]\n",
      "[15577, 51797, 35722, 33975, 30833, 50446, 1990868, 97167, 10571, 61869]\n",
      "[269, 768, 264, 1, 25706, 6, 10988, 152, 435, 4]\n",
      "[[0.         0.00100208 0.00171455 0.00203503 0.00272178 0.003373\n",
      "  0.00341508 0.0034532  0.0035939  0.00381483]]\n",
      "45713\n",
      "37\n",
      "[[26792 18100 17840 25168 10362 17670  9167 18903 14328  3922]]\n",
      "[45713, 30235, 110148, 1904290, 19306, 1901314, 47389, 5161, 6391, 16751]\n",
      "[37, 529, 3338, 49, 2, 221, 401, 544, 237, 2585]\n",
      "[[0.         0.00093932 0.00179757 0.00179829 0.0018038  0.00195464\n",
      "  0.00241557 0.00262351 0.00263207 0.00268733]]\n",
      "9428\n",
      "637\n",
      "[[ 5452  6326  5023 12316 15988  1518 14670 19098 17276 12808]]\n",
      "[9428, 42122, 9877, 71558, 45991, 34603, 61246, 1990108, 32228, 818]\n",
      "[637, 34, 922, 500, 357, 15625, 28, 296, 971, 521]\n",
      "[[0.         0.00213765 0.00388152 0.00462306 0.00502769 0.0051418\n",
      "  0.00520579 0.00532297 0.00535887 0.0054533 ]]\n",
      "1904743\n",
      "72\n",
      "[[25801 27006 13193  4079 25603 28212  7746 21680 29820  2550]]\n",
      "[1904743, 1903367, 44615, 42276, 99189, 1900248, 4436, 1900451, 48060, 51693]\n",
      "[72, 48, 1047, 2913, 30, 3, 32, 290, 44, 5529]\n",
      "[[0.         0.00142074 0.00245113 0.00246011 0.00276501 0.00309146\n",
      "  0.00333972 0.00341447 0.00352106 0.00380555]]\n",
      "1904041\n",
      "32\n",
      "[[26965 21339 27730 24949 17478 30774 14910 21333 23453 12727]]\n",
      "[1904041, 43011, 924, 1904649, 98943, 1905393, 1905136, 70266, 32829, 35329]\n",
      "[32, 607, 1, 62, 426, 935, 4, 951, 82, 2136]\n",
      "[[0.         0.00252717 0.00320287 0.00330531 0.00350692 0.00365841\n",
      "  0.00378363 0.00389421 0.00405572 0.00415635]]\n",
      "1901966\n",
      "1\n",
      "[[15851 22732 14475  7694  6567 23755 24878 29498  2282 12852]]\n",
      "[1901966, 1982, 9756, 46570, 71518, 72737, 5147, 61203, 6564, 1905395]\n",
      "[1, 9, 649, 573, 280, 91, 53, 4, 5396, 103]\n",
      "[[0.         0.00187581 0.00191067 0.00288065 0.00298618 0.00303255\n",
      "  0.00349126 0.00351226 0.00372592 0.00373547]]\n",
      "60821\n",
      "25\n",
      "[[27552 23525 17178 25160 16369 12962 26647 18283 21573  2817]]\n",
      "[60821, 1903061, 71560, 140343, 70797, 99139, 39695, 99560, 30540, 35268]\n",
      "[25, 138, 7285, 64, 3, 32, 33, 988, 619, 5231]\n",
      "[[0.         0.00196602 0.00276591 0.00353491 0.00409296 0.00417531\n",
      "  0.0043259  0.00502655 0.00536001 0.00579167]]\n",
      "10751\n",
      "91\n",
      "[[25692  3138 17611 26559 29569 31586 26889  3709 27885 17543]]\n",
      "[10751, 4140, 32814, 2001051, 36011, 47544, 2000655, 9035, 1905705, 99172]\n",
      "[91, 2393, 1193, 28, 163, 1, 31, 1135, 3, 1322]\n",
      "[[0.         0.00106082 0.00110443 0.00158412 0.00260294 0.00280593\n",
      "  0.00290769 0.00343922 0.004181   0.00419355]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def id2GO(intID):\n",
    "    return 'GO:{:07d}'.format(go_term_to_modif)\n",
    "\n",
    "keys = list(index.keys())\n",
    "for i in range(10):\n",
    "    select = random.choice(keys)\n",
    "    dist, ind = tree.query( embedmat[index[select],:].reshape(1, -1) , k=10)    \n",
    "    print(select)\n",
    "    print(c[select])\n",
    "    \n",
    "    print('returned terms')\n",
    "    print([ obo[ id2GO(i)] for i in list(ind) ] ) \n",
    "    \n",
    "    print([ reverse_index[i] for i in list(ind.ravel()) ])\n",
    "    print('count')\n",
    "    \n",
    "    print([ c[reverse_index[i]] for i in list(ind.ravel()) ])\n",
    "\n",
    "    print('sampling')\n",
    "    print([ sampling[reverse_index[i]] for i in list(ind.ravel()) ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for the net to transfer function from coevovling hogs\n",
    "\n",
    "#stateful rnn to output after receiving all annotations and profiles\n",
    "#feed it HOGs profiles and function vector for topk Hogs in order of jaccard\n",
    "\n",
    "\n",
    "\n",
    "#translation model RNN output. series of pts as function\n",
    "\n",
    "\n",
    "def err_annot( ypred, ytrue):\n",
    "    \n",
    "    #err for not enough predictions.\n",
    "    #concatenate all outpt of rnn and use the to gen err\n",
    "    # all pts pred vs all pts annot  \n",
    "    # select min dist match for each pred and add penalty term for over or under\n",
    "\n",
    "    #custom error from funcitonal profile of input hog\n",
    "    pass\n",
    "\n",
    "def seq2seq_model_builder(HIDDEN_DIM=300):\n",
    "    \n",
    "    #here max len is the number of annotations we can feed the model\n",
    "    #these are embedded go vectors paired with phylo profiles\n",
    "    \n",
    "    \n",
    "    #order the terms by IC to get some sort or directionality to the pred...\n",
    "    \n",
    "    encoder_inputs = Input(shape=(MAX_LEN, ))\n",
    "    \n",
    "    #stack lstm? integrate functional info into higher abstraction level\n",
    "    \n",
    "    layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "    x = layer(encoder_inputs)\n",
    "    \n",
    "    for b in lstmbottle:\n",
    "            #layer = CuDNNGRU(LSTMoutdim ,  name='gru_'+str(n),\n",
    "            #return_sequences=True, return_state=False, go_backwards=False, stateful=False )\n",
    "            \n",
    "            layer = CuDNNLSTM(b , kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                    recurrent_constraint=None, bias_constraint=None, return_sequences=True, return_state=False, stateful=False)\n",
    "            \n",
    "            layer = Bidirectional(layer, merge_mode='concat', weights=None)\n",
    "            x = layer(x)\n",
    "            \n",
    "            layer = Dropout(.5 , noise_shape=None, seed=None)\n",
    "            x = layer(x)\n",
    "            \n",
    "            layer = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones')\n",
    "            x = layer(x)        \n",
    "    \n",
    "    layer = CuDNNLSTM(b , kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "                bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                recurrent_constraint=None, bias_constraint=None, return_sequences=True, return_state=True, stateful=False)\n",
    "    \n",
    "    encoder_outputs, state_h, state_c = layer(x)\n",
    "    \n",
    "    \n",
    "    #just feed the decoder a start signal\n",
    "    \n",
    "    decoder_inputs = Input(shape=(MAX_LEN, ))\n",
    "    \n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_inputs, initial_state=[state_h, state_c])\n",
    "    \n",
    "    #merge decoder outputs and give an error on the bag of annotations rather than consider them as a sequence\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on generator \n",
    "def placeInHog(seq):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_rand_hog(p,gaf):\n",
    "    #training on hogs with annotation\n",
    "    \n",
    "    #grab HOG id from index of profiler.\n",
    "    hog_id = random.choice( p.lsh.index )\n",
    "    go_terms = { mr.omaid:gaf[mr.omaid] for mr in pyoma_dbobj.iter_members_of_hog_id(hog_id) if mr.omaid in gaf  }\n",
    "    pts = [ embedder(int(index(t.split(':')))) for t in set(go_terms.values()) ]\n",
    "\n",
    "def getProfiles(hog,k , Nannot):\n",
    "    #should return profile of top k Hogs and GO annots\n",
    "    fam = hashutils.hogid2fam(hog_id)\n",
    "    res = p.hog_query(fam = hog , k = k)\n",
    "    hashes = p.pullhashes(res+[hog])\n",
    "    go_terms = { hog_id: set([ gaf[mr.omaid] for mr in pyoma_dbobj.iter_members_of_hog_id(hog_id) if mr.omaid in gaf])  }\n",
    "    #filter for IC?\n",
    "    \n",
    "    \n",
    "    gopts = {  hog_id :np.vstack([ embedder(int(index(t.split(':')))) for t in go_terms[hog_id] ]) for hog_id in go_terms }\n",
    "    \n",
    "    jaccard = { hog_id: [hashes[hog_id].jaccard(hashes[hog])] for hog_id in res }\n",
    "    sorted_jaccard = list( np.argsort( list(jaccard.values() ) ) )\n",
    "    sorted_hogs = [ res[i] for i in sorted_jaccard ]\n",
    "    \n",
    "    #grab only top 100 annotations above info cutoff\n",
    "    \n",
    "    retmat = np.array([ pts + jaccard[h] + [1] for h in sorted_hogs for pts in gopts[h] ] )\n",
    "    if retmat.shape[1]>Nannot:\n",
    "        retmat = retmat[0:Nannot]\n",
    "    else:\n",
    "        retmat = np.vstack( np.zeros( ( Nannot - retmat.shape[0] , len(pts) +2  )  ) ,retmat)\n",
    "    #also return a vec of jaccard between query and results\n",
    "    return retmat\n",
    "\n",
    "def grab_explicit_profiles():\n",
    "    \n",
    "    #return the query Hog and top k profiles as vectors\n",
    "    #return comparison vectors(intersection, xor ) between q and topk k\n",
    "    pass\n",
    "\n",
    "     \n",
    "def generator(cafain , profiles = False):\n",
    "    #select CAFA query\n",
    "    \n",
    "    #placeinhog\n",
    "    \n",
    "    \n",
    "    #getprofiles\n",
    "    Hogs, jaccard , annots  = getprofiles(qHog)\n",
    "    \n",
    "    annots2vec(Hogs,annots)\n",
    "    \n",
    "    #grabexplicitprofiles\n",
    "    if profiles == True:\n",
    "        pass\n",
    "    \n",
    "    #annots2vec\n",
    "    \n",
    "    #yeild data\n",
    "    pass\n",
    "    \n",
    "#return HOG annot\n",
    "#return Bag of annots for HOGs\n",
    "\n",
    "\n",
    "#top k HOGs w closest jaccard. Order in terms of Annot info content or Jaccard?\n",
    "\n",
    "\n",
    "#generate a map of all go terms in new space to retreive closest terms quickly and calculate err\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def decoder_err(decoderbag, truthbag ):\n",
    "    #punish premature stop signal\n",
    "    \n",
    "    #select min of each pair of terms\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "'''\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')'''\n",
    "\n",
    "'''\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.'\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "'''\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "decoder_bag = Concatenate(axis=-1)(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_bag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####loss is continuous now\n",
    "\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='rmsd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make final pred by inversing projection pts\n",
    "\n",
    "#Project 45k pts of DAG to low dim space using embeding layer\n",
    "\n",
    "#use kd tree to retreive close terms to predicition pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#HOG dim = 12k * nHOGs \n",
    "#ontology dim = dimGOOBO * Nhogs\n",
    "\n",
    "\n",
    "#possibilities: reduced tree(clip all leaves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good to have: keras error fun for go term pred ( info content )\n",
    "\n",
    "#deepnet to softmax?\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
